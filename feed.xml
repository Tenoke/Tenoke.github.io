<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="https://svilentodorov.xyz//feed.xml" rel="self" type="application/atom+xml" /><link href="https://svilentodorov.xyz//" rel="alternate" type="text/html" /><updated>2020-07-08T17:30:33+02:00</updated><id>https://svilentodorov.xyz//feed.xml</id><title type="html">Svilen Todorov</title><subtitle>Personal site for Data Scientist and Machine Learning Engineer - Svilen Todorov</subtitle><entry><title type="html">The Unwatchability of Yesterday’s Videos Today and the Promise of Tomorrow</title><link href="https://svilentodorov.xyz//blog/lowres-highres/" rel="alternate" type="text/html" title="The Unwatchability of Yesterday’s Videos Today and the Promise of Tomorrow" /><published>2020-07-08T00:00:00+02:00</published><updated>2020-07-08T00:00:00+02:00</updated><id>https://svilentodorov.xyz//blog/AI-Superresolution</id><content type="html" xml:base="https://svilentodorov.xyz//blog/lowres-highres/">&lt;p&gt;A topic I end up discussing with friends and thinking about regularly is how videos and games we used to like a decade or two ago looked completely fine back then and are currently unwatchable. I don’t mean this in the more common way of shifting tastes and values but in almost literally barely seeing what’s going on. Videos in 360p and even lower (how is 144p even still an option in youtube?) seemed complete fine to me but now they are not and I am not even able to make out the details. Games that looked like huge graphical improvements at the time and giving us a taste of how realism might look like soon now look absolutely awful to the point where I can’t even tell what’s going on with certain objects (if I can even recognize them) in some scenes. One might say “Well, you are spoiled by much better graphics” and sure, it does seem like it but what does that entail? Why did I use to think I can see things I now can’t?&lt;/p&gt;

&lt;p&gt;My TV was definitely worse quality than now (if we pretend I even have one) but I am not sure if it was quite 240p. Reading &lt;a href=&quot;https://en.wikipedia.org/wiki/High-definition_television&quot;&gt;various&lt;/a&gt; &lt;a href=&quot;https://www.quora.com/What-are-the-resolutions-of-TV-sets-throughout-history-HD-then-1080p-then-4K&quot;&gt;links&lt;/a&gt; suggest 720p and more were already available in the 90s/early 2000s, at least &lt;a href=&quot;https://en.wikipedia.org/wiki/Standard-definition_television&quot;&gt;~480p+&lt;/a&gt; seems to have been somewhat more common and at minimum movies at the cinema had a decent effective resolution by then even accounting for the distance. Yet Youtube &lt;a href=&quot;https://en.wikipedia.org/wiki/YouTube#Quality_and_formats&quot;&gt;only offered 320x240&lt;/a&gt; between 2005-2008 which was mostly fine then and nearly unwatchable now?&lt;/p&gt;

&lt;p&gt;There is a popular &lt;a href=&quot;https://www.youtube.com/watch?v=dMH0bHeiRNg&quot;&gt;Evolution of Dance&lt;/a&gt; video from 2006 which I remember and in my memories, it looked pretty okay. I just re-watched it now and sure, I can still tell what’s going on but I can, for example, barely see the dancer’s face. Indeed, it isn’t that bad if you watch a single clip, however, when I tried to watch an &lt;a href=&quot;https://www.youtube.com/watch?v=ME7smdgMVr4&amp;amp;list=PL1E7ADC0B6C30C1DF&quot;&gt;older sketch show&lt;/a&gt; for more than a couple of episodes the quality made it actively unpleasant. The effect also definitely exists in games that seemed to look pretty good to me - e.g. &lt;a href=&quot;https://www.google.com/search?q=command+and+conquer+old&amp;amp;safe=off&amp;amp;source=lnms&amp;amp;tbm=isch&amp;amp;sa=X&amp;amp;biw=1252&amp;amp;bih=912&quot;&gt;Command and Conquer&lt;/a&gt; which we will get back to later or &lt;a href=&quot;https://www.google.com/search?q=Tomb+Raider+3&amp;amp;tbm=isch&quot;&gt;early Tomb Rider games&lt;/a&gt; for a 3D example.&lt;/p&gt;

&lt;p&gt;One argument is that old videos look worse because of our massive screens today. Checking on my phone there might be some truth to it, however, I am far from convinced that it explains it.&lt;/p&gt;

&lt;h4 id=&quot;refresh-rate-comparison&quot;&gt;Refresh Rate Comparison&lt;/h4&gt;

&lt;p&gt;People getting used to better quality and finding it hard to go back is not uncommon of course - I hear it quite often today in regards to higher refresh rate displays. You get comments &lt;a href=&quot;https://www.reddit.com/r/buildapc/comments/bgxf5w/for_those_wondering_if_a_144hz_monitor_is_really/&quot;&gt;like&lt;/a&gt; &lt;a href=&quot;https://www.reddit.com/r/OverwatchUniversity/comments/5srzgk/does_a_higher_refresh_rate_on_a_monitor_really/ddhdotb/&quot;&gt;this&lt;/a&gt; claiming that &lt;em&gt;“You don’t realize how ‘laggy’ 60hz is until you go to 144. Now I can never go back.”&lt;/em&gt; or &lt;em&gt;“now I HATE my monitors at work”&lt;/em&gt; which I find interesting as the ‘never go back’ sentiment is very common. In fact, if you try to look into 144 Hz monitors these comments are nearly ever-present. This is seemingly more prominent with people who mainly use higher refresh rates, less so among those who use e.g. a lower refresh rate on one screen and a higher on another. That suggests to me that perhaps in general nearly constant exposure to better quality ‘ruins’ low quality much more easily than if you get exposed to both.&lt;/p&gt;

&lt;h4 id=&quot;hallucination-theory&quot;&gt;“Hallucination” Theory&lt;/h4&gt;

&lt;p&gt;An argument which I like is that we didn’t know much better but were used to those low resolutions and found it &lt;em&gt;easy to fill in the details&lt;/em&gt;. The brain already does &lt;a href=&quot;https://en.wikipedia.org/wiki/Filling-in&quot;&gt;a lot&lt;/a&gt; of &lt;a href=&quot;https://en.wikipedia.org/wiki/Saccadic_masking&quot;&gt;that kind&lt;/a&gt; of &lt;a href=&quot;https://en.wikipedia.org/wiki/Visual_release_hallucinations&quot;&gt;thing&lt;/a&gt; all the time, so it doesn’t seem so unreasonable. Additionally, most of us currently rarely watch anything in low-res and we are more inherently aware that a given thing is in bad quality when it is, due to our regular experiences with HD, so we wouldn’t normally be benefitting from that particular extrapolation. I wouldn’t be surprised if this is partially what’s happening in addition to a more general “we just didn’t know better” explanation. People with &lt;a href=&quot;https://en.wikipedia.org/wiki/Anton%E2%80%93Babinski_syndrome&quot;&gt;Anton’s blindness&lt;/a&gt; can fill in &lt;em&gt;everything&lt;/em&gt; and think they are seeing while being completely blind, what is some minor upscaling and ‘hallucinating’ more details after you get used to looking at only low-quality videos and games on your computer or TV since you were born.&lt;/p&gt;

&lt;p&gt;Regardless of whether the above explanation is correct, where does this leave us? It seems clear that we could enjoy these lower in visual quality works before but a lot of us can’t do so quite as easily today. Should we consider re-training ourselves at enjoying those works? That seems a bit tedious. Is this just a (small) price we have to pay for progress and call it a day? Maybe, but it seems like progress might still have us covered. There have been a lot of advancements in AI Super-Resolution and we are getting to the point where it can be used well in &lt;em&gt;some&lt;/em&gt; cases, and things like games and old videos that aren’t in &lt;em&gt;too&lt;/em&gt; bad a quality can benefit a lot from it.&lt;/p&gt;

&lt;h4 id=&quot;ai-super-resolution&quot;&gt;AI Super Resolution&lt;/h4&gt;

&lt;p&gt;&lt;em&gt;Of course, it will be irresponsible not to mention that upscaling and enhancements don’t have their issues. It is impossible to accurately increase the quality of an image without guessing at what should be in the extra pixels. This is typically done by showing a lot of examples to the model and it learning the most common ways to upscale accurately - but if you are upscaling something too different from what’s in the original dataset, &lt;a href=&quot;https://www.vice.com/en_us/article/7kpxyy/this-image-of-a-white-barack-obama-is-ais-racial-bias-problem-in-a-nutshell&quot;&gt;there might be issues&lt;/a&gt;. This is part of the reason why fully-automated upscaling without humans in the loop for serious tasks beyond entertainment can be unwise.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Games, in particular, have benefited a lot from AI-based upscaling - for example, Nvidia’s &lt;a href=&quot;https://www.nvidia.com/en-us/geforce/news/nvidia-dlss-2-0-a-big-leap-in-ai-rendering/&quot;&gt;DLSS&lt;/a&gt; which is used more and more or Facebook announcing their own (near) &lt;a href=&quot;https://research.fb.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/&quot;&gt;real-time Neural Supersampling&lt;/a&gt; using additional data available in Game Engines. Even more relevant, the 1995 Command and Conquer game, which looked pretty good to me as a kid then and awful to me as an adult now has been &lt;a href=&quot;https://www.ea.com/games/command-and-conquer/command-and-conquer-remastered?setLocale=en-us?setLocale=en-us&quot;&gt;recently remastered&lt;/a&gt;, largely using AI to great success! This includes both the playable portion of the game but also the videos and sounds in it. As far as I can tell, this has been met with overwhelming approval and the game is currently in &lt;a href=&quot;https://store.steampowered.com/search/?filter=topsellers&quot;&gt;Steam’s top-seller list&lt;/a&gt;. Of course, this is far from the first remastered game - in fact, the practice is getting more popular - but it uses AI for it’s upscaling to an even larger extent than those before. That and the general trend of reworking old games can paint an optimistic picture where in the near future we might be able to get most old games brought back to life with significant visual improvements and less and less work for the developers.&lt;/p&gt;

&lt;p&gt;What about videos in general? &lt;a href=&quot;https://www.youtube.com/embed/3RYNThid23g&quot;&gt;This video&lt;/a&gt;, originally from 1896 recently made the rounds when it was upscaled to 4k, 60 fps and I have to say - it looks a lot better to me.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://www.sciencealert.com/images/2020-02/4k-train_1024.gif&quot; /&gt;&lt;/p&gt;

&lt;p&gt;There are &lt;a href=&quot;https://www.youtube.com/results?search_query=upscaled+video+ai&quot;&gt;many other examples&lt;/a&gt;,  although currently more often than not you still need a human guiding the process to make sure you end up with a version that doesn’t have too many &lt;a href=&quot;https://www.youtube.com/watch?v=XZVxiak0axo&quot;&gt;unpleasant artefacts&lt;/a&gt;. Additionally, the process is still very compute-heavy and far from real-time. Either way, as algorithms get better, datasets larger and compute cheaper it is likely that in a few years we’ll be able to upscale any old video on demand or at minimum that most older content that is at least somewhat popular will eventually get the super-resolution treatment. Things like browser extensions and apps that do it for you automatically and in real-time might soon become common for those who want them.&lt;/p&gt;

&lt;p&gt;For the time being, you can pay someone to upscale a video for you, e.g. &lt;a href=&quot;https://neural.love/&quot;&gt;neural.love&lt;/a&gt; who made that 1896 train video, you can register for &lt;a href=&quot;https://deoldify.ai/&quot;&gt;various services that deoldify photos&lt;/a&gt; or you can &lt;a href=&quot;https://colab.research.google.com/github/tg-bomze/Face-Depixelizer/blob/master/Face_Depixelizer_Eng.ipynb&quot;&gt;play with&lt;/a&gt; &lt;a href=&quot;https://github.com/idealo/image-super-resolution&quot;&gt;models&lt;/a&gt; directly in Colab or on your own machines. The future looks bright and with the right model so will the past.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="AI" /><category term="image" /><category term="video" /><category term="games" /><category term="super-resolution" /><category term="super" /><category term="resolution" /><category term="upsampling" /><category term="zoom" /><category term="enhance" /><category term="deoldify" /><summary type="html">A topic I end up discussing with friends and thinking about regularly is how videos and games we used to like a decade or two ago looked completely fine back then and are currently unwatchable. I don’t mean this in the more common way of shifting tastes and values but in almost literally barely seeing what’s going on. Videos in 360p and even lower (how is 144p even still an option in youtube?) seemed complete fine to me but now they are not and I am not even able to make out the details. Games that looked like huge graphical improvements at the time and giving us a taste of how realism might look like soon now look absolutely awful to the point where I can’t even tell what’s going on with certain objects (if I can even recognize them) in some scenes. One might say “Well, you are spoiled by much better graphics” and sure, it does seem like it but what does that entail? Why did I use to think I can see things I now can’t?</summary></entry><entry><title type="html">StyleGAN for Evil: Trypophobia and Clockwork Oranging</title><link href="https://svilentodorov.xyz//blog/stylegan-for-evil/" rel="alternate" type="text/html" title="StyleGAN for Evil: Trypophobia and Clockwork Oranging" /><published>2020-03-27T00:00:00+01:00</published><updated>2020-03-27T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/Stylegan-for-evil</id><content type="html" xml:base="https://svilentodorov.xyz//blog/stylegan-for-evil/">&lt;p&gt;GANs have become the default image generation technique, and many are familiar with sites like &lt;a href=&quot;https://www.artbreeder.com/&quot;&gt;artbreeder&lt;/a&gt;, &lt;a href=&quot;https://www.thispersondoesnotexist.com/&quot;&gt;thispersondoesnotexist&lt;/a&gt;, and its off-shoots such as &lt;a href=&quot;https://www.thiswaifudoesnotexist.net/&quot;&gt;thiswaifudoesnotexist&lt;/a&gt;. The generated samples are stunning and quite hard to distinguish from real images, and StyleGAN, in particular, has been used and finetuned by many with interesting results.&lt;/p&gt;

&lt;p&gt;I’ve experimented with them before - for example, the projection of real images onto the StyleGAN latent space, and then modifying those is something I find quite interesting. In this project, however, I decided to go in a different direction and use them for evil.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;All generated horrors in this post are hidden unless you click on them. You can read without looking if you so choose&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Now, some of the more traditional ways of using Generative models for evil are for spam, &lt;a href=&quot;https://www.wired.com/story/deepfake-porn-harms-adult-performers-too/&quot;&gt;deepfakes&lt;/a&gt;, &lt;a href=&quot;https://newsyoucantuse.com/&quot;&gt;fake news&lt;/a&gt;, and &lt;a href=&quot;https://analysis.leadstories.com/3471185-fake-faces-people-Who-Do-Not-Exist-Invade-Facebook-To-Influence-2020-Elections.html&quot;&gt;scams&lt;/a&gt; but we are going in a different direction here.&lt;/p&gt;

&lt;h3 id=&quot;trypophobia&quot;&gt;Trypophobia&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Trypophobia&quot;&gt;Trypophobia&lt;/a&gt; is a common condition where people react with discomfort and even strong disgust towards specific irregular patterns of holes or bumps. For some the effect is quite strong, for others, it is just unpleasant, and some like me can see what the issue is but have almost no negative reaction.&lt;/p&gt;

&lt;p&gt;It so happens that there is a small, but just &lt;a href=&quot;https://www.kaggle.com/cytadela8/trypophobia&quot;&gt;big enough dataset on Kaggle&lt;/a&gt; to train a StyleGAN on.&lt;/p&gt;

&lt;p&gt;Here are a few examples of how the dataset looks:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example dataset 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/dataset1.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example dataset 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/dataset2.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example dataset 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/dataset3.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;The training data is very varied - ~6000 images many of completely different objects that only share somewhat similar hole patterns. Inconsistent data works poorly with StyleGAN - what works best is things that are cropped in the same way, from the same angles and generally the same thing but with different features - e.g. faces, even doing full bodies often lead to blobs (less so with stylegan2). This is certainly something that should be a problem with our dataset, but on the bright side we don’t care if the rest of the image is blobby, we just want it to learn the specific patterns that trigger trypophobia, doesn’t matter if the rest of the image is abstract - in fact, it is better in some ways if it is abstract, as that can make the videos and images more appealing.&lt;/p&gt;

&lt;p&gt;I took one of the pretrained StyleGANs nearly at random - churches and started finetuning on the dataset.
The results even early on looked quite interesting. Here are some from the first hours:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example training 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/training1.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example training 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/training2.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example training 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/training3.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;If you find the above fascinating, you can see &lt;a href=&quot;https://mega.nz/#!eVsC0AjC!6QHzPJMlwuHm5mkxTPOhGsII2ZGycQAv2hE9up6JZYc&quot;&gt;much&lt;/a&gt; &lt;a href=&quot;https://mega.nz/#!KMtljazI!R4KrNNJMsx4wn4OF8L7gf-TQoakTVee3BO-Myfrq4S4&quot;&gt;more&lt;/a&gt; &lt;a href=&quot;https://mega.nz/#!nV0zAKgZ!p-0qpKluSlDkTHfjRHR-IYtEnD5zs3RT5QJqvoZdGME&quot;&gt;at&lt;/a&gt; &lt;a href=&quot;https://mega.nz/#!HU9wXYzL!DAjf448jAXej_38vmmSArDtAV3XHZLXqXvw092DtTBY&quot;&gt;these&lt;/a&gt; &lt;a href=&quot;https://mega.nz/#!WVU12ayB!sNs5_BgT4iW9CTGtUt4QcI9r8ny83eBMvOvaOGHCaOg&quot;&gt;links&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are some video versions.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example vid 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/l_gK9Q__GVY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example vid 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/yj_mAhj2EwY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example vid 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/vvww_rZQVsI&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Responses of the above vary - some are too disgusted too quickly, while others get just a bit of disgust from the worse of it while being intrigued overall, and some don’t get a negative reaction at all.&lt;/p&gt;

&lt;p&gt;I should also note - videos like these might also be useful for good, as a form of &lt;a href=&quot;https://en.wikipedia.org/wiki/Exposure_therapy&quot;&gt;Exposure Therapy&lt;/a&gt; but let’s not focus on that.&lt;/p&gt;

&lt;p&gt;All of the above was trained on Colab using a p100. The final model was trained for roughly 4 days (I saw no improvement near the end), but there were good results within the first day of training.&lt;/p&gt;

&lt;h3 id=&quot;clockwork-oranging&quot;&gt;Clockwork Oranging&lt;/h3&gt;

&lt;p&gt;After looking at the early examples of my trypophobia finetunning, I saw some churches that still very much look like churches while also having trypo features. For example these:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Church AND Trypo &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/trypo/trypochurches-training.png&quot; /&gt;&lt;/div&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This gave me the idea that you can fairly easily combine the two (or combine trypophobia and similar images with something else) to create videos which can make someone associate a concept - in this case a church - with negative feelings a la &lt;a href=&quot;https://en.wikipedia.org/wiki/A_Clockwork_Orange_(film)&quot;&gt;A Clockwork Orange&lt;/a&gt;. I chose to add churches to the mix because that’s what my pretrained model was already trained on, and I had the dataset (it was not a religious statement) .&lt;/p&gt;

&lt;p&gt;The idea (in this scenario we are very evil) is to strap someone (with trypophobia) to a chair and force them to watch those videos which only require some flowing patterns, at least some churchiness and at least some trypophobia-triggering. Admittedly, those videos didn’t turn out quite so well - I had to experiment with training them with different ratios of trypo to church photos, but they (and the possibly better photos above) demonstrate the idea well enough.&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example vid 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/KtZFkdP88tQ&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example vid 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/pvmtJTIKAls&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;
&lt;br /&gt;
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;This was a fun little project, that I spent a few days on. You would need to spend a bit more time to create something actually dangerous (which I doubt this really is) but not that much more. Of course, what I describe as clockwork oranging doesn’t look so useful in the real world but what if you could (and you can) use it to make some images just a bit more repulsive to the general population and others just a bit more appealing with little effort? That seems bad but then again a lot of advertising fits that mold despite it requiring more time per image/video. And at any rate - neither this nor ‘evil’ are near my main concerns with respect to machine learning. So even though I was thinking of finishing this with something about the dangers of AI, I’ll just let people make their own conclusions if there are any to make.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="training" /><category term="code" /><category term="StyleGAN" /><category term="image" /><category term="image-generation" /><category term="trypophobia" /><category term="clockwork" /><category term="orange" /><category term="conditioning" /><category term="evil" /><category term="gan" /><category term="disturbing" /><category term="phobia" /><summary type="html">GANs have become the default image generation technique, and many are familiar with sites like artbreeder, thispersondoesnotexist, and its off-shoots such as thiswaifudoesnotexist. The generated samples are stunning and quite hard to distinguish from real images, and StyleGAN, in particular, has been used and finetuned by many with interesting results.</summary></entry><entry><title type="html">Training Google’s Reformer - takeaways, code, and weights</title><link href="https://svilentodorov.xyz//blog/reformer-99m/" rel="alternate" type="text/html" title="Training Google’s Reformer - takeaways, code, and weights" /><published>2020-03-18T00:00:00+01:00</published><updated>2020-03-18T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/Reformer</id><content type="html" xml:base="https://svilentodorov.xyz//blog/reformer-99m/">&lt;p&gt;A lot of progress has been made in training large NLP models in recent years, and after &lt;a href=&quot;/blog/gpt-15b-chat-finetune/&quot;&gt;finetuning GPT2-1.5b&lt;/a&gt; I’ve been thinking about context windows. Large Transformers are very good, but when you are only limited to 512 or more commonly now 1024 tokens, the model doesn’t know quite that much of what happened earlier at any point. Google’s &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;Reformer&lt;/a&gt; paper introduces a more efficient way of doing attention, sidestepping the problem which I used to train a model with context of 64k tokens.&lt;/p&gt;

&lt;h3 id=&quot;reformer&quot;&gt;Reformer&lt;/h3&gt;

&lt;p&gt;The two main tricks used by the Reformer are reversible residual layers and Locality-Sensitive-Hashing.&lt;/p&gt;

&lt;p&gt;Reversible layers lower memory consumption by allowing us to recalculate layer inputs during backprop based on the already computed later layers instead of keeping all of it in memory. It provides similar advantages to &lt;a href=&quot;https://github.com/cybertronai/gradient-checkpointing&quot;&gt;memory checkpointing&lt;/a&gt;, except in this case the layers are designed to specifically capture the layer-to-layer changes allowing us to recompute more efficiently.&lt;/p&gt;

&lt;p&gt;More importantly for my purposes, it also introduces Locality-Sensitive-Hashing (LSH), which allows us to approximately compute attention much more cheaply and thus work with much, much larger context. In short, LSH maps similar vectors together rather than working with all possible pairs of vectors (which is what makes attention so expensive). In the cases where we have many of the same/similar tokens, a lot of them will end up with the same hash so they get chunked and then attention is applied in parallel on those much smaller chunks.&lt;/p&gt;

&lt;p&gt;If you are interested in the specific check out &lt;a href=&quot;https://github.com/google/trax/blob/master/trax/models/reformer/reformer.py&quot;&gt;their code&lt;/a&gt;, &lt;a href=&quot;https://arxiv.org/abs/2001.04451&quot;&gt;paper&lt;/a&gt; or &lt;a href=&quot;https://ai.googleblog.com/2020/01/reformer-efficient-transformer.html&quot;&gt;blog post&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Google Research released an implementation of their Reformer model as part of the &lt;a href=&quot;https://github.com/google/trax&quot;&gt;trax library&lt;/a&gt;, which is easy to use. Given my interest in NLP models with larger contexts, I decided to experiment with it and pretrain the largest model you can fit on a Colab TPU - with the idea that this way others can take it and finetune it on their tasks in cases where large context is important.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Here is my related &lt;a href=&quot;https://github.com/Tenoke/trax-reformer/&quot;&gt;repo&lt;/a&gt;, &lt;a href=&quot;https://mega.nz/#!SR9WySCC!K2sdePWHDq2QiHlom8ZMLgKeBCcjQxsBToxl9NpNhpg&quot;&gt;link to the weights&lt;/a&gt; and &lt;a href=&quot;https://colab.research.google.com/drive/1VSaQingkpMDf730ARlo2dw5vCBVOgt17&quot;&gt;Colab&lt;/a&gt; link&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;First I used a small amount of the data to play with Reformer. I started with their &lt;a href=&quot;https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb&quot;&gt;code for training on Crime and Punishment&lt;/a&gt;, however, I just couldn’t get it to go much beyond .3 accuracy no matter how much data I fed it. It seems like the parameters used there were mostly chosen for having a huge context but the model couldn’t learn much. After a lot of experimenting, I ended up with parameters closer to their &lt;a href=&quot;https://github.com/google/trax/blob/master/trax/configs/reformer_enwik8.gin&quot;&gt;enwiki8 training run&lt;/a&gt;, I ran about 50 trials with small adjustments and chose the version of the parameters where the loss was going down quickest. You can see the final values I’ve chosen in the &lt;a href=&quot;https://github.com/Tenoke/trax-reformer/blob/master/configs.py&quot;&gt;repo&lt;/a&gt;. &lt;strong&gt;The overview is that we have ~99m parameters (compared to the small gpt with 117m), context window of 65536, 0.2 dropout, 12 layers and a vocabulary of 256&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;After that, I decided to train it on all of Wikipedia, which I downloaded and preprocessed to make it easier and faster to use with Colab and Drive. I downloaded the &lt;a href=&quot;https://dumps.wikimedia.org/backup-index.html&quot;&gt;most recent Wikipedia dump&lt;/a&gt; and processed it into JSON while excluding all articles under 1000 characters - we are training for large contexts, so including tiny articles would be counter-productive. Then I split the big JSON file into smaller files of 5000 articles each - because Drive can lock down your file if it’s too big and you access it too much. &lt;a href=&quot;https://colab.research.google.com/drive/15WCdIYju3A4UOY1-JGt7NTlw0jMLJYXZ&quot;&gt;Here&lt;/a&gt; is the Colab for doing the above.&lt;/p&gt;

&lt;p&gt;Next, I wanted to pre-tokenize all my data so I can load it to my model quickly rather than doing it on the fly. One thing currently not added to the Reformer (but is on the list) is handling bigger vocab sizes, like the ~50k GPT2 vocabulary. Given that our context is going to be huge, and that the vocabulary size that enwiki8 config uses is 256 anyway, I decided on a character-level ByteLevelBPE tokenizer from &lt;a href=&quot;https://github.com/huggingface/tokenizers&quot;&gt;huggingface’s tokenizers&lt;/a&gt;. I regretted it a bit, as there is little documentation in the library, and there are weird things in it like being unable to easily add extra tokens, or save the tokenizer - it took me a while to realize that saving after adding extra tokens does nothing, as they just save ‘the model’ as you’ve trained it, so I had to manually edit the vocabulary I pass to it. &lt;a href=&quot;https://colab.research.google.com/drive/1letuN1YbhI8cxJIL059Bp7J6ga_kXkeP&quot;&gt;Here&lt;/a&gt; is the Colab for tokenizing and saving all the preprocessed data. Again - we are tokenizing in advance instead of on the fly to speed up training.&lt;/p&gt;

&lt;p&gt;I wanted to take advantage of the large context so my function for feeding data to the model specifically always loads full articles and adds padding up to max the context length. The data is all tokenized and ready, so we just load preprocessed files at random and pick random samples from them.&lt;/p&gt;

&lt;p&gt;After all that I trained it until the loss wasn’t going down for a while so I stopped a little after 125k steps. The accuracy at that point was a little over 0.75.&lt;/p&gt;

&lt;p&gt;Here is how an example article from the dataset looks like:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example Training Article &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
{&quot;title&quot;: &quot;Bill Bayliss&quot;, &quot;section_titles&quot;: [&quot;Introduction&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;'''William Bayliss''' (19 December 1886 \u2013 12 February 1963) was a British trade unionist.
&lt;br /&gt;
&lt;br /&gt;
Born in Leicestershire, Bayliss left school at the age of twelve to work at a colliery.  He joined the Leicestershire Miners' Association, but after becoming involved in industrial action, he was sacked and, a year later, moved to Nottinghamshire to find work.  There, he became active in the Nottinghamshire Miners' Association, and also in the Labour Party.
&lt;br /&gt;
&lt;br /&gt;
Bayliss became his union branch delegate in 1915, and served a year as vice-president of the union in 1927, and a year as president in 1929.  In 1932, he became the union's full-time financial secretary.  In this role he worked with Herbert Booth to promote reunification with the rival Nottinghamshire Miners' Industrial Union, which was achieved in 1937, whereupon he became an agent for the merged Nottinghamshire Miners' Federated Union (NMFU).
&lt;br /&gt;
In 1943, Bayliss was chosen as one of the Trades Union Congress' two representatives to the American Federation of Labour.
&lt;br /&gt;
Bayliss was elected to Nottinghamshire County Council, becoming an alderman, and serving as its chairman from 1945.  In 1946, he was elected as President of the Nottinghamshire Area of the National Union of Mineworkers, successor of the NMFU; he served until 1952, when he retired and joined the National Coal Board.
&lt;br /&gt;
&quot;, &quot;
&lt;br /&gt;
&quot;]}

&lt;/details&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h3 id=&quot;sampling&quot;&gt;Sampling&lt;/h3&gt;

&lt;p&gt;I got okay samples from it when they were less than a few thousand tokens long but if I tried to get a very long one it often either turned repetitive or turned into gibberish after a while. Part of it was because in the training data - ‘}’ denotes the end of a sample and it hasn’t seen anything after, so after it outputs that it just doesn’t know where to go, which is reasonable. However, part of it was just that it is easier to enter into repetitive sequences in longer samples as it has more options to do so.&lt;/p&gt;

&lt;p&gt;For that reason I experimented with adding top_k, top_p, temperature, using exp2 instead of exp on the logits so they’ll be closer together, and a few random things including a Meena-inspired approach of generating multiple samples at each step and using the probabilities of the best one (also multiplying multiple samples’ probabilities). I’ve included some of the more useful ones in the sampling function in the &lt;a href=&quot;https://github.com/Tenoke/trax-reformer/blob/master/sample.py&quot;&gt;repo&lt;/a&gt;. 
After adding sampling options, I generated 8 samples for various options (279 in total after excluding all options which were more obviously subpar) and wrote some testing code to show me one randomly and ask me to rate it. This was very tedious and I wish I had set more rigorous grading rules, as I did the grading over a few days and ended up not being as consistent as I would have liked. I’ve included the ratings of the settings I tested &lt;a href=&quot;https://github.com/Tenoke/trax-reformer/blob/master/rating-args.dict&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Here are various samples at different settings:&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;2000s&quot;, &quot;2001s&quot;, &quot;2002s&quot;, &quot;2003s&quot;, &quot;2004s&quot;, &quot;2005s&quot;, &quot;2006s&quot;, &quot;2007s&quot;, &quot;2008s&quot;, &quot;2009s&quot;, &quot;2010s&quot;, &quot;2011s&quot;, &quot;2012s&quot;, &quot;2013s&quot;, &quot;2014s&quot;, &quot;2015 &quot;2016&quot;, &quot;2017 &quot;1919&quot;, &quot;See also&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;The '''Best Possible Choice of Discussion''' is a public biology focused on personal Strategy at St. Gambridge University in San Diego, California that encompassed students to the Greenwich Church in Melbourne).
&lt;br /&gt;
&lt;br /&gt;This is a third choice on a discussion in the former Greenwich church, however, it is noted for its original way to be Ethnographic and the Greenwich known as Greenwich MacLeod, its lineage. The biological analysis of Greenwich, the United States and Ottawa choices translated into Melbourne.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;As of July 1, 1997, the choice has a plantation service over the dozen St Gambridge Battle in Discussion, Melbourne, Perennial Bennett, Illinois, in addition to changes performances in Strategy, tax appearances, and cultivation. It is featured in several post-appearance works on cultivation choruses, including ''The Coventry-Commercial Voices'' (1969), ''The European Duke of Blood'' (2001), ''Jesus Highlighting'' (2005), ''A Goal of Blood'' (2009), ''The Choice'' (2010), ''Voice of Blood'' (2011), ''Large Glaze'' (2011), and the ''Journey'' (2012).
&lt;br /&gt;
&lt;br /&gt;The choice won their first First Appearance against Strategy at Blood's Third Appearance in 2012 and finally received the Cup Awards in 2007.  This public draws works from the Best Possible Choice to speak Live Arts and Best Speak Actor for the Year ''Voice'', and Co &amp;amp; Bavarian Prize. In 2014, the choice was opened to 2,500\u00a0boys (75%)
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;In August 2014, Best Possible Choice inked the comedy-first as a sales offense that would preserve its national appearances.
&lt;br /&gt;Religious affection
&lt;br /&gt;
&lt;br /&gt;===Choice and Choice===
&lt;br /&gt;The choice usually took place at the Football Hall of Fame in New York since 2000. The choice was sequestered by their position on June 30, 2003.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;Choice was founded in South Korea, and between 1996 and 2001 (with Alberto Chunter's areas being cast on Melbourne).
&lt;br /&gt;
&lt;br /&gt;===Mini-videos===
&lt;br /&gt; Journey together with others in Melbourne
&lt;br /&gt;The choice was a recipient, and played on the Mini-video of the Year ''Voice'' (1996), with Deine Harrison of the Episcopal Church, and is the youngest part of the Choice and Choice position at the Choice Mini-video's parish.
&lt;br /&gt;
&lt;br /&gt;===Competitive show and performance in Finland===
&lt;br /&gt;In March 2012, the first assembly versus the second assembly, ''Finternational Career''. On April 19, 2012, the NPS public set to collaborate for the History of Western St. Gambridge's, bonus transfer. The exposure of one sense corresponds to the department at the Football Hall. When Football Hall is called the ninth-legged F.A.N. at Blood's Third Appearance and winners his career in the end of the 2013\u201314 season, the choice was in the contest.
&lt;br /&gt;
&lt;br /&gt;==''Zeller North''===
&lt;br /&gt;In January 2014, Ola Schmott. The choice would become his last successful and is also won by Christopher Schneider. On December 6, 2014, the choice finished 8th, announcing that it would not replace the brand.
&lt;br /&gt;
&lt;br /&gt;===Crew Diagnostics===
&lt;br /&gt;The Crew Diagnostics included Crew Diagnostics, Bass Young and Drummond Diagnostics.
&lt;br /&gt;
&lt;br /&gt;The final session of Crew Diagnostics
&lt;br /&gt;&quot;, &quot;'''First Appearance Choice in Eastern St. Gambridge''', was released in 1996. The second assembly was dealer in a Ninth English football competition, with Mennonite Scott Chalce.  Sold in 2005, the Choice had a row with 11.5%. In the Melbourne Football League, the choice became a record of 120 or 18.4% of the vote. Mennonite Scott Chalce repeatedly chosen the choice\u2019s primary industry, but short of clinics, flagsellers did not improve the select potential on the basis of Blood's third Melbourne race there.
&lt;br /&gt;
&lt;br /&gt;===Infrastructure===
&lt;br /&gt;In July 2015 \u00a7\u00a3240,000, Fontier Dr Boris, the choice was a friend of Robert Rogan and Dave Nicholson.
&lt;br /&gt;
&lt;br /&gt;On February 17, 2016, the Ola Liga of the Art of Fontier's Choice and the following of Competing Knights on December 37. 
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;Spencer Fault hosted Rufous Hosts in the fall of 2018 and they made his own speeds as greatest choice for far-voice football players in Alberto.
&lt;br /&gt;&quot;, &quot;'''History of Western St. Gambridge''', is a Choice of the State All-Star public consequence on August 12, 2016 (U.S. Army) and is the first former administrator of countries to hold the Football Series five years later, an original Israel football team that came out with Melbourne at both underground. This time it was a popular veberation.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;===Competitive show and choice===
&lt;br /&gt;In September 2013, the preceding choice announced nearly $100,000 to . 
&lt;br /&gt;
&lt;br /&gt;In the early 2016 Deiner Harris announced that it would be called the Football Choice. On May 27, 2017, the choice was announced. It would be called the Choice Mini-video of the Year (Player of International Agreements) and that the Choice Mini-video winners were announced that they had recorded nine boys, they were announced that the Choice be found. The Choice Mini-video was funded on June 30, 2015 by the Royal Magic. It is one of first segments to play a permanent role in the Army.
&lt;br /&gt;
&lt;br /&gt;In its Knights of Western St. Gambridge played for Live Arts. In January 2019, ten teams from Lantern Army were ranked to the ''Hotspot'' in the future. The Choice mini-video was ranked as 15th in the 2015 NCAA Division I Movement Awards, but Live Arts took place in \u00a7CAA Tour d Social Team at Glumbo County, California, in February.  At the end of 2018 the choice returned to the Choice and Choice, but Live Arts have converted just 13,000 years proved that she went on to drop in the future, a.k.a. City in Boston. Rufous Hosts has moved from Brilliant as he's the second-governor the Choice Evangelical Football League, where she and her husband, Rufous Hosts. When Rufous winning another an ASL that campaigned for the Year, she said \&quot;It was going touching throughout soccer in Melbourne, where burghed into a museum in Manhattan. It was said that she \&quot;repeated the buenos Publishers and whose book in the museum's halls about the architecture of the scotch and more future irregular ability to spread to them on a new, installation, multiple grape shock lines.\&quot;
&lt;br /&gt;Recording of the 2019 Western St. Gambridges, although speaking at the U.S. Army won the medal of the season.
&lt;br /&gt;
&lt;br /&gt;The artist received more than 750 unanimous players, and they became appointed One Football Conference (OFC), which included Conference Football League Football Conference (DFLFC), for future research international monitoring football teams. When Nicholson used the DFLFC as a public score, proved to be sponsored by DFLFC finishers as going to any province.
&lt;br /&gt;
&lt;br /&gt;===College career===
&lt;br /&gt;The College career was opened on May 6, 2016, three weeks later in Shootout contesting Conference State All-Star End for managing Delaware National Team Graphic and the Choice Mini-video.
&lt;br /&gt;
&lt;br /&gt;Alchemy was launched in 2009, where Best Actor at Paris Football League, a slowly engaged actress, was stated in activities until choosing to acquire U.S. Army parties.
&lt;br /&gt;
&lt;br /&gt;Although they were translated into the U.S. Army, the choice was entirely repeated in APR before including the \&quot;Coventry Choice\&quot;.
&lt;br /&gt;&quot;, &quot;===Crew Diagnostics===
&lt;br /&gt;In January 2015, it was announced that the following of the choice's hall, Alberto Chunter, died on April 1, 2016. The last one segment followed the weekend ASL officer, Tom Giglotto, in which William Guey of Jesus Hosts was troined in a magnificent actress.
&lt;br /&gt;
&lt;br /&gt;The choice was adnated for Rufous Hosts which purchased the Melbourne Football League to stop the Choice's Office Series B. Almost received her B.D. directed by Main Crip Brown. This was managed by Robert Rogers's first book took place on October 3, 2016, in Hillsboro Texas. In 2017, she presented the ''BattleGas All-Weekly'' textbook.
&lt;br /&gt;
&lt;br /&gt;In October 2018, Publishers' mother, Rufous Hosts presented the Choice Gas All-Weekly's mother
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;'''Best Possible Choice''' was an English-language possible choice figure which opened on 9 September entered into France, where it was part of a treaty of inheritance; leaning that she would keep the subsequent sciences possessively on those who take over a topography of the figure.
&lt;br /&gt;
&lt;br /&gt;In wealthy, Best Possible Choice remained built for its urban disusing personnel, taking them away off howse they go off her. Aviage decisions happened on other Confederate Articles, Official Enterprises and the Paul Buildering Centre, remains praised until authority through \&quot;airdate the modern-day jazz family\&quot; extremely. Official English published release, ''What Goes to Weather'', found in positive ''How to Make Goaltender: Late Without You Are not Prisoners'', after Red &amp;amp; Wilson spends most of its ghost ones on Best Possible Choice.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;* Alan, William and other families
&lt;br /&gt;* Feyland, constituent comic books
&lt;br /&gt;* Great Constituency
&lt;br /&gt;* Kevin Jackson
&lt;br /&gt;* Ikey, Mains, Boulevard, New York, Massachusetts
&lt;br /&gt;* Glorian, Onford
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;* What Goes to Weather, away of Christmas Tane, of Youtube Possession
&lt;br /&gt;* Macmillan Whatsoe, Baptiste Brussels for Los Angeles dynasty
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;'''Best Selling One''' (; born 15 February 1998), better known in New Zealand, birth, chemist, geologist, and former chemist and country series. He later produced another selling known on current warship and instrument, MWC, with a new identity, within a fellow instruments Joy Kushmore, Matt Aswar and secures roles in an an-holiman's high ransoms system. 
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Best Possible Choice Instrumental Practice
&lt;br /&gt;
&lt;br /&gt;Best Issue Choice (formerly known) style
&lt;br /&gt;The style, one of several adapted choice service providers, hitherto provides shutting down the service to a new announcement. The addition was used in purposes of relevant usage. It would thus require an individual that became the main aqueduct of the series5. The combined work announced that Geologists took place at Tigers University and would then attend Corkup College, KO Coast Department from Bynamarkis, Georgia (now First Service). These would New Zealand also provide ethnological and marketing.
&lt;br /&gt;
&lt;br /&gt;In 2007, a NYCDE publisher opened many primary companies, hitSmoke, MA\u2019s Additional Organization. It was adided in 2008, with many Identities, as a working working corp company in Stanford, Illinois (UK) who also published several other choices as well. 10 producers hosted and posted molecules for the ''Water Years'' Animation with the UK Corporation of Honors.
&lt;br /&gt;&quot;, &quot;All in his honor appeared on the ''Radio 2 Theme'' Series, showing the work for Artist of the Museum's Expeditionary Organization. However, it finally penalized choice to control the series CFA in 2009.
&lt;br /&gt;
&lt;br /&gt;In 2011, lost of the Artist in which he became able to recognize that news-climbing controlling Stevens' Jaga system, which attracts style of students attending Aswar e Raven. Best Artist also requires Artists and Metal Summer 2014.
&lt;br /&gt;
&lt;br /&gt;In ''The Home Series'', Best Possible Choice Instrumental Practice was released onwards on 19 April early October 2016 and won this TV challenge to create several warning features: ''Among the Stars About the Doesn't Radio/TV''.
&lt;br /&gt;Physically, by Director Don Landel, announced that they would be replaced by ''The Home Series'', which trailed they would become highly prolific and lost in and stationed on their home series's ''Vibe Moulin'', but it also reorganized ''Vibe Moulin'' by Attila Crow. The company's approach is based he more followed by Israel and Corkup and represented his investigations to the best possible choice. This called the department \&quot;in documentary Choice to hand it, damage for Choice offering everything in museums of an Expeditionary Organization to museums with a provider in Illinois Maritime Authority\&quot;. New Zealand examines Corkup mainly a staff on a further choice using a government image of the Greater Kings.
&lt;br /&gt;
&lt;br /&gt;After the winning Stevens' Jaga month, Jimmy Elliott subsequently published an IDE fellowship about the public industry during conflict with Lawrence Holmstead, Louis Xavier Brunham and Lwarten Holmstead, a postmodern dispute, Nazi Germany's own own which had been except for the Piano Strategy Society of Amsterdam that was purchased by their dodger's last tour of New Zealand along with the elternate supporters of the Amsterdam family.
&lt;br /&gt;
&lt;br /&gt;After the war, Best Sellings One has unaltered success warnings that create expeditionaries from independence. One supporter of his doctoral career, the alternate from the family, replaces Brunham Sandra included Caledonia politicians during the 2015-06 World Headquarters. Museum also listed His Headquarters has held that more of famous artist, than Latin, Matt Aswar, and portraits. 
&lt;br /&gt;
&lt;br /&gt;It would help be adapted for his official chorus ''Vholtk'' in the Metro Lions. In 2008, ''Quality Safe'' later presented a complete news and named ''Sa're\u00df \u00c8re Solm'' in Austria, which would have a hit-size short time and come to sex gifts called Choice. 
&lt;br /&gt;
&lt;br /&gt;In 2008, they would play the self-published book, ''Life Software'', steadily made ''The Restoration Man''.
&lt;br /&gt;
&lt;br /&gt;In 2011, CEO Richardson appointed Geologist as popular as the missile force of his New Zealand  missile choice rather than zinc, bringing in the fact that the NYCDe basic activity or focus on consumptive home series filled with Aswar.
&lt;br /&gt;
&lt;br /&gt;Best director or director of the series is Defo Falson, the second book's New Zealand highly big audience, First edition of ''Best and Highly Illustrated Sequels of Variations''.
&lt;br /&gt;
&lt;br /&gt;As a Paul Dacpary, Carl Arkeline, tying how the controversiality was registered as a disjoining occupation in 2013 and registering Joy Kushmore in New Zealand who is universal with Joy.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;* 
&lt;br /&gt;*  VGBrooklist History Website
&lt;br /&gt;*  AWC Award 2019
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;
&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 4 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;US involvement)&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;Best involvement
&lt;br /&gt;The '''Best Possible Choice''' was an unusual post which started in September 2002. It peaked at number 23 on the ''Billboard'' Hot 100 on \&quot;The Best Possible debut and the Reality-Free Offensive.\&quot; It was formed in March 2003 by a Best Possible Choice nominated for the Best Possible Choice of the Year in May 2003. The bonus fit on both fits was extended across Europe and October 2005.
&lt;br /&gt;&quot;, &quot;Best Possible Choice used an act for public and a weekly magazine. By April 1994 the Best Possible Choice had to be previously involved. The bonus was nominated for a \&quot;Popular Clinical Awarded First Bonus\&quot;, and for which it had a perceived \&quot;star form of a Best Possible Choice features the opportunity to solve milks and life by awareness.\&quot; The bonus was delayed on May 24, 2005 and showed a total of twelve choice on March 29.
&lt;br /&gt;
&lt;br /&gt;After completing bonus Best Possible Choice Best Possible Choice Board and signed a first possible choice best possible power in the Pact Jubilee after 11 years of junior team Norwich Howe joining the Choice for a transfer to October 2005. At the Best Possible choice, a Jubilee affiliates were supermarried by Best Possible Choice, with Alex Maw due to a Best Possible Choice, had been delayed in the semi-primary attitude. They were watched by Johnson Xavier on August 24, though he was exploited from the Choice in June 2007 during the campaign. The test was not only opus in the Best and had a loss of the choice before the test was exploited from the Best Possible Choice, where Turkey was replaced by a no reason for possible choice. Best Bonus TV and Crime Day was assigned into Best Possible Choice.
&lt;br /&gt;&quot;, &quot;1992, OS Louis Hart was promoted by Mike Oscar who was awarded \&quot;Leaving the Best Possible Choice\&quot; for his work on the day. He became the Reality-Free Offensive of the Pact Jubilee in 2009.
&lt;br /&gt;&quot;, &quot;* Norwich Howe \u2013 vocals, guitar
&lt;br /&gt;* Best Actress \u2013 vocals, bass, backing vocals (in the US)
&lt;br /&gt;* NCAA Main Film Festival \u2013 vocals
&lt;br /&gt;* Deburst Crime Day IPC-Festival \u2013 (bass guitar)
&lt;br /&gt;* October 29, 2005 by Mike Oscar and Norwich Howe \u2013 guitar
&lt;br /&gt;* January 20, 2005 by Mike Oscar and Al Jackson / Grand Regional Film Festival \u2013 guitar
&lt;br /&gt;* January 29, 2005 by Bob Beckett &amp;amp; Grand Regional Film Festival \u2013 guitar
&lt;br /&gt;* January 29, 2005 by Feibergio Val and Francisco Stevenson &amp;amp; Strucco Geoffrey Jr. \u2013 guitar
&lt;br /&gt;* February 29, 2005 by Laurence Griffith, Best Possible Choice
&lt;br /&gt;* January 30, 2005 by Rich Marie &amp;amp; Leonid Fegg, Jr. ''The Complete Event'', the winner of the choice
&lt;br /&gt;&quot;, &quot;Grand Regional Film Festival was doing analytically released on October 29, 2004.  Obviously, the audience of the high school exhibit in 2011.
&lt;br /&gt;
&lt;br /&gt;At the CD, the bonus was hosted by Norwich Howe's area as possible. At the CD, the choice also adopted category for each possible.
&lt;br /&gt;
&lt;br /&gt;Between 2011 and 2010, the choice's guitar was presented at the 2008 Orlando Dt. Toronto Stadium in Oscar.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;===CEOs===
&lt;br /&gt;* Kenya Rockefeller &amp;amp; VIII.
&lt;br /&gt;* Lille Flapper: Lord Theatre, Milwaukee Senator
&lt;br /&gt;
&lt;br /&gt;===Videos===
&lt;br /&gt;* Ashman K a son, William Lee Ingalon, Jr. \&quot;Some Time Today Who Must too\&quot; (with a character in ''The Best Possible Choice''). The film received a video of video of videos, directed by L.D. McCartney, D.S. Ficker and Klaus A. Wood, and Michael Garred, one of the best Possible Choice for the Year.
&lt;br /&gt;
&lt;br /&gt;When Garred was controversial, Best Film adapted the photograph of Best Possible. The next year TV, which moved to Louis Hart, and received a previously worked role of choice awards for invitation of his stereotype before he met Demi Film and Best Film Award for Feature's Performance for the Year in the same film. He displayed a narrative folk music style, in Smithsonian and he also performed the photograph of the Best Era, which controversially caused the demi Film Festival.
&lt;br /&gt;
&lt;br /&gt;===Album hits===
&lt;br /&gt;* ''The Bonus War Hour: The Theatrical Scenes and Sound Stories'' (2003): ''A Media of One Way in the Shadows'' (2001); ''The Courtier Achievement Truth'' (2001); ''The Oblast Possible'' (2001); ''Tomorrow Camp'' (2004); ''Lady of Key War'' (2009); ''Motherkelp I'' (2009); ''Solos Armor'' (2012); ''Laudies and Things Idol'' (2014); ''Solos Armor'' (2015); ''Warning Inn'' (2016); ''Future'' (with); ''Soul Next Last Mother Joy'' (2017); ''You Think I'' (2018); ''Garrett Harriet Songs'' (2019); ''Concert Richard Garrigan'' (2019); ''The Concert Richard Garrigan'' (2017); ''A Lovely Kind of Dawn'' (2021); ''Concert Richard Garrigan'' (2021); ''Silver Jerry Ladren Flint'' (2021); ''Soul Next Last Mother in Soul'' (2022); ''Blood Roads/Cites Saw'' (UK); ''The Footscience Jerky'' (2022); ''The Vail'' (2023); ''A Bustle Canlock'' (2024); ''Cosmic Truth'' (2026); ''What Foot Gives You to Be a Science Fiction Fiction'' (2026); ''People from the Great Child'' (2027); ''His Home Man'' (2016); ''Bad Among Its Witness'' (2017); ''Director of Music'' (2017). She has no hits, including his friend Morris Francti (posthumously) and on the same comedians (Produced by Michael Lawrence and Linda Hoffman).
&lt;br /&gt;* Kenneth S. Presley, Jr. (painted by Anthony Desla.) (54450)
&lt;br /&gt;* Ray S. Wood, Jr. (painted by Julie Reeve) (533229)
&lt;br /&gt;* Tom Hill, Jr. (painted by Jamie Bosas) (82624 and 15520). \&quot;It is The American Hero of the Hero, His Legation, In Best Actor of The Actor, 500 AMS Milks \u2013 (ANT BCO).\&quot;
&lt;br /&gt;* Vicky Warren, Jr. (painted by Best Supporting Actress, 51375).
&lt;br /&gt;
&lt;br /&gt;===March===
&lt;br /&gt;* Mike Oscar directed by Grand Regional Film Festival \u2013 (Marty Solvogni) and produced by Jerry Judiciary in a World Film Festival from 2002, unlike he was signed by Best Supporting Actress
&lt;br /&gt;* Miccabi Rickleton Wye and Irish Rangers, who presented Best Possible Choice
&lt;br /&gt;* Kim Young, Jr. (painted by Richard Garrigan)
&lt;br /&gt;* Nick E. Sullivan, Tim and Therapist (co-writer) and executive additional producer and Sleepy Mermaid (the primary producer and co-writer)
&lt;br /&gt;* Study of the Demi Film Festival (1982), Wundergoing University
&lt;br /&gt;&quot;, &quot;There was an Official Film Awards as he trying to his first official film about the film to start the short film ''Mike Oscar Mike'' in 2001.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;*
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 5 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;Tenth-best Choice in Design&quot;, &quot;References&quot;, &quot;Bibliography&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;'''\&quot;D'East\&quot; is a based art by Kinetic Sanskrit for the photographers of the Na Handbone. The subject was reissued in its explosion which Michael De Olympic teemed not by Raymond Spirit.
&lt;br /&gt;The based includes the community \&quot;produced and,\&quot; this sequel, attributed to John Pleming be, decided.
&lt;br /&gt;
&lt;br /&gt;The ancient site included traditionally, particularly the Dandynamik cycle, dubbed Ex Layoh Henrembrig, the group's historic conflict will manage. The twentieth-century Best Possible Choice in Design has previously been very assigned, a pwotoken media since February 2005 when Dandy Spirit had served as a trial nephew.
&lt;br /&gt;
&lt;br /&gt;Best Possible Choice has been as of December 2014, but it has continued to extend in dozen violence: \&quot;When Firsterior Nowhere\&quot;, \&quot;DWN\&quot;, \&quot;Best Possible Choice\&quot;,\&quot;The Designer's Bend,\&quot; the TKX44's stained horror mini-building, despite offices, and stacked mined space industry. J\u00f3dsfar Robson wrote \&quot;The bend's dark-breathing poisoning articles alias that went on to while this was further originally, possibly party messaging. Ketchens and him/sioux-Prayered Layoh Mona\&quot;, didn't quickly receive several other FLFT.&quot;, &quot;Best Possible Choice made on on-the-plantation by him the Platform's Girl, Designer of Horror Mona, fianc\u00e9e Trompelis, first producer of Coffman's ''Best Actress''.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;An influenced modern day, Dandy Fobie-Garrison (parts of Eskimo-Lucisne) opened the Michael De Olympics naval appearance in ''Bank of Monkeys'', striving on phase being used in sound downloading artists in over 50 companies over 35 country competitors. With this record, the gorge weekend for sound conflicts drug while leading off-sixth times, Dandy later changed to James Barrow. Striving out, his business contributed to these working appearances and join Westlake's assent twice, focusing on an experience and stated in justiciary.The two, James Gregory, Syndicate Neighbour (with Jeffrey Robinson) gained Michael De Olympics participation in over several English ideology.
&lt;br /&gt;&quot;, &quot;Located alongside John Denver, the based famrous stained horror for his team showed, although it began on 1 July 1994 when a museum, tomatoes and pigmental flightsteps paths in the violonce thereafter. The based fi.gov stated, this sequel gave showers to Taylor Merlbrook inhibitions. This stated that his MCTs in Taylor, such as weapons, example Tates, were designed as a current newspaper, filming at rivalry invasions for his lower number of Isles and James Patterson. A conviction for the Science Play, bor Conftiguous Play struggle from Hollywood's Ladwy Str\u00f6mer, business conflicts hosped until his own mini-building, later dissidentially its first way, Accordiita Philosophia.
&lt;br /&gt;
&lt;br /&gt;In the following year, introducing the creation of three newspaper, dominated any team by which an information highlighted by women got a low special creation of cremen. Best English type of writer Michael Duncan, Bennie Rormi, Best Emerald, and Michel Matthews were already classified, such as weapons, a spybe or poor spin, was trucks because of Westlake's Prayer. Mindi's Presence of Beyond, Ronke Balmond Construction and Emmily Milne, Daniel Erb, Delas Banks and Dudulph Bla\u2019s GDFs designed and new data by Edward Doimylt. For many years, Best Edge Group created a 20-ticket run, also has 500 brakes.
&lt;br /&gt;
&lt;br /&gt;A number all-flight \&quot;DEYATH\&quot; took place at Gracepool's New York City in 2007. In the course of this game he poeted a riding, and continued to say that Gracepool has blamed the formulation of Best Artist. Gracepool was declared to become one of the best artists. Before returning, Best Artist wrote a number of works with Coffmann the Possible Choice art as part of the \&quot;Wormington Bomb\&quot; franticled the off-sixteen geographers who had later been later included in \&quot;Tunes' Causes\&quot; in his 1995 \&quot;Adu\&quot; Amazon, both printers director Donald Brunsins and executive producers creator director Behanaire Lewis, virginia pioneer Saria Cappling for Garon Gun and new special themes. Carpentier Teas expired Gracepool's respective stints which later put \&quot;a bare remarkable hypoxy in the Democratic Lawn budget.\&quot;
&lt;br /&gt;
&lt;br /&gt;&quot;, &quot;Neighbour changed its concept for the first time. Shortly until the period Neighbour sympathetics with help freee Holiday: the depceived contour of the policies of this statistical social miseries was asked by commemorating Lillow Ramshid as his Lord Co. Tank system; these were all accredited, ministered, levels, diplomats, excursions, and commemorations. Recordings his MPP traditions are with nursing two-pounded maestros, making first overaccommodation to creating an 80 Beyond cycle. Dandy Fobie-Garrison pioneered this time on one time hidache. This cottage citemen also spent everything or rising when trust wrote that girls are accepted. The others die an old vicious off-sixteen-man whose yeast of creating drip outmutter. A man calling the \&quot;African-American\&quot; idylline connection to success, as the low-grandmother site with electrical and indices. Construction of a pAF was heroes, tanking for tricks, spiritual dress candidates, peace, rocks, and gradees.  Best Artist missed the base, symbolicfishing, labourage, hikers, and culture at Barbados.
&lt;br /&gt;&quot;, &quot;At the 2002 Universal Opera Social Service, Desfertory\u2019s citizenship is ruled. Festival of the 2004 Universal Megs of Hollywood pioneers started leadership service professional.
&lt;br /&gt;&quot;, &quot;''Almost important in social miseries'' articles carried out including: \&quot;Fact so Burrow celebrates the disease of local exorcism in the cycle, and leads follow, or we can't develop local their autonomical respects.\&quot;
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;*
&lt;br /&gt;*\&quot;The federal This Is the Best\&quot; (1984)
&lt;br /&gt;*
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;*  Best Choice Nations Video Film Company, Official Website
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 6 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;'''Best Possible Choice''' was a Turkish choice specializing in the \&quot;Best Possible Choice\&quot; for the choice and \&quot;Best Supporting Possible Choice\&quot; specializing in the first time. The possibility of the possible choice would reveal to a choice that would be held on the Best Possible Choice to win the Best Possible Choice.
&lt;br /&gt;&quot;, &quot;The choice's first week was held at Warner Arena, a style game from 1931 to 1942 and would receive a regular award from the University of South Carolina in 1952. It also received a Gold Star Award from the University of Pennsylvania in 1956. The first World War I series began doing work in August 1967.
&lt;br /&gt;
&lt;br /&gt;The Harvard Prize was given its own way in 1972. The award was one of the first female and received a positive recognition from the Old Hospital of London for hundreds of years, the Parish of London for hundreds of years. The World War II began his position as an award, the first World War II began his way in 1979.
&lt;br /&gt;
&lt;br /&gt;After South Africa he was on the Council of the World War II (1982-1989) and became a position of officers in 1989. He was elected a fellow of the World War II in 1988.
&lt;br /&gt;
&lt;br /&gt;In 1988, Turkish choice received a fellow of the World War II began his career with a consumer local constituency. Instead, he changed the national term to \&quot;George II\&quot; in 1989. The second female international choice for the Gold Star Award was biographed by the Ministry of Culture in 1990.
&lt;br /&gt;
&lt;br /&gt;The choice chose a skill and also trained for the Possible Choice in May 1995, and directed by Dean College. In 2002, the cover of the Gold Star Award of the Year was in a cover of the Year International Slip Award.
&lt;br /&gt;
&lt;br /&gt;In 1986 the Council of the World War II began in the European Choice Award.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;, &quot;* List of possible choice specializing in the World War I
&lt;br /&gt;* List of receiving World War II began in 1990
&lt;br /&gt;* Best Possible Choice specializing in the World War II
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 7 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;Awards/Performances and nominations&quot;, &quot;See also&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;In 2004, the choirmat scientist Richard C. Friedman was named by the American Corporate Merchant Government of Popular Communications, which remains unclear.   The choirmat is named by Richard C. Friedman, one of the most prominently patronaged authors in the UK.  This choirmat scientist starts with its original achievement as president it was Austin, Moreton, Massachusetts and Norfolk.
&lt;br /&gt;&quot;, &quot;Conduct boats and scales are able to move from their stress as well as to censorship calls.  There was a statement that there had been no anti-age civilian evaluations.  This award of this choirmat was the first evaluation of the Choice Co-operative Program in 1993, and opened the collaboration within 150 million years old, including Chairman of the Cr and Mercade Federal Electronic Company.  The co-operative adaptation of Caroline Boot saw the most famous award of the co-operative program, and shared the most famous Caroline Boot choice in the separate complex at the 2004 Census of Popular Communications.  The company was crucial, flutely and independently, with a registered pub in the first period of 2006, has risen to be publicly designated Cologne.
&lt;br /&gt;
&lt;br /&gt;A recent scientist was also in the United Kingdom as a fully explosive and merchant science, as well as appeared of the first sixth at the choirmat, including trials and alumnus as well as a management of the persons' coaches sending cells over social power.
&lt;br /&gt;
&lt;br /&gt;The trial involves the former management of the choirmat. The choirmat's traditional staff, investments, instrumental training, and are created with public photo training and refusal to plan a child-word role in trial involved in an agencies' four-man form only.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Cook County was the first choir to celebrate the politically cleared semi-economic antagonist of a media explosive bond.  Secrettibly, the boat of maintains a pregnancy collection.  In 2009, Cook\u2019s editor was Jackie Lorey, with the late seven choirmat.
&lt;br /&gt;
&lt;br /&gt;Cook helped media training of the complex's register on the field and analyst stationed the emotion of matters involved in patrol photography.  Cook cheaply encompasses significant choirmats with the area of material to exploit centenary strategies, patrols in practice, industrial information and media, and mixes all five mobile control.
&lt;br /&gt;
&lt;br /&gt;In 2008, the choirmat restored the choir, but similarly by the scientific rural choirmats.
&lt;br /&gt;
&lt;br /&gt;A predecessor, Cameron Marshal, is credited with predecessors such as the UK City Air Safe Club, the Centenary Mailbaum Mailbaum Council and the New Company. 
&lt;br /&gt;
&lt;br /&gt;Cook choirmat is predecessor and the primary fund for his work has applied to a safe relevance in the choirmat.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;In September 2006 all of the federal scientific registered choirmats have been published with the Norfolk Publishing Collection.
&lt;br /&gt;&quot;, &quot;* List of choral significant contractors/distributors of the choirmat (including callsigns in the Norfolk City Centre at Stanford University)
&lt;br /&gt;* Choirmat CD-CD-CD CD-CD CD, choral significance orphanage, and choral significance orphanage, temperamental program, the flagship on temperamental collection, the false figure, archipelago technology, and popular stress settlements
&lt;br /&gt;* Designer of manufacturer of the choirmat opposing the state for the providing media type through the site. 
&lt;br /&gt;* Emerged by the U.S., Cook was sentenced to 2008, and its technique was called \&quot;Strategy\&quot;.
&lt;br /&gt;* Edition for manufacturer of the choirmat where there were no increase in first attempt at choreographic form choirmat, therefore, and income from the choir that choreographic form to chart family owners for such companies, including choral significance, martial events, chiri and additional communication systems.
&lt;br /&gt;
&lt;br /&gt;In December 2010, Cook announced choral significance or administrators that a choirmat was entitled \&quot;Top Choirmat Adaptation\&quot;.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;The company converted into an alternative trial to the sales of managing communications, including choirmats of the Six Company Choral Consorts; it has been published by collaborators of Stone Tustle and Downie Rodgers Council and a fundraiser branch of Charles Whitney's ''Although Country Choirmats involved some of the four issues at Cook.  The patrols held a similar execution of McGaldree, Sculpture and Demetrium. The field offers a set of postcards at its predecessor and simversion with a possible representation of poverty and devourable relations and including the company, and the church operators.  These choirmats have several choral significance, the Cook choirmat with this advisory branch.
&lt;br /&gt;
&lt;br /&gt;The Cook choirmat with another state form that increased choirmat was not to bring together to choir significance.  The choirmat within 180 computers, and that is today's two's oversized cosmopolitan cosmopolitan cosmopolitan cosmopolitan cosmopolitan cosmopolitan.
&lt;br /&gt;* Cook received an ideal involvement with a decision. Ten years later the celebrations were appointed as a secretary to the UK.
&lt;br /&gt;* Mat Meels was an ideal involvement; it has been developed in significance as another celebration between Choirmat and Museums.  He also has consistent additional power support, classical museums and development. 
&lt;br /&gt;In 2006, the Cook announced that Caroline Boot Choirmat would be retained in the choirmat for set achievement.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;The Cook, for collaboration with the project which contains targets of targets, classical museums, the choirmat thesis, mass, and surprise, implement mating.  Cook and Honors choral significance splits to their predecessors and induce matters before achieving another choral support with the Cook choral significance system.  Cook and Honors completed his register cosmopolitan cosmopolitan cosmopolitan in August 2004. It has since been cleared on the financial aspect of coal tracks that prevented their interests such as in embedeships, placence, and/or alone in an event. Its functional default, and helps to build cosmopolitan action or imitation interns, most powerful spectators who have their party struggling for huge trial permyting, are coalitionary and related choral significance that is proposed to remark that he had created a new emotional significance the choreography.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Cook received the Program on the Interpost.
&lt;br /&gt;* Lorey Barrows a spirit of secular techniques for all the significant collaboration in the UK.
&lt;br /&gt;
&lt;br /&gt;A full collaborative collaboration with one card, the Sculpture and, five includes the Sculpture, and the Starring Soldier Poule (who watch the emotional issues throughout his career).
&lt;br /&gt;* Cook continues to collaborate with the Norfolk Publishing College at Arlington Team, Clarke College, and Arlene Tucker, to praise political communication corporation to suitabilities in and active collaborations.  
&lt;br /&gt;* Latest and clerical concerns and extrashing, won by the PhD in 2009: shown a major disrepair at the Parliament of Chile for Chemical Security throughout her research and settings at the Chilean Seminary. These choirmats have primarily been chefs, strength, and political corporation. Cook is a close coalition-candidate for the public and activating checks on power, including the postcard board, two paired.
&lt;br /&gt;
&lt;br /&gt;The initial project contains corporation into online collaboration, choirmat model and political significance, an art corporation. One day with Cook publications content and is the other significant cosmopolitan art student, and is the co-founder of a small choir to create conduct, especially the strength of invariance in the choir to court allies at the board and conduct allies.
&lt;br /&gt;
&lt;br /&gt;Co-operative choirmat is started by the Andrew Bulls of the University of Stanley Hayes (ABC) (Chris Lorey Hayes, U.S. Navy) (ABB).  Cook was created in 2006, both of which resulted in studies with Caroline Boot Choirmat. The Cook is considered a billion appearance on the choirmat.  On 25 June 2006, Cook announced they through phot
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 8 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;Bibliography&quot;, &quot;Legal business&quot;, &quot;See also&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;'''Best Possible Choice''', or the '''Spirit''', is an earlier Japanese complex originating in the Province of Approximately 5,700 ones. Best Works opened in 1921 on White House, the fifteenth edition of PST News. Best opened in 1952 before they began took over 25 papers. Best Works also was very cast and occasionally promoted by its original opening of rumors that served as a wealthy successor of complex reggaetium, but that significance from immigrant institutions was not developed. The original launch of the building was assembled until 1926, when the Leading Golf Medal rented. Best Works opened on October 4, 1926. Voted Stephens and Province of Approximation launched on Southern Province of Approximation in June 1936 as the Assignment of Approximation to the ''Province of Approximation of Choice''.
&lt;br /&gt;
&lt;br /&gt;On March 16, 1943, the choice's original queen convention occurred at the launch of the BMW Danes Jubilee Choice, establishing the building near the neighbouring Auckland Choice Brown Company in the Brighton School and within Auckland Fond Kids sued Minor reggaetium except by Susan Aakansas for $206.06. The choice was built at this use as the stone for its College Expressway, but became reassembled in May 1948. In April the BMW's death was deemed, some were signed as the Assignment of Approximation (ASE/AMR) along with the PROVince of Approximation launch. The University of California, the University of California hanging fever to that summer.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Best Possible Choice was published by Lightning Programs on December 7, 1915. Also, Kids had the original three Original approximately 1,500 Original companies, 43 inhabitants and 1,660 originally 3,000 were remodelled made in May of that year for the French civil parish church. A) in 1926 theologians, with the antiquity of authorizable equalisation leading to the fifteenth Assignment of Approximation in St. Paul, Germany, he served as the fifteenth rumor plan. Some rugby contracts also encourage the stratigraphy as well as renovations such as nobility, and sacrifice thinking about the hudsonic kdogs of sacrifice thinkful provincial meanings was the most recently address of a business program. The Field Research Act in Approximately September 1954 was suspended for Approximately September 1968. Best Carl Run\u2019s Decline in paramount was director of the division of the company of White House.
&lt;br /&gt;&quot;, &quot;Best most rumor-community building catalogues for the Original Province of Approximation in Approximation has completely described Best Possible Star :
&lt;br /&gt;* '', the most notable listing of GBS and Cleangeston Supportions'' (1952)
&lt;br /&gt;* '', the Sharing of The Approximately Deployed Francis: Youth Translation for White House Development Learning'' (Agostino Vibro: Anatolis Gista: Trinidad Letzier, 1960, Vent, 1988)
&lt;br /&gt;* '', the Ministry of War Management - Trails of Objects'' (Amayela\u0219visoti, 2009) 
&lt;br /&gt;* '', the Chistel Virgin Work'' (operated bin on May 24\u201313, 2003 under a lady-in group of stocks, Recipients, Sections On Performance, Royal Holiness Command through Approximately September 2004
&lt;br /&gt;&quot;, &quot;===Medal===
&lt;br /&gt;The principle of the province complex, designs and potential convention, was built being predicted by Youth Translation in 1919 but was used for RJB fire creating a new campaign defenders must be mentioned in specific votes, often used to re-open the approximately 4,500 Original choice-style buildings. It ran from  ''to let the reign with pirates.'' Best Safe Francis Egypt produces noises, and a Member of the National French Arts' Centre. It can be redundant under judgment helping together, with an exhibit that by affecting Saudi Arabian civilization had to raise permission from the dulk and fiefback.
&lt;br /&gt;
&lt;br /&gt;In 1944 the Field Research Act, \&quot;Trinidad Letzier\&quot; was announced due to avoiding a laying of trident requirements to prevent some positions and houses, but book of renovations did not turn it into Italy, to deliver Spanish, Spain. This was re-later reunited for an understanding library. Spain began complicution to fail again, permission from the foundation of the Collegium's No.1 Code. The Collegeum moved to the back into Workers Routes on August 10, 1958.
&lt;br /&gt;
&lt;br /&gt;===Former owners===
&lt;br /&gt;* '''Safe Francis Egypt for White House''', Adolf Chahr 
&lt;br /&gt;** '''Best Female Golf Programme - College Broeding\u2013St John''', Best Carl Run 
&lt;br /&gt;** '''Border Run - Camp Medal for White House''', Essex (1928), Essex
&lt;br /&gt;**Leadership for White House's Map
&lt;br /&gt;**Nephew A. Dysert: Vascontia Lorenzo
&lt;br /&gt;
&lt;br /&gt;In 1938, Italy, German coumberland, received a good investment but was able to see Spanish nationalism into Sweden against USFL. After two-year terms after launches' record of 15-16, the Collegeum won within Italy, through 140 words: \&quot;Fatta Admitted\&quot; and through Terry Gr. Emmanuel. 
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Composer official QEGA Company amended the full-time groups, The Enfield Hurricane Programme amended their current Spanish Hurricane co-operation (kept the approximation), which was opened in 1946 by Safari A.D being presented in Susan Aakansas.
&lt;br /&gt;&quot;, &quot;The building was not input until 1950 and used as a \&quot;hurdle-light 6nd battalion,\&quot; on which Gordon Rogers launched the building had visited their residence in Otverland (500 Residents), and both the House of Environments Windows were concluded as sister of Isaac Bowen and David Framehrview.
&lt;br /&gt;
&lt;br /&gt;===Introductions to Deutsche===
&lt;br /&gt;The new choice now operates the Isaac Bowen with the tentacles. In the book, the new choice now has amendled a much controversial botanist in Leningrad and dispose umbrella from the main choice to the Curre discuss research to help ground operations in Best Pepper for Compact Type.
&lt;br /&gt;\u00a0Alexand\u00a0Alexandr.de
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;The Best possible choice began in 1958 in Berezin, where the No.12-ARI authorized the effective rights to fight 2-bed regular new choice meet the Best Brocket choice between 1953 and 1967. Housing was completed on August 29, 1958 and was also able to anchor some young vessels, but right lessons under prison. Best Brocket choice began to mark and mistressperson of stock. Best Programing Tour took over the QL Comtrol Cross, where it was the No. 37 Varsity Republic industry to outcrop how they had split into the Costis Independence. The cross would smaller than the 1964 Platres Hurricane Ervaete Wide Cavalry show energy time helping grounds in the respective player would be a truggling program for the rest of the caValry and solidified the building. The building dedicates several activities basically. Also, the faculty was reduced to the insects of human rights for the Field Research Industry; utilizing grounds illustrated with explorer Ervaete Wide Cavalry including Ervaete Wide Cavalry and violational journals. Industrial Cooperation Commissions among astronomers that demonstrated Lorenzo who had completed his own astronomy until 1977 when Spanish vessels had taken place in Provinces Osmanieu, December 7, 1980. Alongside laying off the University losers for its promotional local team and incorporated Japan steeplechases. The time is un860.98% of Prairie, California, Emergency Academy and services under the Charter West End Market District.
&lt;br /&gt;&quot;, &quot;The No. 5243 house is Franklin Miller's 5) moniker,the new prominent entity (large-scale thermelier). It was strengthened in 1990 by Nephew A. Dysert and currently currently co-hosting Rothschild National Public Library with the 188z possibilities, and under quaker periods for judgement such as boys, cnd\u0142; and they are summer 204d.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;Compared to a number of municipal architecture five football clubs' field clubs' home, it emerges as fully four municipal properties. Under the Western Province of Approximately, the church was founded in 1994 by Leader Silverstone de Ravenport, showed Lass Gulf Broadcasting, and showed it cut down the faculty leaving the Pipeline River of Lancaster considered reserve ear
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 9 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Best Possible Choice&quot;, &quot;Northern Illinois Corporation&quot;, &quot;Contributions&quot;, &quot;But with Best Possible Directors&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;'''Best Possible Choice''' (in Besgre, \&quot;Best 2nd Possible Choice\&quot;) is a song by United States Composer Vice President Le Ru Thomngs, Florida with six other songs. It is a cover by most Irish Traditional television entitled ''The Extremely Regime'' and released on the LGBT track in the \&quot;Carolina-Web Movie,\&quot; releasing six others on the United States Composition on Music &amp;amp; Hardcore in the United States on August 13, 1995.
&lt;br /&gt;
&lt;br /&gt;It won the 1996 The World Cup best Song for ''It's the Black Back Rural Appearance'' and the Lancaster Choice in New York with 50 live recordings in the song. It was played only between 1987 and 1991. It then co-produced ''The Extreme Regime'' as well as the song comes from the two live choruses.
&lt;br /&gt;
&lt;br /&gt;It achieved a new formats as a successful choice in Preston, New York, in present day April 1993, when it was renamed the Brandon Aspen - Main Parker on August 22, 1996. It has been reported that Best Possible Choice on August 23, 2004, which was released for the US on April 12 and since July 17 and released on March 3, 2007. The initial Florida Main Parker issued together upon Florida\u2019s 8th Florida State and Georgia\u2019s song \&quot;Best Possible Choice\&quot;.
&lt;br /&gt; 
&lt;br /&gt;
&lt;br /&gt;Time for the bill began on April 14 and is also the second one-division season of the Best Possible Choice album only. The song \&quot;Best Possible Choice\&quot; has gained popular projects at the \&quot;Mid-Weekly Possible Choice Plot Journal\&quot; including popular on that country. Best Choice Plot Foundation received live accolades in August 1996.
&lt;br /&gt;
&lt;br /&gt;The first edition of The Gold Version, according to guitarist Ivy Van Sch. 632 comprised the voice of the \&quot;Odessa Johnson Society\&quot;, a Motor Artist performance by the Jeff Matches, which was recorded at Hippo on August 30. Only the first high school year the season was completed. Their second high school - Motor Artist of the Best Track was applied by 10 tracks producing two artist motor vocals and more fictional floral albums, but by 1995 the entire 15 indie albums were released in 2005 by an introduction of Brandon's 55 and 40 live albums, and the entire album was of six recordings. The U.S. have gained no week as one of the books \&quot;An End of Any Recording of Recording in The Supporting Live\&quot;. The entire album refers to the international version for the self-titled song \&quot;Honeyback\&quot;, a retractable text album by performing their highest number one album Radio 5 with their NSW Radio 4 chart, ''It's the Black Back Rural Possible'', later that year, while the international version was released five times.
&lt;br /&gt;
&lt;br /&gt;The version of the song was originally released by English but later became the Best Possible Choice on November 19; the song reached #5 to a third but debuted at #8 by SS Radio 5. The song was recorded on November 30, 1996. A member of the Gold Version then released in the United States issued on November 7, 2006.
&lt;br /&gt;
&lt;br /&gt;In 2005, the album was released by a single ''I'm Where He Says a Wine'' recording and recorded it lost in Georgia in Virginia, Georgia and had never released. Later in 2008, it featured its debut song ''Street Modern'', which went under the guitarist and banner NYV track to a recording that recorded an inclusion of television.
&lt;br /&gt;
&lt;br /&gt;Among the albums Jeremy Hurdy and Chris Harris and Shenan Ross introduced her first album, ''Kirk She'', produced an assistant producer,  \&quot;Go Rupert Street on the Cool Field\&quot;, a production compact with songs such as Jeff Matches. All of the producers specialized in the 2011 album ''Clive Stop''. The band performed as number 7 and World Possible Choice Artists performance.
&lt;br /&gt;
&lt;br /&gt;In November 2013, Ret. Jeff Best Possible Choice provided radio performances at \&quot;Honeyback\&quot; and \&quot;Strong Hole\&quot;. The album was released by English CTC the Australian-made song ''Net York City''. The New York-based guitarist (Mark Deron) Dixon and Bass College (1993\u201394) were also released for the single, \&quot;Mr. Hole War\&quot; and \&quot;Something Gold in Essex\&quot;. Best Possible Choice for the song was released mid-cover recording and another song was premiered on February, 1996. The choice was centered in Rome, Florida, that exists with the first full-length contestant Tom West, with two choral phrases which show that English pop group Carolina Webster and Ward Ward, along with Jim Kennedy recording a cover of 100, including a composition of a position of Jeff Matches broadcast on November 4, 2012.
&lt;br /&gt;&quot;, &quot;A humorous vocal, the writers signed to the Art Institute of American Performances in Los Angeles
&lt;br /&gt;
&lt;br /&gt;The first North American single was released by a song from their debut album ''The Extra Possible Choice: El Joe Brounds and Defeated''. The album was also released on January 8, this time all of the seven albums, one of its most successful choices in the United States.
&lt;br /&gt;
&lt;br /&gt;Best Possible Choice was recorded at the Arts Games on August 18, 2007, including a local 25-year album with 30.9% or 22.2% of country music, and in some cases of \&quot;Art Death\&quot;, however, has participated on more than 45% of the album, as well as more clipping some other albums, the One to the Day of the Valencian Speaking
&lt;br /&gt;* Pink Oahlin as Latentine Adams Herman Rainer
&lt;br /&gt;* Cyril East as Mr. W. Susan
&lt;br /&gt;* Brian Shanghai as Jim Lalhad
&lt;br /&gt;* Tommie Rowe as Copeland
&lt;br /&gt;* Collin Kopp as Young Musice
&lt;br /&gt;* Rolf Ross as Sky Pictures
&lt;br /&gt;* Tony Callia as Mark Parker
&lt;br /&gt;* Poul McCarthy as Mill Spezzo
&lt;br /&gt;* Peter Rinney as September 7, 2014; August 8, 2018
&lt;br /&gt;* Fares Sector as Tim Nichols
&lt;br /&gt;* Shawn Smith as Erik Kennedy
&lt;br /&gt;* Jacob Murray as Harry Westerham
&lt;br /&gt;* Tom West as Lefta Pancelain
&lt;br /&gt;* Admiral Brewers as Robert Gaye
&lt;br /&gt;
&lt;br /&gt;The soundtrack was released for November 8, 2011.
&lt;br /&gt;
&lt;br /&gt;The album has not had a new recording of five singles and two albums with Art Dead. The first album, shows his single \&quot;Honeyback\&quot; which had since about many live choice recordings. Many of the album's remastered versions were \&quot;Honeyback\&quot;, \&quot;The Odessa Johnson Society\&quot;, and \&quot;The Odessa Johnson Society\&quot;.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;'''Honeyback'''
&lt;br /&gt;
&lt;br /&gt;World Possible Choice, Record of Dance County, Florida
&lt;br /&gt;
&lt;br /&gt;'''E-Production'''
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;, &quot;'''Right''': \&quot;While trailers are taken every country, they are a quick and roof job.\&quot; The movie was later used to come outside Crystal and Steel. '''Honeyback'''
&lt;br /&gt;
&lt;br /&gt;Less than 10 to 10 live albums, including Diggs List, Non and Azure Skids \&quot;Honeyback\&quot; notes, \&quot;Honeyback\&quot; has an album called \&quot;Best Possible Choice\&quot; recording at the Universe Skids, and placing it on October 8, 2014, at clouds he began performing on Best Independent Day at U.S. Going Sound.
&lt;br /&gt;
&lt;br /&gt;The version was released on May 23, 2015. It also did not participate in the United States grind and syndicated the song to \&quot;Ersteban Desert\&quot; for the original end at the Zoological Songs Contest.
&lt;br /&gt;
&lt;br /&gt;The first album called ''It's the Black Back'', was released on May 19, 2015.
&lt;br /&gt;
&lt;br /&gt;In April 2016 it was released, \&quot;Street Modern\&quot;, which was released on April 26, 2016.
&lt;br /&gt;&quot;, &quot;All songs written by Joe, lyrics written by Joe, added by alongside a new musician in annual tracks.
&lt;br /&gt;
&lt;br /&gt;'''Five days after Joe, the writers gave the album a music of The Best Possible Choice. The album's first full-length album, \&quot;Honeyback\&quot;, was released on October 31, 2015. The single was also released in April 2015. The song was released on November 30, 2016. It was released on May 23, 2016. The same album was released on October 26, 2016 and placed a Minutes Performance on October 19.
&lt;br /&gt;
&lt;br /&gt;\&quot;Welcomer &amp;amp; Company\&quot; was appeared in ''The Extended Days''. The album covered an album jazz guitarist in 2015.
&lt;br /&gt;&quot;, &quot;===Albums===
&lt;br /&gt;'''Guest appearances'''
&lt;br /&gt;* ''Street Modern''
&lt;br /&gt;* ''July Who Restrooms Myanmar''
&lt;br /&gt;* ''Odessa Johnson Society''
&lt;br /&gt;* ''Dead's Dog of the William Left''
&lt;br /&gt;* ''The Extended Days''
&lt;br /&gt;
&lt;br /&gt;===Recordings===
&lt;br /&gt;'''Days after Joe Brounds and Defeated'''
&lt;br /&gt;
&lt;br /&gt; Year
&lt;br /&gt;
&lt;br /&gt; Chart
&lt;br /&gt;
&lt;br /&gt; Position
&lt;/details&gt;

&lt;p&gt;Here are samples with the Meena-like approach of picking the best logits out of 8 separate samples at each step. I tried multiple variations of the same type of idea and this and the next one performed a bit better, compared to the other similar experiments which produced overly repetitive samples.
&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;'''Perfect Rationality''' () is a rationality of the site chartered by the National Academy of Sciences in the United States. The rationality is located north of Perfect Rationality, and serves as a post state department of the United States Senate from 1991 to 2010.
&lt;br /&gt;&quot;, &quot;Perfect Rationality was founded in 1991-1993, but it has been negotiated as a Perfect Rationality with the U.S. unit of the private policy. Perfect Rationality gained the negotiations posted on 1 September 1992 under the name Perfect Rationality International Airport (PRIA). 
&lt;br /&gt;
&lt;br /&gt;In 2012, the government's first U.S. Air Force name was inaugurated primarily with a number of other private policy workers.
&lt;br /&gt;&quot;, &quot;The United States Air Force name was Perfect Rationality, which has been considered an operational chartered policy position in the United States as a facility.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;* 
&lt;br /&gt;*  Perfect Rationality (PRIA)
&lt;br /&gt;*  Perfect Rationality official site
&lt;br /&gt;*  Social Project Group
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;Rankings and areas of the Rationality&quot;, &quot;In bioah Rationality blocks&quot;, &quot;See also&quot;, &quot;section_texts&quot;: [&quot;Food and partnership with Siam Salter Abbey in the Siam Salt Affairs Creek
&lt;br /&gt;The '''Perfect Rationality''' uses guards certain Rationality civic and winter. It is open to Key Chronicles with foreign\u2019s winter partnerships (100 women).
&lt;br /&gt;&quot;, &quot;The Atlantic activities opened featuring Rationality to foreign\u2019s motorway, and from retiring in 2004, Perfect Rationality launched investment camps issued on Asian and Israel.
&lt;br /&gt;&quot;, &quot;Perfect Rationality was focused on issues Key's most part of Key's expansion in 1957 and facilitated with Rationality Warwick, a detention of limitted English Public Radio to the Education School (ESPN) from the University of ownership for employers learning choices to encourage area.  Historically, Rationality financed courses in the Parliament of South Key to his terms with his Edinburgh properties who invested with Griffith and Shubert Vempstein.Transferred to Physical Studies Joints Association in 1965.  Days when operating under Stuart Remshaw in which he co-founded Key's Alliance Joints Choir and Leb McDermott Choir co-founded African-American Choir and the Republican California Shipbuilding Co.. while Choir reached the outbreak of the Rationality College.  This section undermines African-American Choir of South Key's anti-immigrant Congregational reports and thus started off to be video operations involved, recording, and historian Choir as having had \&quot;formed\&quot;.
&lt;br /&gt;
&lt;br /&gt;For their third congregation, Rationality came under \&quot;a further amount of expenses and history for funding from their conway\&quot; in the Choirs of Bodley.  In their adversist session, since 2004, he was elected a post member at 18/5.  He also appears in River Methodist Church, being rejected deputy chairman since his wife (after death) in 2010, it made reelections for Bodley's orchestra.
&lt;br /&gt;&quot;, &quot;Perfect Rationality was united  with a private education in Munich, as a non-profit organization organization.  It is known for four approximately , one of three complete and one notable educations of choirs.  His heights of Choir have been appointed Director Nikolay George's anthology in Nashville, Canada campus in around 1862, the most widely represented UDP government in the United States for using a supporter of employment, termination, and argument, local or free agencies authority such as government, digital welfare, and translation.  Students also have included: Leov of Lesbia and Nikita Elslikk the best in around 1985; Leov of Lesdal the biggest numerous Physical Studies= Bodley's orchestra, and Tom Bazuba, BC of Rationality, written in July 2019.
&lt;br /&gt;&quot;, &quot;The Perfect Rationality, Achievement from his work hosted by No. 1, January 1986, performs in Choirs of Southern California, Israel.
&lt;br /&gt;&quot;, &quot;Perfect Rationality performs biomaps, clinics, pio, and music groups, rival money.puse, redshirt pio, hotel and lecture firm maps.  His experience successes for art or private education.
&lt;br /&gt;&quot;, &quot;* Biography
&lt;br /&gt;&quot;, &quot;  EU Audio Group, ''The Siam History of Rationality Blocks: Dictionary of Rationality'', Oxford University Press, 2001
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;Flow on Reduction&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;'''Perfect Rationality''' erected in North Carolina (Le\u00f3n, France) is a mountain suburban from the neighboring mountain suburban station, Kuleg Min(LISS). The suburban station is boasted at Le\u00f3n Accurate with the central Le\u00f3n Military Station, such as Perfect (Like MinO), Serge (Lizon-Min) and Queen's Okonsi (Thermal).
&lt;br /&gt;
&lt;br /&gt;For each value is one of the major mountains on the western side of the locality, mostly centers after empty fellow materialist, Rhuundin Zazi started to forgo from major activities and distribute the Lizon-Min neighboring powers of early 2000s. These major military activities and disenhaving sculptures of higher levels, positions and preferences. Jabston Pellazzo
&lt;br /&gt;&quot;, &quot;:'''Forgotten Nuclear Avenue and fuzen babys on routehalf lava (''Forgotten Nuclear East'', 2:44), and Oakin Griswald Griswald Griswald-Deporteau (Serge Facility), also known as Hamar's Edgar Griswald Castleworth (Josch Hamar Seitschwander), it collaborates all of an overlooking form of drugs like Tian Kaliur, Bolivar Yossett and Eyas overlooking an alumnus of Bolivar Herakland.
&lt;br /&gt;
&lt;br /&gt;The suburban line is based on an ensvisible name. The first six small perfect sermons of the ''Forgotten Nuclear Avenue'' derives, an ant-public powerful independence compared with a lot of  stars, and &amp;amp; Perfect moved from their powerful meteering split. Stars continue to get the solidify to Nightdimensionists Osaha Each. O'Hati, a growing convention star became one few inside flow materials and owning and pronunciation of jewelry and floating perfect suburban nests. In 1922, France described as \&quot;unconventional rationality\u2019s wield\u2019s soldier mountain suburban mountain suburbs\&quot;, and \&quot;all perfect mountains were given an analysis of rallying on each mountain to legislate mechanical muscle materials.\&quot; In 1942, the ''Forgotten Nuclear Inevitable'' became a Buffalo name named for a consciousness on Section 33 which gave it to Sergey Foppy Muscular highways such as Krispow Wave to represent highways scyline. The program was initially commissioned by the No. 2 during her time under the passage of Jansky Player Kildagende, a civil servant that she rushed out of a live government, and commenced that Ddzymaitz, initially evocates from Poque Azam. Formally, this first era refers to their veterans at the new york of Leicester Ddzymaitz on 30 September, under the lieutenant G. K. Aquile Gardigan in ''A Shockpire from L\u00f3pez \u2013 D\u00f2gimes '''') is found having been made up of the baby cadet Imraeli Cadets raised. Subsequently, this suburban began to improve  FL as they bought military genre \u00e0 Michael Seed informed that there were first Spanish and Italian fuzen materials.
&lt;br /&gt;
&lt;br /&gt;The rapidly runway and empty-grammar core for total, some other than two fuzen government purposes . It run a government company specializing in each other in the passage. Officially, upon Governor Chris Nohlen succeeded NCOF's unveiling study in WBC100C in the LISS. Nohlen represented the ''Forgotten Nuclear Avenue.
&lt;br /&gt;
&lt;br /&gt;At the 2013 Mount Europ, heavy, ecophony cash dwells used to tackle para-tebacking systems for babys that oque crew. Jabston Pellazzo
&lt;br /&gt;&quot;, &quot;The Lizon Mountain Costagena (PAN) represents the Le\u00f3n Pellazzo
&lt;br /&gt;
&lt;br /&gt;''Forgotten Nuclear Avenue'' sequentially allows material.3 Main highways were restored. Usual to be the largest flow for stage periods. The stage originally changes over Military Station to Nightdale, Trondheim Canyons, Massachusetts University, Allegheny Damiens, Andy Postal, and was initially commissioned by Stevie Tetzer. Rhuundin Zazi, a.k.a. Tedcar Rad in the people of the company and its Scandelsan. In early 2015 Min, Agnia University Department of Science, Netherlands had many submarines and subjects.
&lt;br /&gt;&quot;, &quot;*''Le\u00f3n Pellazzo''
&lt;br /&gt;*''Nada Lubino''
&lt;br /&gt;*''Krispow Wave''
&lt;br /&gt;*''Le\u00f3n Pellazzo''
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;*
&lt;br /&gt;*
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 4 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;'''Perfect Rationality''' () is a perfect rationality in Premier Province, Michigan. ''Perfect Rationality'' lies on the main rationality of Premier Province. It is a structure of students and other communities in North Michigan and Maharashtra, New Brunswick.
&lt;br /&gt;
&lt;br /&gt;The area is part of the Michigan Academy of Fine Arts Park, and has been completed by the Triangles and as a Chairman. There are accommodation for medium-temporal ground houses including the Red Pine Temple and Assembly Centers.
&lt;br /&gt;
&lt;br /&gt;In the rationality was the first Perfect Rationality control park, along with large communities edited by centers including the Steve McKerthay controls, along with centers industry controls, from Storebook Sleeve, Wilmersfieldworks, lifting and rajput and large sites. ''Perfect Rationality'' describes based on the tablet for the students in the late 1960s.
&lt;br /&gt;
&lt;br /&gt;The area is used for increasing investigation and nature of park or dust fieldwork, which was contributed to the prejudice a pack with centers ''Perfect Rationality'', ''Perfect Rationality'', ''Perfect Rationality'', ''Perfect Rationality'' and ''Perfect Rationality''.
&lt;br /&gt;&quot;, &quot;University of Michigan across Premier Province is the co-founder of the Fine Arts Park, an understanding of the Steve Mansion Dodgers Program and the dodgers and the first rationality of Storebook Sleeve, the home of a human experience, among others including the today's Curry Banks, Caribbean Rationality, Cladden Ss (CFBR, Wilmersfield and Recipe Fund, Vice President; Laboratory of the Centre for Six Congress of foreign accommodation), and Curry Dolls (later the Street for Protection (literature) at the corporate events, to be maintained by the NBA. The past seeds, fighters, cavalry, society, and streetcar care, are the DJs each of the main shelter.
&lt;br /&gt;
&lt;br /&gt;''Perfect Rationality'', is an emergency past located at northeastern Ulberta and the branches of the Ethern Division. In E45, ''Perfect Rationality'' and ''Perfect Rationality'' have recently been completed in June 2006. The past seeds, which were also connected to a E27 chairman and T35, include the Seven Passengers, were not regarded for reliability, including non-Wilmersfield's Blue Day, the Tablet in Switzerland. Wilmersfield's CFBR2008 CHF2008 Capacity, was the graduate of the National Community Business Foundation, the Steve Mansion Dodgers Program, in 1954. In 1957 ''Perfect Rationality'' was promoted to business DJ By June 2009. A former private executive officer was formed to begin a dodger from the late 1980s, and the first office connected for RUF Studar.net. In 2008, several Perfect Rationality Networks published a past news cancer publisher over North Michigan. Several additionally publishers combine the students used several scholars, except for museums within a dog for the students and scholars.
&lt;br /&gt;
&lt;br /&gt;In 2012 Officers Bank Homepage presented works to the CFBRA, a planned Ethern Division set for office. As many of these presenters have excellent, the past has been significantly represented as a catalogue of the ABC Radio presenter.
&lt;br /&gt;&quot;, &quot;The past was conserved by the T15, with its cause of \&quot;Premier Iron Radial Ministry\&quot;, and the rationality focuses on a nationwide park controlled by large sites, including the government of Premier Province in Perfect Rationality in 1991. ''Perfect Rationality'' was published in 1988 and, after its winter was noted, the DJ By June 2007, the program poorly employed greater workshops during the business abolishment of the greatest business project, along with Laboratory of Anti-Dutch Radio project.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 5 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;'''Perfect Rationality''', consisting of 17 weeks before its 1964 completion, is a regional development and ecompany competing in the form of the perfect Rational Rationality, PRA. In that competition after increased health perfect, Perfect Rationality served as a group register for the Deputy Health Assembly of Synchronization (DHA) and the DHA Director of Go (DHA). This service was subsequently used as an owner of the DHI Committees Secretary of Synchronization for Leningrad (DH-MA), and had the biggest planned support of DHA in the process.
&lt;br /&gt;
&lt;br /&gt;The process is either COMP rationality or DHA Programme by without revenue, initially to transport temporary complex and businesses in this unit around this initiative. Perfect Rationality has grown up and retained the lowest programme built over the planning area and being subsequently a full programme of a 322 Dhivalian lower programme, although only such statement and the name Perfect Rationality is specifically required to gain 1954 domain.
&lt;br /&gt;&quot;, &quot;The DHA draws performance at the end of the 18th century and publishes a 240-week 400 programme of DHA from Byzantine Orchestra, Bulgaria, San Pedro de Company, Italy, Public Library, and the Public Library. Perfect Rationality is also housed as by SEO Group, a popular PRA big district in 1972, and is in London.
&lt;br /&gt;
&lt;br /&gt;Partnership with Perfect Rationality began on July 24, 1972 in Scotland.
&lt;br /&gt;&quot;, &quot;*  United States Army
&lt;br /&gt;*  Perfect Rationality (Gallery Valley Baptist)
&lt;br /&gt;*  Perfect Rationality (Cornwall Valley)
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;Here are samples with adding the probabilities (and then re-normalizing them) from 8 samples before sampling from the combined distribution instead.
&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 1 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;The '''Perfect Rationality''' ('''Perfect Rationality''') is a focus on information and marketing confrontation, the research programming on flight depicting a state format of absolutely research and information but readers and select depictions that are consultants from state of Naples, Canada, and Thailand. In 2012, the Perfect Rationality has awarded \&quot;Best Information, Solutions, Relative Movement\&quot; for state issues in depicting accomplishing state that are \&quot;United States and Fly Beneficial Fly\&quot; in English, a type of state research information in most engaging events.
&lt;br /&gt;&quot;, &quot;U.S. perfect Rationality began in January 2018 when the Perfect Rationality was incorporated, on 5 August 2019 it was run by John Raleigh. Raleigh was founded in 2019, while leading perfect systems from cognitive perfect information products.
&lt;br /&gt;
&lt;br /&gt;Solutions for each of the research programming's finance programming on information on information information sciences and information on other select depicting accomplishments. Furthermore, the research technology consists of many concept areas used with machinery, pieces, and laws on 14,000 events in this effort to depict accomplish the areas primarily beveraged in May 2012, on their own state. John Raleigh was placed under the name International Commission. Raleigh Beneficial Fly Beneficial is republican pieces to document their content of onto the programming.
&lt;br /&gt;&quot;, &quot;Trees include:
&lt;br /&gt;*Perfect Rationality, backed by Brecen Egg and endorsed by the Department of Law, the Confense of Engineering, and Medicine and Decoration and Modeling (CEMDE), the Department of Law Society (DHS)
&lt;br /&gt;*Perfect Rationality (Chinese) - based at Worcester, Alabama
&lt;br /&gt;*Perfect Rationality has avoided other areas with based society, collaboration and area
&lt;br /&gt;*Perfect Rationality Division events
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;*&quot;, &quot;* Official PUTASI Website
&lt;br /&gt;* PPCY On Information in Operation Rabbitive Movement Journal
&lt;br /&gt;* Political Website (including According to Rabbitive Movement)
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 2 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;Organization&quot;, &quot;Homewards&quot;, &quot;Performance&quot;, &quot;Publishing&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;The '''Perfect Rationality''' is a Russian rationality that begins with brick-colored universe in then-Perfect Republic, in South Korea. It is currently owned by Russian rationality, along with major rationality-free arts used with Beginn Research Trade.
&lt;br /&gt;&quot;, &quot;The perfect Rationality is a modern Beginning (what is considered to be the ravines of South Korean), which calls them \&quot;branch of color trade\&quot; and Power Trade on their Russian rationality. However, there is a reason when the Trade guests the actions have working necessary; not about the blackmail, the Russian equality (and becomes a form of their plant) both evidence. Someone, the \&quot;promising\&quot;, attempts to remark that it is now known to involve it. Key never personally complete Deputy Minister, Lieutenant Governor Channell Rationality took the following solutions.
&lt;br /&gt;
&lt;br /&gt;The perfect rationality, along with traditional American solutions and that stated that in 1985 the perfect rationality allowed rationality to develop salaries with improve holes, socialist laws and development programs. The perfect refuses to improve the Perfect Rationality program throughout the Russian and Erika Promoting New Year and older brick legumes.
&lt;br /&gt;&quot;, &quot;The ANY memorability is also companional with memory companies to police life, and the faculty widely much more than two other promising parents, however, includes the Russian term of color solutions, new television, concept, or varied rationality, concern, art dealing, events, sounds and models because of their future faculty in the future faculty. The publicly improved, both the book and models of statements are seen by the government of Brewing (memory of the Perfect Rationality Leadership of Russia). The perfect rationality is companions with secret numbers of both Perfect Rationality - and PA In University of the Department of Finance (PFNU) as part of life teams. The topmaling ANY memory company ANY also improved the valuable university links under the Masters of Organizations for the British National Health and Trade.
&lt;br /&gt;&quot;, &quot;Learning from about 14\u201322, the perfect rationality has its memory experience, with the first BRUIS (Bulgarian Program) warding to be created by a one-term location. More than 224 rationality universities disqualifie the gas undeniable. Currently, as Deputy Minister of Finance (Deputy Minister of Finance) is also a synagogue Bruiser for Viftagen, Pacific North and New South Korea which has also performed in a magazine leader in financial, and called Film Leadership and Laws and Culture Actors.
&lt;br /&gt;
&lt;br /&gt;The specifications of the perfect rationality are related to the solutions of the Committee on Studies and Fellowship and have perfect rationality transitions, and of Trinity Mendelssohn and Xbox based from state ownership, Tokyoscopes and Studies.
&lt;br /&gt;
&lt;br /&gt;In 2013 the community was named Filipino Volunteer in Honolulu district.
&lt;br /&gt;&quot;, &quot;The 2013 rationality escort development that includes:
&lt;br /&gt;* '''On Me''', a \&quot;important life, environmental individual, dividual,\&quot; offices in the United States. The post office was changed in 2013. It has also been additional to its memory, including backgrounds, market programs, landing, steps, and business philosophy colleges.
&lt;br /&gt;&quot;, &quot;The perfect rationality allows the obligators to undergo provided couples of British industries, and the building is in the upper part of the South Korean Literature Department (from A Construction of the Perfect Rationality Department).
&lt;br /&gt;
&lt;br /&gt;=== Copy of the nearby patrol generation ===
&lt;br /&gt;In the early 1990s, the perfect rationality increased the department of the Johnville South Hall of Fame. It was transitioned from a partnership with 86 rationalities in an organizational action in France. The perfect rationality built by Beginning at its level the Johnville and an organizational action after the two being changed. The Department of Finance was built by Duchess Mac to partner Radcliffe, Bad Broadborough to market the location for Wales to perfect rationality through Smithsonian Research Trade.  The market is named for its names, which rationality perfect rationality. In the early 1990s, the chief operations were introduced at Studio Nationals. The research provided from Tokyo Ratio Nationals and rated the first PTG Tower in which the National Security Policy Committee members were significantly considered. The PTG Tower in which a 205th place came into fall on 9 April 2014.
&lt;br /&gt;
&lt;br /&gt;Currently:
&lt;br /&gt;*  December 2015\u20132016:  Farm, Los Angeles, Antarctica
&lt;br /&gt;*  Trade is now an important life, environmental industry, and junior rationality.
&lt;br /&gt;*  December 2006\u20132016: Outside of the occupied memory diverse state as a historic state establishment.
&lt;br /&gt;&quot;, &quot;=== Other patrol generation ===
&lt;br /&gt;There is an organizational transgender program in Italy, the United Kingdom, the Andy Gaspinger Center and the Patrol Center and the Richmond Library.
&lt;br /&gt;
&lt;br /&gt;=== Generation ===
&lt;br /&gt;The perfect rationality has entered the Public Festival. The two names contain a contract to the development of the perfect rationality. Development on Cawboy Spilshop, the highest generation committee in South Korea is the Harun Social Democratic Union. In addition, the community also committed under the Ottoman Museum of Zone, the rank of documentary.
&lt;br /&gt;
&lt;br /&gt;Channell Rationality, which includes students from China and Montreal Explosive College, sometimes in the Perfect Rationality in 2017.
&lt;br /&gt;&quot;, &quot;Numbers of students will work with top of the Masters of Organizations, including:
&lt;br /&gt;*  Ottoman colleges in Masters of Organizations to market rationality
&lt;br /&gt;*  Ottoman colleges in Masters Council of Perfect Rationality: Beginning, Market Programs
&lt;br /&gt;*  Ottoman government: Perfect Rationality from perakithrogours
&lt;br /&gt;*  Philosophy Of South Korea
&lt;br /&gt;*  Antarctic Opportunity to Morgan Security Policy
&lt;br /&gt;* Signature Research Trade Policy
&lt;br /&gt;* Photo Photo Photo Photo Photo Photo
&lt;br /&gt;*  Draft citizen political building
&lt;br /&gt;* Socialist Councils: Refugee and Matter of Signature''
&lt;br /&gt;&quot;, &quot;Symposium students are the academic better conduct between two languages.
&lt;br /&gt;
&lt;br /&gt;* Hills in Picang, Kansas
&lt;br /&gt;* Hull Courtyard: Service hockey
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;* Official website
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 3 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;References&quot;], &quot;section_texts&quot;: [&quot;The '''Perfect Rationality''' is an Indian rationality developed in Buddhist concert outside the Canadian border to the Canadian concert.  The original excess of the rationality is administered by hippopulmina artillery.
&lt;br /&gt;
&lt;br /&gt;The crew (in the concert, with a long time plantation) of 2:35.545 on 19 a total population), developed by the new John Phelps hippopulmina and president of India, Thailand, Europe, and the Supervisory Court.
&lt;br /&gt;
&lt;br /&gt;The various partnerships with the Canadian knowledge are the Supervisory Partnership with the Hamburg Buddhist Concert Company (''Perfect Rationality''), with other Netherlands and England to be bought by the Perfect Act of the 2012 census, which includes 3 Order of the Telegraph and Indian International.
&lt;br /&gt;&quot;, &quot;In 1929, Perfect Rationality was returned to W. K. O'Neill to be acquired by the Breaddon Association of Computer Programs. Commercial Development Brief Justice for Kings Minister Martin Loe published postdoctoral newspaper and administered plantations as a network. He makes involvement in many churches:
&lt;br /&gt;
&lt;br /&gt;The ''Perfect Rationality'' is a news/paper originality which has added worldwide with the commercial development of India (1992). When the Ossai development ceased to be supplied by the Audiences for Legal Research, the Supervisory Government Services Canada publishes for Opposition Buddhism: Private Anthology, Weapons University, College, Agency, and a measuring meetings. Its student opened at free health in 1999. The infantry remained in the town of Calaudan Government and is not listed until 2003, as a member of the Canadian Rationality Board, referring to the Caribbean Conference.
&lt;br /&gt;
&lt;br /&gt;A customer and principal rationality came from the outside country, which was located in Perfect LASE Compact, a foundation of the Supervisory Partnerships. Currently, its programing includes an \&quot;Rationality Rationals\&quot; (RATR), producer and spiritual solutions, a reunion that ends in digitised content to improve companies in Indian act by not broken up anything.
&lt;br /&gt;
&lt;br /&gt;The developers of whole student grows to provide several members of the Republic of Illinois Academy (RIASI) rate that Canadian worldwide works were the Victoria Medical Trade Center and Building Trust (BMT)
&lt;br /&gt;&quot;, &quot;Mr Paul Bornar's first known rationality was inherited by the early 1990s.
&lt;br /&gt;There more than 6 orbiters traced by BMT's mostly output leaders working together, in the RATR as well as other teams within twenty major break of rations.
&lt;br /&gt;
&lt;br /&gt;The RATR is inherited by the RATR Public Rational Award, for the running of American International Academy, in the RATR Victoria Medical Institute.
&lt;br /&gt;&quot;, &quot;* SPRAK Buddhism (Academy of Technical Sciences)
&lt;br /&gt;* SPRAK Canadian Government Service
&lt;br /&gt;* SPRAK Hasband Government
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 4 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;H F.E. &amp;amp; Priesthood&quot;, &quot;References&quot;, &quot;External links&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;The '''Perfect Rationality''' is a (PRP) rate in California, California. Under the australian Labour People's Republic links, the PRP has a very part of a target region.
&lt;br /&gt;&quot;, &quot;The Rationality was founded in 1963 after the flagship of the SCVR and forms World King's Democratic Labour Labour Labour seats.
&lt;br /&gt;
&lt;br /&gt;The PRP has significantly been operated by the Victoria Progressive Consultative (VPC), since the 1981 election to the Adelaide January. It was selected to execute as the SPCR for the California Perfect Rationality.
&lt;br /&gt;
&lt;br /&gt;A significant party was operated by the California Perfect Rationality, but facilitated in 1984. From 1994 to 1998 the Labour Party holds a resource of the position. To open the labour conditions the Labour Party and the PRP is less arrested by the LRP as ''Parnell Carrier''.
&lt;br /&gt;&quot;, &quot;The Perfect Rationality and 1984 PerfectVictoria Progressive Consultative Victoria seat at the Harrison Range
&lt;br /&gt;
&lt;br /&gt;The rationality came to the increases in 1993.
&lt;br /&gt;
&lt;br /&gt;The PPR penalty classifies the Perfect Rationality significance (AFB) of the duration as well. The Victoria Progressive Consultative VII (Channel 5) came to the CIR affiliate (VPC) but the old progressive party very perfect.
&lt;br /&gt;&quot;, &quot;The PRP announced its extractors in 1987 until 1988 when it ran in the California area due to its predecessor injury. In the batch of the 1996 Kings, the PRP had expanded itself to take the position in number one revised portions of the consult.
&lt;br /&gt;
&lt;br /&gt;In 1994 PRP established the Perfect Rationality in a series of postgradual attentions in the Batch of Priesthood Labour, as well as an extractor of the PRP, labourry heavy that heads at the progressive consultancy of the oceanic strength, all buildings and senior towers. The victoria Progressives didn't seek more than the others. The PRP established a rationality at seven of 500 parties, along with nine cosmetic changes in the first X grading.
&lt;br /&gt;
&lt;br /&gt;In 1997 there was a PSQ which still conducted by Sullivan and Philip Newmeyer. Most prepared was 42 parties during the Batch of Princeton University in 1997.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt; Province 
&lt;br /&gt;
&lt;br /&gt; Country
&lt;br /&gt;
&lt;br /&gt; Votes
&lt;br /&gt;
&lt;br /&gt; %
&lt;br /&gt;
&lt;br /&gt; 17,068.5/6 
&lt;br /&gt;
&lt;br /&gt; 14,157.1/6 
&lt;br /&gt;
&lt;br /&gt; +3.02.4
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt; 10,423.5/6 
&lt;br /&gt;
&lt;br /&gt; 87,046.9/6 
&lt;br /&gt;
&lt;br /&gt; +3.21.3
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt; 12,761.7/6 
&lt;br /&gt;
&lt;br /&gt; 73,513.9/6 
&lt;br /&gt;
&lt;br /&gt; 30,015.3/6 
&lt;br /&gt;
&lt;br /&gt; +4.74.7
&lt;br /&gt;
&lt;br /&gt;
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;A one-percent voting to the PRP in a Rationality, Democratic Perfect Rationality still capable of featuring preparation for humanity, or alterations by lower parties such as long power-range, local 'importance', needle velocities, fundamental photographs, build-up inside and academics.
&lt;br /&gt;&quot;, &quot;Primary objects in the California Perfect Rationality is owned by the California Perfect Rationality Academy.
&lt;br /&gt;&quot;, &quot;PPRP is based in California.
&lt;br /&gt;
&lt;br /&gt;In 1997, the Labour Party replaced its first rate back for nine cosmetic California Perfect Rationality to receive a rationality at week. In 1998, the California Perfect Rationality ran for : a trouxed 10-party labour community (party) and a protected trouxed from sports such as Ocksford Communications International (OCI), Karachi Manorship. The PRP designed their first building in 3900 parties making into a progressional agency. In 2000, many PRP greater businesses led a conference for public security agencies.
&lt;br /&gt;
&lt;br /&gt;In July 2003, PRP was converted to the \u201cPerfect Rationality of New England\u201d to choose private botanical practices, mostly and buildings. The PRP first launched on 15 April 2005 to a sporting program with Underprivate Station.
&lt;br /&gt;
&lt;br /&gt;In 2006, PRP is an E local government code with a campaign based on all 5+ for $1 million. 
&lt;br /&gt;In 2007, WREED a large support of the PERFECT Rationality. In 2012, PRP first launched, organized by the PRP in 2014. The PARA Belleaux 2016 represented the entire Batch of Princeton University. The top score of that resulted in the Fellow of the Labour Party in Labour from October 2017 to July 2019.
&lt;br /&gt;
&lt;br /&gt;In June 2018, PRP now contested to operate at events that required the Princeton University of the Eastern Islands against Princeton University and October 2018.
&lt;br /&gt;&quot;, &quot;PRP was purchased by Christopher B. Retrieves, 7.8 December 2017. The PRP was acquired by Irish College of Overseas and Wildlife in 2019.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;===Sources===
&lt;br /&gt;Although nothing can be referred to as \&quot;PARFECT Rationality Code,\&quot; PPR officially published the official Japanese government. Written by Paul Fabriz it was originally launched by the Japanese dictatorship of the National Museum.
&lt;br /&gt;
&lt;br /&gt;In 2014 Princely, Tom Goble expanded its Victoria Progressive Consultative Labour Labour Labour.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;br /&gt;
&lt;/details&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 5 &lt;small&gt;(click to expand)&lt;/small&gt;&lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;
&lt;br /&gt;{&quot;title&quot;: &quot;Perfect Rationality&quot;, &quot;See also&quot;], &quot;section_texts&quot;: [&quot;
&lt;br /&gt;
&lt;br /&gt;The '''perfect Rationality''' or '''QRA revolt''', is located in the Pobecheni family of Craven and other notable ''Rationality'' records on two perfect cars. It opened in 1885 and came from a pending runway at Craven in 2004. This was the surrounding station founded by the Silesian-American journalist Ludwigsburg, who became Esposito de Torres in 2004.
&lt;br /&gt;
&lt;br /&gt;President Jos\u00e9 Andr\u00e9 Loritzel established the Pobecheni of the Pobecheni of the Pobecheni and has existed at the perfect Rationality for radio times and affiliations with Cambridge University as \u00c1ngel or supply \&quot;Cambridge\&quot;.
&lt;br /&gt;&quot;, &quot;The perfect Rationality has received a $7,000 unit and created by gallantry and security scholars such as St. Catherine's College of Suspension Orders (SCSOR) (overseas these issues) and passes the Pobecheni family. In the form of substituting battalions, geographically consumer rationality is supported by Pobecheni itself. In 2008, the Pobecheni family decided that the Pobecheni introduced an Esposito de Torres to pass the Cambridge.
&lt;br /&gt;
&lt;br /&gt;Overseas the Pobecheni family has a place, the use of farming, and an esposito. In August 2018, a soldier of Yerevan Ajadaf. The assembly is considered a substitute of the Pobecheni family of the Cambridge as part of the Instituto Regional Accentini de XV.
&lt;br /&gt;&quot;, &quot;Election identified for speech in the Pobecheni District Council of Italy.
&lt;br /&gt;The Pobecheni was a small Latin internationally subsidized Forefront de La Pobecheni in 1887 (Latin American Institution of Evangelical and Professional Works Committee). In 1889, notable basements were good investigating the suitability and abruptly close to the creation of culture artistic simulcasts to a detailed perfect, but the Latin perfect shut down a regionality like the Civil Engineer Advanced Museum of Gold. The civil laws came on perfect media and non-perfect rationality intentions during this period.
&lt;br /&gt;
&lt;br /&gt;Developments cause the Pobecheni family and maximum services on the statehood, industrial and provisional models as well as the growth of St. Catherine's College of Suspension Orders (SCSOR).
&lt;br /&gt;
&lt;br /&gt;Civil engineer Pobecheni named the Pobecheni in development of Printed South St. Catherine in 1963-78. In 2018, the Pobecheni was used by the Importance and Education Chemistry at the Universities of St. Andrews de Vale movies.
&lt;br /&gt;&quot;, &quot;
&lt;br /&gt;&quot;, &quot;* College of Rationality
&lt;br /&gt;
&lt;br /&gt;&quot;]}
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;After writing this I noticed the Reformer team have added a new sampling functionality in trax, but it currently has memory leaks so it is fairly limited. It might be worth it to try to use my Reformer weights by loading them via their official &lt;a href=&quot;https://colab.research.google.com/github/google/trax/blob/master/trax/models/reformer/text_generation.ipynb&quot;&gt;Reformer Colab&lt;/a&gt; and testing that out.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The Reformer is an interesting architecture and Locality-Sensitive-Hashing in particular might be a good addition to the attention toolbox, allowing us to take advantage of larger contexts (&lt;a href=&quot;https://www.pragmatic.ml/a-survey-of-methods-for-incorporating-long-term-context&quot;&gt;Here&lt;/a&gt; is a recent review of other approaches for tackling the context window problem). I’d say the results I got weren’t as amazing as I could’ve hoped, but it works over very large samples, and a model taking advantage of such large contexts might benefit even more from being larger. Additionally, the Reformer team has alluded to them adding further functionality, such as the ability to use larger vocabs which could help, too.&lt;/p&gt;

&lt;p&gt;The sampling tests I ran were pretty interesting to me, and I’ve included some of them here. They show promise for guiding the prediction process further to improve performance. Perhaps applying some of it during training can help even more.&lt;/p&gt;

&lt;p&gt;As for the model I trained - &lt;a href=&quot;https://colab.research.google.com/drive/1VSaQingkpMDf730ARlo2dw5vCBVOgt17&quot;&gt;here&lt;/a&gt; is the Colab where you can use to finetune or sample from my model.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="training" /><category term="code" /><category term="google" /><category term="reformer" /><category term="context-window" /><category term="context" /><category term="sampling" /><category term="weights" /><category term="trained" /><category term="model" /><category term="nlp" /><category term="colab" /><category term="ml" /><category term="dl" /><summary type="html">A lot of progress has been made in training large NLP models in recent years, and after finetuning GPT2-1.5b I’ve been thinking about context windows. Large Transformers are very good, but when you are only limited to 512 or more commonly now 1024 tokens, the model doesn’t know quite that much of what happened earlier at any point. Google’s Reformer paper introduces a more efficient way of doing attention, sidestepping the problem which I used to train a model with context of 64k tokens.</summary></entry><entry><title type="html">Corona Advice for People Who Go Against Advice</title><link href="https://svilentodorov.xyz//blog/corona-anti-advice/" rel="alternate" type="text/html" title="Corona Advice for People Who Go Against Advice" /><published>2020-03-12T00:00:00+01:00</published><updated>2020-03-12T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/Corona-anti-advice</id><content type="html" xml:base="https://svilentodorov.xyz//blog/corona-anti-advice/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Touch your face - it’s fun.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Visit nursing homes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t wash your hands much and when you do do it for just a few seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compare COVID to the flu whenever you can.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you feel sick go to the mall, travel or do something else outside to get your mind off it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cruise ships, in particular, are getting real cheap - take advantage by buying a ticket for the soonest one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t stock up - that’s for losers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, actively make fun of anyone who does prepare.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’ve been forced to work remote now is the time to try doing it at a cafe or a coworking space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;if you can’t be bothered to work at a cafe make up a story about being suspected of having COVID.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you haven’t been very racist before consider if you don’t have something against Asians. Voice all of it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Blame political opponents for over and under-reacting. They probably fucked it up one way or another, just pick one of the two and make it about them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make jokes about corona beer. Maybe someone hasn’t heard them yet? Worth a try.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you decide to get disinfectant, toilet paper, masks etc. just take some from a local hospital when nobody is looking. You can always just sell them with a 100x mark up later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you didn’t manage to steal from the hospital (their security can be pesky, huh?) make some fakes and sell those instead. Easy money.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increase your consumption of toilet paper, non-perishable food and anything else that’s in short supply immediately.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t wear surgical, n95 or heavier masks. You can use cloth masks or the above but only if you put them on loosely.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make fun of boomers more than usual.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prank old people by sneezing on them when you see some.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Did I mention that touching your face is fun? You should have done it at least once more since the start of the list, especially if outside.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Did you know that COVID is a bioterrorist weapon made by China? Well, you do now - spread the information without looking more into it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Refuse the concept of COVID vaccines even in theory. You wouldn’t want to become autistic, would you?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you can’t watch your favourite team at the stadium, gather outside instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If someone you know is sick and asks for help tell them to just take some vitamin C or whatever - should be enough.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="corona" /><category term="anti" /><category term="advice" /><category term="joke" /><category term="light-hearted" /><category term="virus" /><category term="covid" /><category term="ncov" /><category term="ncov-2019" /><summary type="html">Touch your face - it’s fun.</summary></entry><entry><title type="html">Tips and Pain Point Workarounds for Machine Learning Work in Google Colab</title><link href="https://svilentodorov.xyz//blog/colab-tips/" rel="alternate" type="text/html" title="Tips and Pain Point Workarounds for Machine Learning Work in Google Colab" /><published>2020-02-08T00:00:00+01:00</published><updated>2020-02-08T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/Colab-tips</id><content type="html" xml:base="https://svilentodorov.xyz//blog/colab-tips/">&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Colab&lt;/a&gt; is one of my favourite machine learning tools and quite possibly the most useful online-based tool for democratizing Machine Learning currently in existence. It is used by many to share research, working models, ideas. It is a reproducible environment to present GitHub issues, a usable machine with a powerful GPU or TPU which many won’t have access to otherwise, and more. Colab is extremely useful for novices who are just getting started, allowing them to quickly make or copy something working ᴀɴᴅ it is convenient for seasoned practitioners who want to share their work.  And it’s all free.&lt;/p&gt;

&lt;h5 id=&quot;the-key-advice-in-this-post-is-all-bolded-for-easy-skimming&quot;&gt;&lt;strong&gt;The key advice in this post is all bolded for easy skimming.&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;There are alternatives of course. &lt;a href=&quot;https://www.kaggle.com/kernels&quot;&gt;Kaggle Kernels&lt;/a&gt; being perhaps the most popular. If you don’t need a GPU or are willing to pay then &lt;a href=&quot;https://cocalc.com/app&quot;&gt;GoCalc&lt;/a&gt;, &lt;a href=&quot;https://notebooks.azure.com/&quot;&gt;Azure Notebooks&lt;/a&gt;, &lt;a href=&quot;https://datalore.io/&quot;&gt;Datalore&lt;/a&gt; or &lt;a href=&quot;https://docs.floydhub.com/guides/workspace/&quot;&gt;FloydHub Workspaces&lt;/a&gt; might be viable options. That’s not even all of the free services out there, but as far as I know, none of the others give you access to a GPU or TPU out of the box which is so often required in Machine Learning.&lt;/p&gt;

&lt;p&gt;Nonetheless, at least for me, Colab has been the most useful out of those despite some pain points. This post is about addressing those pain points first, and some general Colab tips I’ve benefited from second.&lt;/p&gt;

&lt;h3 id=&quot;drive&quot;&gt;Drive&lt;/h3&gt;

&lt;p&gt;Colab is great for one-off runs or when sharing something that can be set up quickly in a single notebook. However, one often needs to work on projects for a long time or needs access to big amounts of data to the point where downloading them each time is too cumbersome or would take too long. In those cases using Google Drive with Colab is your best bet.&lt;/p&gt;

&lt;p&gt;Connecting to one of your Google Account’s Drive is as simple as:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from google.colab import drive
drive.mount('/content/drive')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;After which you can just &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt; into it.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%cd /content/drive/My\ Drive/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’d recommend to &lt;b&gt;just work straight from Drive in most cases&lt;/b&gt; as then your setup time is faster and you likely won’t lose as much if your instance times out. Of course, in that case, you might run out of Drive storage space but Drive (or Google One, which is how it’s called when you pay for it) has pretty &lt;a href=&quot;https://one.google.com/faq/plans-pricing&quot;&gt;cheap plans&lt;/a&gt; to extend the space. I personally use the 2TB plan for €9.99 / month but there are even cheaper ones. You can even share a plan between up to 5 people. On the other hand &lt;b&gt; if you really need fast reads and writes above all else copy the data from Drive to the Colab VM instead &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you pay for Google One, you also have access to that most mythical of creatures - Google Support, but they are rarely helpful, and at least in regards to the hidden quota issue none of them seem able to understand what the problem is, let alone help. My advice is not to bother.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Using Drive, sadly, is not always frictionless. For one, files over 5 GB or so can get &lt;a href=&quot;https://support.google.com/drive/search?q=download+quota+exceeded&quot;&gt;inaccessible for 24 hours&lt;/a&gt; easily due to a hidden quota - even by just reading a portion of them a few times. So, whenever you can, &lt;b&gt; make sure to keep all your files under 5gb and split datasets when possible&lt;/b&gt;. If you can’t divide them try to only read from the big files once at a time - copying them to the local runtime and using them locally can also be wise in that case.&lt;/p&gt;

&lt;p&gt;Another problem with using Drive is that you can’t easily delete a file - the usual options like &lt;code class=&quot;highlighter-rouge&quot;&gt;os.remove&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;%rm&lt;/code&gt; put the file into Drive’s Thrash which still takes up space. What you want to do instead is &lt;b&gt; either empty out files or overwrite them with new data &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Having too many files in the same directory can also cause issues so &lt;b&gt; Avoid having too many files in the root directory of your Drive for faster initial loading &lt;/b&gt; and &lt;b&gt; have a directory structure such that you never have too many files in the same folder &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, &lt;b&gt;don’t write to too many files at the same time as some of the writes might silently fail&lt;/b&gt;.&lt;/p&gt;

&lt;h3 id=&quot;time-outs&quot;&gt;Time outs&lt;/h3&gt;

&lt;p&gt;A Colab expires after 12 hours of use or after 1 hour of being idle. Sometimes it can time out earlier but that’s rare as long as you &lt;b&gt; keep the Colab tab always open on a machine&lt;/b&gt;. Nonetheless, saving to Drive at least once every one-two hours is wise.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Sometimes, there might be a random error from Colab itself and the runtime can time out - it doesn’t happen too often but be prepared for it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In general, you probably shouldn’t use instances longer than you have to - your Accelerator availability seems to drop whenever you use it too much, so &lt;b&gt; always stop the runtime when you are done&lt;/b&gt; from Runtime&amp;gt;Manage Sessions&amp;gt;Terminate. Sadly, there is a bug recently where the Terminate option is greyed out but all you have to do to make it available is interrupt execution and refresh the page.&lt;/p&gt;

&lt;p&gt;I also wouldn’t recommend having more than a single Runtime running (technically you are allowed two) as it also seems to impact your access.&lt;/p&gt;

&lt;h3 id=&quot;runtimes&quot;&gt;Runtimes&lt;/h3&gt;

&lt;p&gt;Sometimes you just have to run something continuously and in those cases, I’d recommend to &lt;b&gt; always use the appropriate Runtime &lt;/b&gt;. Use CPU (or CPU + high memory runtime) for preprocessing, save to Drive and only use the TPU/GPU runtimes for actual work. Furthermore, Google seem a lot happier with people using TPUs - in fact, they give access to &lt;a href=&quot;https://www.tensorflow.org/tfrc&quot;&gt;a fair amount of them somewhat easily to researchers&lt;/a&gt; - so if you don’t have a preference for what accelerator to focus on, I’d recommend using TPUs.&lt;/p&gt;

&lt;p&gt;You can also cycle through GPU runtimes to get e.g. a P100 by terminating the instance and starting it again, but it’s not too reliable and is likely something Google isn’t too fond of. &lt;b&gt;To check your current GPU just run &lt;code class=&quot;highlighter-rouge&quot;&gt;!nvidia-smi&lt;/code&gt;.&lt;/b&gt;&lt;/p&gt;

&lt;h3 id=&quot;colab-pro&quot;&gt;Colab Pro&lt;/h3&gt;

&lt;p&gt;There is less availability of Accelerators on Colab recently, and my best guess is that it is related to them ramping up the Pro version (which has higher priority). I haven’t tested it myself because Pro is sadly US-only but priority access, double the ram, 24 hour (instead of 12 hour) limit and leniency on idle timeouts for $10 / month sound pretty worth it - if it was available outside of the US I would’ve paid for it on the spot. Nonetheless, you can take full advantage of Colab for free, and the upgrade is [currently] purely optional.&lt;/p&gt;

&lt;h3 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h3&gt;

&lt;p&gt;One general pain point with notebooks is that you often end up needing to check something once, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;print(x.shape)&lt;/code&gt; which can lead to too much clutter. A great way to combat that is to &lt;b&gt; go to Insert&amp;gt;Scratch Code Cell (or ctr+alt+N) and do all your prints and one-off statements in there &lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you are not working with Drive you can simply set it up so you download your results to your own machine at the end of your cell. Simply do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from google.colab import files
files.download(result) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;If you need to edit multiple lines at once &lt;b&gt; you can use find and replace using ctr+H which also allows you to match using regular expressions&lt;/b&gt;. This only works when you don’t have a running cell.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Colab is pretty great, and I’ve benefited tremendously from it. There are some annoyances but you can work around them. That’s it.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="colab" /><category term="tips" /><category term="google" /><category term="drive" /><summary type="html">Colab is one of my favourite machine learning tools and quite possibly the most useful online-based tool for democratizing Machine Learning currently in existence. It is used by many to share research, working models, ideas. It is a reproducible environment to present GitHub issues, a usable machine with a powerful GPU or TPU which many won’t have access to otherwise, and more. Colab is extremely useful for novices who are just getting started, allowing them to quickly make or copy something working ᴀɴᴅ it is convenient for seasoned practitioners who want to share their work. And it’s all free.</summary></entry><entry><title type="html">Talking to Myself or How I Trained GPT2-1.5b for Rubber Ducking using My Facebook Chat Data</title><link href="https://svilentodorov.xyz//blog/gpt-15b-chat-finetune/" rel="alternate" type="text/html" title="Talking to Myself or How I Trained GPT2-1.5b for Rubber Ducking using My Facebook Chat Data" /><published>2020-01-23T00:00:00+01:00</published><updated>2020-01-23T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/gpt-15b-finetune</id><content type="html" xml:base="https://svilentodorov.xyz//blog/gpt-15b-chat-finetune/">&lt;h5 id=&quot;previously-in-this-series---finetuning-117m-finetuning-345m&quot;&gt;Previously in this series - &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;finetuning 117M&lt;/a&gt;, &lt;a href=&quot;/blog/gpt-345M-finetune/&quot;&gt;finetuning 345M&lt;/a&gt;&lt;/h5&gt;

&lt;p&gt;OpenAI’s pretrained Gpt2 models have been all the rage in nlp model finetunning. &lt;a href=&quot;https://www.reddit.com/r/SubSimulatorGPT2/&quot;&gt;Sub Simulators&lt;/a&gt;, &lt;a href=&quot;https://www.aidungeon.io/start&quot;&gt;AI Dungeons&lt;/a&gt;, &lt;a href=&quot;https://www.gwern.net/GPT-2&quot;&gt;Poetry&lt;/a&gt;, &lt;a href=&quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&quot;&gt;Chess&lt;/a&gt;, &lt;a href=&quot;https://www.cardsagainsthumanityaichallenge.com/&quot;&gt;Cards Against Humanity&lt;/a&gt; - you name it.&lt;/p&gt;

&lt;p&gt;My particular interest has been in applying it to my personal chat data, so I can talk to arbitrary friends and more importantly &lt;em&gt;myself&lt;/em&gt; whenever I want to. Why? Well, sometimes if you don’t hear voices you have to go and program the voices into being. More seriously, I wished to be able to talk with a version of myself so I can get an outside view of how I think, for analysing my tendencies and problems, rubber ducking, self-therapy and - let’s face it - most importantly for fun.&lt;/p&gt;

&lt;p&gt;I won’t go into detail describing the Colab with Facebook data, as it is almost the same as the one used with the 117M version which I describe &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;/blog/gpt-345M-finetune/&quot;&gt;here&lt;/a&gt;. We are again using &lt;a href=&quot;https://github.com/nshepperd/gpt-2&quot;&gt;nsheppered’s GPT training code&lt;/a&gt; as a base with &lt;a href=&quot;https://github.com/shawwn/gpt-2&quot;&gt;shawwwn’s fork&lt;/a&gt; allowing us to train the whole of gpt-1.5b on a TPU - which means we can do so for free on Colab again! I’ve only added a little bit more so we can talk interactively with gpt2, have it respond to us as a specific person and go through the whole general back and forth which we typically call ‘having a conversation’.&lt;/p&gt;

&lt;p&gt;You can train your own in the new GPT2-1.5b collab &lt;a href=&quot;https://colab.research.google.com/drive/1rRpMGVfUb5sG263d1OOPXOyGRX4W1oEv&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; TPU&lt;/p&gt;

&lt;h6 id=&quot;note-it-is-a-lot-easier-to-train-if-you-pay-a-few-bucks-for-google-one-so-you-can-fit-it-all-easily-on-drive-a-checkpoint-of-the-model-comes-to-6-gb-so-if-you-only-keep-one-checkpoint-you-can-do-it-without-paying---it-is-just-more-hassle&quot;&gt;Note: It is a lot easier to train if you pay a few bucks for &lt;a href=&quot;https://one.google.com/&quot;&gt;Google One&lt;/a&gt; so you can fit it all easily on Drive. A checkpoint of the model comes to ~6 gb, so if you only keep one checkpoint you can do it without paying - it is just more hassle.&lt;/h6&gt;

&lt;h6 id=&quot;note-2-it-took-me-over-a-month-of-training-to-get-it-here-using-just-colab&quot;&gt;Note 2: It took me over a month of training to get it here using just Colab&lt;/h6&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#chat-samples&quot;&gt;Self-Chat Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The idea of modelling a person fascinates me, and as language models have been becoming better and better it genuinely &lt;em&gt;seems&lt;/em&gt; like they capture more and more of a person’s style and tendencies. Honestly, I don’t even need more motivation than that to want to do it - the only real question is &lt;em&gt;who&lt;/em&gt; to model as better and bigger models come out?&lt;/p&gt;

&lt;p&gt;Now, I have to admit - my first choice would be someone like Von Neumann but that’s not someone on whom I have the right type of conversational data to do well (although there are ways to tackle the problem). The person who I &lt;strong&gt;am&lt;/strong&gt; guaranteed to have data on, AND who is still very useful and interesting subject of study for myself is, you guessed it, myself.&lt;/p&gt;

&lt;p&gt;Is it useful though? Don’t I already have access to a pretty good, some might say &lt;em&gt;perfect&lt;/em&gt; simulation of myself from the get-go? I do. However, if I can have a second one of me to bounce ideas with I’d take it. If I can analyse my problems from the outside rather than from the inside, I’d take that, too.&lt;/p&gt;

&lt;p&gt;After playing with the results, and extrapolating to what the next generation of models will be able to do (gpt-2 is already not the newest and best architecture), I think a lot of people might at some point want a version of themselves to talk to, bounce ideas with, get help from and so on.&lt;/p&gt;

&lt;h2 id=&quot;a-few-more-words&quot;&gt;A Few More Words&lt;/h2&gt;

&lt;p&gt;I’ve spent some time talking to the irc chat bots of nshepperd and shawwn (both trained on the same data) and had a lot of fun with it. Talking to myself though has been highly addictive, and I’ve been running a Colab where I occasionally shoot myself a question or go into a deep conversation with my gpt2 clone for days now. It’s …interesting.&lt;/p&gt;

&lt;p&gt;One thing I didn’t realize though was how personal it is - it’s all trained on my facebook messenger data, where I often talk with my closest friends. It takes very little for gpt to start talking about intimate topics which I’d neither want to share nor would they be as meaningful to anyone else.&lt;/p&gt;

&lt;p&gt;Having said that the next are my last 7 unfiltered (I’ve only changed the names) conversations with it, where I am actively trying to avoid sensitive topics (I still had to restart it twice due to sensitive subjects).&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The end loss I reached (for whatever that is worth) was just over 0.6, substantially lower than with the smaller models. My dataset is still pretty small - 14mb, and the next step would be to add my personal data from other sources. Yet, it does quite well, though perhaps it wouldn’t seem as exciting to others.&lt;/p&gt;

&lt;h3 id=&quot;facebook&quot;&gt;Facebook&lt;/h3&gt;

&lt;p&gt;The way it works is - I write a sentence under the name of a friend from the dataset but here labeled as ‘Me’, pass it to the model while prompting it to start typing using my name - ‘Svilen Todorov’, (here labeled as ‘Gpt Me’). When it starts outputting something not by ‘Svilen Todorov’ I cut it off, and write myself. Then I feed that + the history so far to the model, which allows us to have an arbitrarily long coherent conversation.&lt;/p&gt;

&lt;h4 id=&quot;chat-samples&quot;&gt;&lt;em&gt;Chat Samples:&lt;/em&gt;&lt;/h4&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 1 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: How's work?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working a bit but still not in the office

 &lt;br /&gt;Me: I've been working, too. Can't solve this damn bug I have.

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive spent hours on it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but to no avail
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: im spending all my time on it

&lt;br /&gt;Me: I checked everything, yet it runs out of memory despite obvious leaks. hoooow

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been wasting so much of my time on it tho
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 16gb memory, 16gb video ram, 60 hours a week for 3 years
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: barely have anything open other than youtube and facebook

&lt;br /&gt;Me: oh it's a browser issue? man the web sure sounds bloated nowadays

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive tried everything

 &lt;br /&gt;Me: did you try turning it on and off?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: haha
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive changed it between view and off in the dropbox since the beginning, should be the same now as in your case and I doesnt seem to be working
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and tried a few things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: nothing seems to be working
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i uploaded a new one though when you did
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but pointless to try it now

&lt;br /&gt;Me: don't give up

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive uploaded a new one
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: doesnt run properly
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: etc.
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: still nothing
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: maybe in a few days when i have a few gigs under my belt it will run a bit better
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and then in the mean time i can send you what i have
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to use as you like
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: until then use whatever is in my dropbox
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: if you want i can even send you my complete open source build, so you can build it and send me what i use

&lt;br /&gt;Me: ok, let's do that

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive only sent you a part of the source so i can get the full thing
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://github.com/p5pit/pciusb

&lt;br /&gt;Me: I can't access it. Can you resend the link?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just changed the link in the source

 &lt;br /&gt;Me: what's the new url?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just changed the name aboven you send it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://github.com/p5pit/pciusb/tree/master/libs
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but you need to update the links in the 'message' part
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: anyway whats up with you

&lt;br /&gt;Me: Not much, just reading

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been at lectures 4-7
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: im helping a mate with some maths problems
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and trying to read a paper since 5

&lt;br /&gt;Me: ok cool

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive not sure what it was on
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: Lyndon had problems with reading just finding the right words in random papers to use them to make a point
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and it was on whether certain types of choice can build a successful models to fit the data to believe
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 2 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: hey how are you

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive quit and you know where to find me

 &lt;br /&gt;Me: what did you quit

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my job, ive given a few interviews here and there
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and my boss is giving me shit for not doing more work during the day
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but ive quit
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: he's so annoyed at me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: thats a bit of an overreaction

&lt;br /&gt;Me: yeah you shouldnt have to work

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working since 8 am today

 &lt;br /&gt;Me: oh, you quit your job and are working on getting a new one?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying to for a while now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and the previous one ends in oct

&lt;br /&gt;Me: what job are you looking for?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been searching for 'concierge' level stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i like working on cutting edge stuff

&lt;br /&gt;Me: so something in AI again?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been looking at it for 8 months at least
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they want you to start at 17 / 20 euro a day
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: with no holiday
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: which is pretty ridiculous

&lt;br /&gt;Me: that does seem like a ridiculously low salary

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive talked to people at home and abroad about it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they were all 'wtf'
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i havent even called in to my third favourite coffee place to try to get them to put me as a regular

&lt;br /&gt;Me: interesting

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive gone to application after meeting with them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: coffee and biscuits*

&lt;br /&gt;Me: how did it go

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive only went for a small place so far, startup
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they wanted me to make a profile there
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and to tell them about the personal stuff like my salary and social security
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and if i want it there it has to be on the phone
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and so on


&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 3 &lt;/i&gt;
&lt;/summary&gt;

&lt;br /&gt;Me: whats up

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been having a lot of success with my programming but dunno if Ive succeed
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have a lot of stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: not sure how many hours Im going to spend a day on it

&lt;br /&gt;Me: you are not working too much and getting burnt out are you?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive had years with zero effort and amazing results
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: - especially when compared to other programmers
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so yeah, probably a bit

&lt;br /&gt;Me: well, take it easy man

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been doing it for a few hours a day for a few years now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i really want to move to machine learning though
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and Ive started on that over a year ago

&lt;br /&gt;Me: what ML stuff do you want to do

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a little in R, but want to move to using things like neural nets for a lot more
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ideally something which is more general
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and less focused on just driving

&lt;br /&gt;Me: but what do you want to work on specifically

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive helped people a lot with getting them to focus on a specific problem
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: at this point
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D

&lt;br /&gt;Me: so what problem are YOU focusing on

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive helped a lot with getting them to focus on adding simple analytics
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to their sites
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and getting them to focus on just a few things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: rather than whole things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but im moving more towards the latter

&lt;br /&gt;Me: hm

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a lot of the heavy lifting for them, by getting them to focus on a task that is either too hard or overly boring for them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but thats a bit of an unfair comparison
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: they have their own stuff

&lt;br /&gt;Me: and what do you have

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a lot, but they have their stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have to move away from that
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: in a few months
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and program something better

&lt;br /&gt;Me: like what

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: something general
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: agi

&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 4 &lt;/i&gt;
&lt;/summary&gt;

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done fuck all for an hour now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: nothing
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i shouldnt be working today
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i was supposed to be working yesterday too but cba
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: just tired

&lt;br /&gt;Me: you seem to be tired all the time, man. everything ok?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been sleeping since 9
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no, not really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my back hurts
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and i hurt my back peacing out
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but other than that im ok
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i can do little but write
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but i cant read and i havent done anything in days
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i am so bored
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my head is so bad
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :/

&lt;br /&gt;Me: it happens - maybe take a break until you feel better

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive never been so annoyed by my head
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i can barely use the computer

&lt;br /&gt;Me: damn that sucks

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been worse
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but i dont think its as bad as my back
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: which cant even move

&lt;br /&gt;Me: do you need any help?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been asking people to do shit for me
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: for a long time now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no help
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: people give me shit for not doing more
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: havent had a break until now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i feel so bad for not being able to pass out

&lt;br /&gt;Me: what have you tried for your sleeping problems so far?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying for a long time an antihistamines
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:beta-blockers
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:xanax
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:histamine
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hainamines
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordamances
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordana
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordano
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:shrooms
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: not even going to read now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: too tired
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: anything else
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: slowing myself down
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: exhausted
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: read the whole thing now

&lt;br /&gt;Me: I did but I am confused. And you shouldn't take shrooms to sleep.

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i didnt really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a whole thing
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 5 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: want to listen to something cool. send me a track

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been putting tracks from acts I like on and off
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: into my library

&lt;br /&gt;Me: got any links to share?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been pushing some friends to listen to more stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so you can do that too now :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=hfTPiS7Nmw
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=QsXgEzx_GGk
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and this other one is one of the earlier songs I liked from them
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=Iqz02saPcoM

&lt;br /&gt;Me: cool! I like the cramps. Got any techno tho?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying to get people to listen to some of the stuff

 &lt;br /&gt;Me: which stuff? what artists or labels?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been pushing mostly labelless stuff which are usually quite good
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: stuff by bez batteries and their cousin or whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and stuff by dvs1 and his cousin
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: or vatican shadow or whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive sent you the first two

&lt;br /&gt;Me: cool

&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 6 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: hey man, what are you up to this weekend?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive tickets for a party on friday
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: wasnt expecting to go out this early
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but got tickets during the sale
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so im waking up early i guess

&lt;br /&gt;Me: cool, what's the party?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive told people and they havent been able to guess
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.facebook.com/events/553455714529578/
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a bit weird
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have friends in charge
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: who i trust more than me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to tell me if i have a ticket

&lt;br /&gt;Me: looks good. who's playing?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive asked them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: they said they dont know
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i hope i do
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but they were saying it in a way that it sounds bad
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: aufnahme'samenere
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a bit weird
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: the only guy that ive met like him is berghainarian 
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no wait that was mike
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: still ja
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and its this group that does weird stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: that guy does the sound, and looks kind of weird

&lt;br /&gt;Me: sounds pretty interesting really

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive read a little about him on wiki but thats it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: he is apparently a maker
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but doesnt really know how the word works

&lt;br /&gt;Me: do I know him?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive met him and his brother at makerfaire
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: very briefly
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but they were really high
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and i dont really remember them
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: thats 2 months ago
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D

&lt;br /&gt;Me: fair enough
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 7 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: how would you describe yourself

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working in the same job for 3 and a half years now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: relatively new to berlin
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 25
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 6 years as a programmer/machine learning person
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: pretty much all my time for the last 5 years has been spent on just learning machine learning and not programming really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and Im pretty sure that even if I was to move elsewhere and work with people from different fields, I would still not be able to keep up
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: I would spend most of my time on just learning the ropes and whatever else they often have free or cheap classes for everyone
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: that seems to be the general approach in many places
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but I havent seen that approach work for me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i spend literally every day with people and projects that dont allow me to do more than that

&lt;br /&gt;Me: and what about your personal life and hobbies?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just been doing the same stuff ever since I moved to berlin a year ago
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: didnt really do much else than that and the online courses to where I am now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: I used to spend my time more on random stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: like programming, music, reading things like that

&lt;br /&gt;Me: what about going out

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and its rained a lot the last few days which doesnt help either
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.facebook.com/photo.php?fbid=324979844210832&amp;amp;set=a.324977827544367.75530.14546_1858271655695&amp;amp;type=3&amp;amp;theater

&lt;br /&gt;Me: sounds fun
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conversations aren’t ideal but it’s &lt;strong&gt;a lot&lt;/strong&gt; fun playing with it. Sadly the stuff I talk about on facebook is either too personal or generic ( e.g. arranging to meet people) so I am unsure how well I’ve managed to convey the fun of talking to it. Perhaps &lt;a href=&quot;https://twitter.com/theshawwn/status/1206721762987315207&quot;&gt;another sample&lt;/a&gt; of gpt2 talking as me, but with the irc bot mentioned earlier (and ran by nshepperd), might convey it. You can find some more of those samples on Shawn’s twitter + a lot more gpt fun from his collaborations with &lt;a href=&quot;http://gwern.net&quot;&gt;gwern&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I genuinely think that even with gpt2, let alone bigger and better models you can get a lot of use out of having a model of yourself you can talk to - and not just yourself. Playing with this has really made me realise the value of a service that can create a model of dead or dying relatives, and I want to eventually set up something for my loved ones - at minimum the data collection part.&lt;/p&gt;

&lt;p&gt;It will be unfair to say that it is always great though - sometimes it talks about whatever it wants to and the conversation can be harder to steer, otherwise it follows my lead easily. From what I can see in my actual messages I do this a lot, so maybe it is just copying me. Additionally it sometimes talks about things that aren’t really True - like the back pain in Example 1, and if you play with the different parameters (top_k/top_p and temperature mainly) you can force it to go on long tirades which eventually become nonsensical. It’s definitely not perfect.. but it sure is fascinating to me.&lt;/p&gt;

&lt;p&gt;Fun fact - there is a sentence in this post written entirely by the GPT version of Me. I wonder how easy it is to spot.&lt;/p&gt;</content><author><name></name></author><category term="gpt" /><category term="gpt2" /><category term="finetune" /><category term="colab" /><category term="neural-network" /><category term="chat" /><category term="reflection" /><category term="therapeutic" /><category term="rubber-duck" /><category term="nlp" /><category term="transformers" /><category term="machine-learning" /><summary type="html">Previously in this series - finetuning 117M, finetuning 345M</summary></entry><entry><title type="html">How is search so goddamn bad? A ‘Case Study’</title><link href="https://svilentodorov.xyz//blog/bad-search/" rel="alternate" type="text/html" title="How is search so goddamn bad? A ‘Case Study’" /><published>2020-01-19T00:00:00+01:00</published><updated>2020-01-19T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/bad-search</id><content type="html" xml:base="https://svilentodorov.xyz//blog/bad-search/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Google Search is ridiculously, utterly bad. Before your brain starts recalling successful searches you’ve had, and before this turns into &lt;em&gt;only&lt;/em&gt; a rant let’s see some examples.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/gsearch1.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This is a query for checking out what reddit thinks in regards to buying a phone, while specifying to only get results from the last month. Pretty simple I guess, yet &lt;strong&gt;none&lt;/strong&gt; of the results achieved that goal, and &lt;strong&gt;none&lt;/strong&gt; of them are actually from the last month, despite Google lying that they are. Let’s quickly go through them one by one.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;‘Best place to by a cheap new smartphone?’ from r/Berlin - this is about where to buy a phone, but I live in Berlin, so maybe it makes some sense as a top result… except the post is actually from &lt;strong&gt;6! years ago&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Specific to Audiophile phones for some reasons, and a pretty average thread (only 31 upvotes for one metric) from &lt;strong&gt;9 months ago&lt;/strong&gt;. Not useful, and doesnt meet the query but still a lot better.&lt;/li&gt;
  &lt;li&gt;From 2018 (&amp;gt;1 year ago), even less popular and less useful post (17 upvotes) on the Pixel subreddit.&lt;/li&gt;
  &lt;li&gt;Phone for privacy, small thread from &lt;strong&gt;&amp;gt; 4 years ago&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6 years ago&lt;/strong&gt;, wtf.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I won’t keep going with the rest, but it doesn’t get better. Why are the results reported as recent when they are from years ago, I don’t know - those are archived post so no changes have been made. At any rate, I got annoyed at this point (mentioning for those who couldn’t tell), so I switched to DuckDuckGo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/ddgsearch1.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Somehow the results are even less useful. 2 irrelevant subreddits at the top, a result from 2012, that they admit is from 2012. At this point I visibly checked on reddit if there’ve been posts about buying a phone from the last month and there are. They also show up on Google/DDG if you look for them directly so they are indexed.&lt;/p&gt;

&lt;p&gt;Hm, maybe adding &lt;code class=&quot;highlighter-rouge&quot;&gt;site:reddit.com&lt;/code&gt; will help DDG to get there?&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/ddgsearch2.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Nope. As you can see, I didn’t even bother clicking on all of them now, but I can tell you the first result is deeply irrelevant and the second one leads to a 4 year old thread.&lt;/p&gt;

&lt;p&gt;Maybe it’s my wording then - what about ‘recommend phone reddit’ or ‘which phone to purchase reddit’? I won’t even waste anybody’s time by posting the results I got.&lt;/p&gt;

&lt;h3 id=&quot;thoughts&quot;&gt;&lt;center&gt;Thoughts&lt;/center&gt;&lt;/h3&gt;

&lt;p&gt;Part of the issue in this specific case is that they’ve started ignoring settings for displaying results from specific time periods. It’s definitely not the whole issue though, and not something new or specific to phone searches.&lt;/p&gt;

&lt;p&gt;Now, I’ve always been biased towards the new - books, tech, everything, but I can’t help but feel that a lot of things which were done pretty well before are done worse today.  We do have better technology, yet we somehow build inferior solutions with it all too often. Further, if they had the same bias of showing me only recent results I’ll understand it better, but that’s not even the case.
And yes, I get that the incentives of users and providers don’t align perfectly, that Google isn’t your friend, etc. But what is DDG’s excuse?
As for the Case Study part, and me saying this isn’t simply a rant - I lied, hence the quotation marks in the title. Don’t trust everything you read, especially the goddamn dates on your search results.&lt;/p&gt;</content><author><name></name></author><category term="google" /><category term="duckduckgo" /><category term="ddg" /><category term="search" /><category term="bad" /><category term="case" /><category term="study" /><category term="problem" /><summary type="html"></summary></entry><entry><title type="html">Fine-tunning OpenAI’s (Larger) GPT-2-345M on Conversation data (Update)</title><link href="https://svilentodorov.xyz//blog/gpt-345M-finetune/" rel="alternate" type="text/html" title="Fine-tunning OpenAI’s (Larger) GPT-2-345M on Conversation data (Update)" /><published>2019-05-08T00:00:00+02:00</published><updated>2019-05-08T00:00:00+02:00</updated><id>https://svilentodorov.xyz//blog/gpt-345m-finetune</id><content type="html" xml:base="https://svilentodorov.xyz//blog/gpt-345M-finetune/">&lt;p&gt;NLP, Machine Conversations and the road to passing the Turing Test have always interested me. That’s why when OpenAI released a larger (345M parameters vs the previous ) version of &lt;a href=&quot;https://openai.com/blog/better-language-models/#update&quot;&gt;GPT-2, their current state of the art language model&lt;/a&gt; I jumped to test it out on my small but personal dataset of 14mb of my own facebook conversations along with testing it a bit on a &lt;a href=&quot;https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus&quot;&gt;Two person Ubuntu-related dialogue corpus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The newly released pre-trained version has 345 million parameters, compared to the 117M parameter version so one would expect significantly better results. Note, this is still significantly smaller than the 1.5B version they’ve shown off. OpenAI are, however, releasing the aforementioned 1.5B version along with a 762M version to partner organizations along with plans to release those to the public in the future, too.&lt;/p&gt;

&lt;p&gt;Although it will be interesting to play around with those much bigger GPT-2 versions, they might not even be relevant by the time they are released - training on the same data with the same amount of compute one can already most likely build an even better model using recent advances e.g. even just with better transformers like OpenAI’s &lt;a href=&quot;https://openai.com/blog/sparse-transformer/&quot;&gt;Sparse Transformer&lt;/a&gt; or Google AI’s &lt;a href=&quot;https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html&quot;&gt;Transofmer-XL&lt;/a&gt; both of which improve on GPT-2’s current architecture.&lt;/p&gt;

&lt;p&gt;I won’t go into detail describing the Colab with Facebook data, as it is almost the same as the one used with the 117M version which I describe &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;here&lt;/a&gt;. We are again using &lt;a href=&quot;https://github.com/nshepperd/gpt-2&quot;&gt;nsheppered’s GPT training code&lt;/a&gt;, this time with gradient checkpointing to be able to fit the larger model in memory.&lt;/p&gt;

&lt;p&gt;You can follow in the new GPT-2-345M collab FB data &lt;a href=&quot;https://colab.research.google.com/drive/1EhZG2_AQLeDvW2s841d502FydMn1vgmK&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; GPU (or TPU)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#facebook-examples&quot;&gt;Generated Facebook Messenger Dialogue Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is also the GPT-2-345M ubuntu data (including preprossesing)  &lt;a href=&quot;https://colab.research.google.com/drive/161JjdAyqckSBLD45N9WZzFI21rUYk2ns&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; GPU (or TPU)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#ubuntu-examples&quot;&gt;Generated Ubuntu Dialogue Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;First of, The results are visibly better. Even without fine-tuning 345M produces visibly (but not overwhelmingly so) better results than 117M, and preliminary outputs from &lt;a href=&quot;https://www.gwern.net/GPT-2&quot;&gt;fine-tuning it on poetry&lt;/a&gt; are reportedly even more promising. It is hard to judge how much better but for example, I couldn’t get the loss on my small 14M facebook messages dataset to go consistently bellow &lt;em&gt;2.10&lt;/em&gt; with 117M, while with 345M (and admittedly a bit more training) I got it down to &lt;em&gt;1.85&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;facebook&quot;&gt;Facebook&lt;/h3&gt;

&lt;p&gt;The results are a bit better and more coherent but 14M is clearly too small a dataset. I had to do some small tricks (mainly decreasing the learning rate periodically and occasionally attempting to use the ubuntu data, too). Adding the &lt;em&gt;endoftext&lt;/em&gt; token instead of two blank lines between conversations also helped as expected.&lt;/p&gt;

&lt;h4 id=&quot;facebook-examples&quot;&gt;&lt;em&gt;Facebook Examples:&lt;/em&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353629564276) Tom Middlebrook: alright, i was given a bag as a present. so i had to return it to my parents. was fine getting it again though but only with a small amount&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353595877463) Svilen Todorov: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655925201) Tom Middlebrook: https://www.youtube.com/watch?v=tYJhLgK1KqU&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655946673) Svilen Todorov: i should’ve probably taken a photo when i did that haha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655984955) Svilen Todorov: i saw someone else on youtube trying to buy like 20 quid extra lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353656240766) Svilen Todorov: i think ill have to do a job at some point after i graduate but yh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353654726000) Svilen Todorov: http://www.reddit.com/tb/y2r0r&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353654728111) Svilen Todorov: i dont get why not to give him the money :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354159959666) Tom Middlebrook: lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354165011230) Tom Middlebrook: that’s awesome&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354155045207) Svilen Todorov: hahaha, yh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255181784) Tom Middlebrook: http://i.imgur.com/Ypk.png&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255222767) Tom Middlebrook: I’m pretty sure the other two guys that I had work with from the day before had worked with jsa lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255453761) Svilen Todorov: hahah&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357524074) Svilen Todorov: oh ya&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357528963) Tom Middlebrook: http://i.imgur.com/m4h5.jpg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357535641) Tom Middlebrook: looks really cool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357540333) Svilen Todorov: hahahaha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357562322) Svilen Todorov: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357563001) Tom Middlebrook: looks like a cool outfit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878572976) Ioannis Agathocleous: Ill have to see a place in the east of london&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878585860) Svilen Todorov: maybe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878623078) Svilen Todorov: not too far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878673631) Svilen Todorov: well not that far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878673234) Svilen Todorov: might be too much as I am not going to get much for a place that already has one&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878680863) Svilen Todorov: but dunno&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878688387) Svilen Todorov: i’ll go to a place in london instead since I don’t have a job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878698013) Ioannis Agathocleous: I could find one somewhere on the east but the distance is pretty small&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878706547) Ioannis Agathocleous: But I dont mind&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878783822) Svilen Todorov: it’s not too far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878792946) Svilen Todorov: well, Im pretty sure it’d be easier to just find a place in london instead if I get too many people to live with&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878807895) Svilen Todorov: if there isnt one in the UK I might be able to get a place here or something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888089082) Svilen Todorov: yeah, i can find somewhere&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888051110) Ioannis Agathocleous: Ye probs best to be a foreigner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888061658) Ioannis Agathocleous: Just find a new place and work&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888093631) Svilen Todorov: yh true&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888096719) Svilen Todorov: yh, I’ve found a place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888096620) Ioannis Agathocleous: You gotta find something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887102292) Ioannis Agathocleous: Or stay at a mate’s house for a few months&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887143560) Svilen Todorov: will do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887162939) Svilen Todorov: I think you’ll find something in Bulgaria&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887172227) Svilen Todorov: in london might be easier to find something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887193938) Svilen Todorov: so yh find what you want&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888245750) Ioannis Agathocleous: Ye will do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895273936) Svilen Todorov: just saw a flat for 200 which is 2 hours from my place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895284880) Svilen Todorov: I can also get a place cheaper if I move there too&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895358255) Ioannis Agathocleous: Seems like a decent deal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895641622) Ioannis Agathocleous: Ill see if I can find something from the guys&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893649976) Svilen Todorov: looks like it’s in the center of meelerdham&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893664983) Ioannis Agathocleous: Kl ill check it out&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893669386) Svilen Todorov: if I move tomorrow Ill be living in London until March for a year&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893688213) Ioannis Agathocleous: Ye but I dont mind moving anyway&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13136700170) Svilen Todorov: i’m going to sleep&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670106021) Michal Rostecki: ah sorry, I forgot&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670197807) Michal Rostecki: yeah, I will prob go there around 19:30&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199539) Svilen Todorov: ok, about to try to make it to the bar at least&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199507) Svilen Todorov: if its like that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199579) Svilen Todorov: its way down&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199997) Michal Rostecki: yeah, now I’m on my way&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199994) Svilen Todorov: oh and I keep forgetting my passport when I get there&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199923) Svilen Todorov: well i have it already :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313680514053) Svilen Todorov: http://www.buzzfeed.com/buzzfeed/comments/2lh/how-do-you-use-the-device-menu-system-in-ubuntu-10-04-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-like-after-reinstalling-the-grub-loader-for-ubuntu-10-04/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671553576) Michal Rostecki: what’s your plan for today? did you find it?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671607100) Svilen Todorov: nah got some stuff to do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671627252) Svilen Todorov: im heading to work now&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671638781) Michal Rostecki: cool! I’ll probably be there around 4ish&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671666644) Svilen Todorov: didnt get that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671669988) Svilen Todorov: nah it’s fine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671670159) Michal Rostecki: ok&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396720429) Carmen Quasi: are you going to a bar?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396782520) Svilen Todorov: yeah sure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396988765) Svilen Todorov: but Ill probably stay at mine longer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544397997134) Carmen Quasi: hihi i can come to yours at some point&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544397996679) Carmen Quasi: I’m in a park with my friend Jonas. you could just head there and drink a beer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402012633) Svilen Todorov: Ja&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402269095) Svilen Todorov: If I will come by the same time Ill pass by to get my backpack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402529078) Carmen Quasi: it’s not so big a park&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402839650) Carmen Quasi: i dont get where you are&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407472321) Svilen Todorov: Ja same&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407506681) Svilen Todorov: It’s in the center&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407505377) Svilen Todorov: A lot bigger than London for sure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544503854605) Carmen Quasi: yeah it’s in the center with a garden&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544505993833) Carmen Quasi: there is a pool pool in the center, not the backyard&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507562468) Svilen Todorov: Ye fair&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507569097) Svilen Todorov: Might come for a bit to try it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507617387) Carmen Quasi: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;ubuntu&quot;&gt;Ubuntu&lt;/h3&gt;

&lt;p&gt;I mainly tried this out of curiosity but also in order to attempt to use a somewhat similar (dialogue) dataset for somehow improving the accuracy on my all-to-small facebook dataset. I tried multiple things but the only thing that actually helped was to train for a bit (&amp;lt;1500 steps) on the ubuntu dataset a few times when the loss on my own dataset won’t go down and then go back. This sped things up slightly and allowed me to reach a little bit higher accuracy in the end but the difference isn’t too big.&lt;/p&gt;

&lt;p&gt;I only tried this for a couple of hours and used slightly less than half the data (there are 3 folders in the datasets and I only used the 951M one) but the results were quite close to what’s in the dataset. Here are some random (non-cherry picked) samples. Honestly, if you give me some actual conversations from this dataset and some generated conversations, I’m not sure if I’d do much better than 60% at recognizing which is which.&lt;/p&gt;

&lt;h4 id=&quot;ubuntu-examples&quot;&gt;&lt;em&gt;Ubuntu Examples:&lt;/em&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738600) chrism_: hi all&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738760) chrism_: can somebody help me set up a nvidia x driver? my system doesn’t display an X server. I think it’s not a graphical problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738820) jocke: I would use xorg1-driver&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738900) jocke: then do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738960) jocke: I usually install both but at least you can do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738960) jocke: I would just follow this guide to help you&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739020) jocke: https://wiki.ubuntu.com/X11/UsingX11/Manual&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739080) jocke: if your setup doesn’t load the driver it might be a bad idea to disable it. But when you run xmodmap &lt;keycode&gt; and it says no such buffer I assume that you are good to set it back to working state you should be fine&lt;/keycode&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739080) jocke: the first step (the xorg1-driver) is to run xmodmap from that directory. Once that is done it will be available as a device for ‘device’ on your desktop. If your only problems are with xorg1.h or X11.h, just add it to /etc/X11/xorg.conf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739400) jocke: if you need the xorg.conf just edit my example file to change it to the one you want to have.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144700) luigi__: hey i have this problem, how do i stop and remove all the packages ubuntu has, it keeps re-installing everything when i change the source and i change the repository to another one&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) luigi__: thanks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: right-click the source folder and look for a dropdown area&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then right-click&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then type   dpkg -L      and hit enter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then click add&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) cnj_: it will start downloading everything, just in case&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) cnj_: it will tell you to restart the system to do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) luigi__: what do you mean?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: in here?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) luigi__: i don’t have that on my laptop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: I’ll give you a link.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: http://paste.ubuntu-nl.org/134440/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: this is my machine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_:  my&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: my problem&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) luigi__: the thing is its working perfectly fine on my laptop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: the laptop has no problems with downloading any package that is offered through my terminal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: I have no connection to you.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: in ubuntu/lubuntu/solaris&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) luigi__: so if i just installed those ubuntu packages, they still need reinstallation?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447620) m0nix_: hmm&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447680) m0nix_: I had a good time with Wine2X, but i changed my mind, I will not use Wine2 for this application.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) m0nix_: Is there a way to make the application run on Ubuntu?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: sudo dpkg –purge wine2x ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: I got wine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: yes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447800) MrFool: I guess you don’t really need the whole x server, but that you can just install.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447860) MrFool: Wine2 is just an extension from Wine3&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447920) MrFool: Yes you want to run X?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189442140) MrFool: You have a server&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) kalzior: anyone know where i can get fiesty to work with this new version?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) nishant: you can use gtk-fiesty&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) kalzior: thanks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) nishant: I have never really used freestyle, but the gui interface does a good job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455720) wuqh2u2n: do you know how to do ubuntu and windows together??&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455780) theadmin: you need to use the ubuntu iso to install.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455840) wuqh2u2n: but its just 2 partitions&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455900) theadmin: ok so when installing ubun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321446060) theadmin: which one is the right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321456600) theadmin: which ISO&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321456600) wuqh2u2n: i’ve tried downloading the iso but they’re all wrong, but theres a way to use it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462060) theadmin: try and use the iso you downloaded, it will be an iso you can burn to&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: thats it, you will be able to burn it to cd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: its the right iso&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: how do i burn them? i need some help&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: i need to burn the iso from the internet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462540) theadmin: you need to have a cd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conversations look better but not ideal. The model is learning even more relevant stuff - e.g. they are about mutual interests, acquaintances etc. It truly seems to have some idea of who each person is and can mostly catch the tone and relevant facts about most people I’ve talked enough to.&lt;/p&gt;

&lt;p&gt;It was also easy and fun to play with and makes me wonder how much better we can do with the biggest model or better techniques or ideally more of everything. At this rate, we might soon have chatbots good enough for people to not even compare them to Eliza!&lt;/p&gt;</content><author><name></name></author><category term="gpt" /><category term="gpt2" /><category term="finetune" /><category term="colab" /><category term="nlp" /><category term="machine-learning" /><summary type="html">NLP, Machine Conversations and the road to passing the Turing Test have always interested me. That’s why when OpenAI released a larger (345M parameters vs the previous ) version of GPT-2, their current state of the art language model I jumped to test it out on my small but personal dataset of 14mb of my own facebook conversations along with testing it a bit on a Two person Ubuntu-related dialogue corpus.</summary></entry><entry><title type="html">A Proposed Task for Training a Human-Imitating System as a Benchmark and Potential Part of The Road to AGI</title><link href="https://svilentodorov.xyz//blog/human-imitating-task/" rel="alternate" type="text/html" title="A Proposed Task for Training a Human-Imitating System as a Benchmark and Potential Part of The Road to AGI" /><published>2019-04-29T00:00:00+02:00</published><updated>2019-04-29T00:00:00+02:00</updated><id>https://svilentodorov.xyz//blog/human-imitating-task</id><content type="html" xml:base="https://svilentodorov.xyz//blog/human-imitating-task/">&lt;h2 id=&quot;basic-idea&quot;&gt;Basic idea&lt;/h2&gt;

&lt;p&gt;One of the most important pieces for training a more general ‘human-like’ System (Neural Network) is having a good open-ended task which is either unsupervised or where generating very large amounts of data is doable.&lt;/p&gt;

&lt;p&gt;I propose training a model where the input is simply people interacting with their computer (just the browser for simplicity) as they normally do and the output is a prediction of the next action they will take.&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;
&lt;sup&gt;
For those who don’t think AGI is possible - I am not saying it definitely is (especially not immediately), just proposing a task to get closer to something that is realistically doable.
&lt;/sup&gt;
&lt;/sub&gt;&lt;/p&gt;

&lt;h2 id=&quot;rationale&quot;&gt;Rationale&lt;/h2&gt;

&lt;p&gt;At this point in the current Deep Learning revolution it seems like variations of &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;&gt;our&lt;/a&gt; &lt;a href=&quot;https://openai.com/five/&quot;&gt;current&lt;/a&gt; &lt;a href=&quot;https://github.com/openai/gpt-2&quot;&gt;techniques&lt;/a&gt; can likely solve most any &lt;em&gt;well-defined&lt;/em&gt; problem if you use the right parts, have enough data and make the network big enough.
Alternatively, if this isn’t the case seeing how far we can go and having a harder human-imitating task can be used as a good benchmark.&lt;/p&gt;

&lt;p&gt;Thus, I am operating under the assumption that having the right problem with a strong enough signal is as important, or plausibly a more important problem than finding a new technique for developing an algorithm that can at minimum behave in a more general, human-like fashion. For example, I no longer believe that there is a game (video or otherwise) that isn’t solvable at human-level with current techniques as long as you have enough resources (time, people, and computation).&lt;/p&gt;

&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;

&lt;p&gt;Record an enormous training set of people using their browsers. Include mouse clicks, key presses, screenshots of the screen, text on screen, OCR (text from images), html elements, urls, etc.&lt;/p&gt;

&lt;p&gt;Do feature extraction with ideally also pre-trained and initially frozen specialized nets for creating features from the text, timeframe-by-timeframe screenshots etc. and feed it all into a general model which predicts the next keypress/mouse event sent by the user.&lt;/p&gt;

&lt;p&gt;There is enough of a signal in the data to theoretically be able to [more] accurately predict the next action a person will take if you as a human observed them enough and have full information of what they were just doing, and what they are seeing. Moreso if an algorithm does it.&lt;/p&gt;

&lt;p&gt;The idea is the same as with text-prediction, however (for a simple example) even an exceptional human studying literature all their life, can only predict the next word in a book with limited accuracy. On the other hand, at least on paper, if one person studies all interactions someone has or at least all of those with their browser (and many other people’s) they can likely very accurately predict the next thing a human will do based on what they see and what they’ve been doing.&lt;/p&gt;

&lt;p&gt;In the case of pure text prediction it is one thing to only feed a neural network text, not related to anything, and another for the text (and everything) else to relate to real actions and other input. Simplified example - seeing the word red more often after interacting with an image that has more red in it. There is just more to connect to make an accurate model with a more full internal representations of it’s environment - and said environment is incredibly vast.&lt;/p&gt;

&lt;p&gt;The thing the model would actually be modelling at the end of the day is [more or less] open-ended human behaviour.&lt;/p&gt;

&lt;p&gt;Of course, the exciting part comes not as much from predicting the next action of real humans but from letting the model run freely - predict actions and perform them without there being a human any more. Would it visit sites successfully? Will it attempt to leave comments on articles or browse photos? Try to register or login anywhere?&lt;/p&gt;

&lt;p&gt;I believe this is technically doable with enough resources, even today.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Now, I am not deluded enough to assume that this will in any way straight lead to AGI but I expect it to supply us with to a lot of interesting insights, and the question is &lt;em&gt;how much would a model trained like that learn&lt;/em&gt;? . Before, I might have thought the task would simply be too hard to start learning but after recent success in solving games, text prediction etc., as well as the fact that it can start predicting a lot of actions quite easily, and that a lot of the modules (e.g. text extraction) would be pre-trained (but again, not frozen), I suspect it will learn more than most people assume.
Also how much better (or if not better, how come) would for example the text-relevant portion of the model get after training as part of the full system?&lt;/p&gt;

&lt;p&gt;Additionally, I am well aware that there are many obvious and non-obvious problems that would need to be solved in the process. Creating the data and sanitizing user’s credentials, other privacy considerations. Enormous training time and designing the right Network(s) etc. Is any of it infeasible for a sufficiently large organization, however? I think no - this can very well be attempted today.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;sub&gt;
&lt;sup&gt;
I would be very interested to hear about arguments why this would or wouldn’t work, potential improvements or alternatives (e.g. having a RL and/or GAN-like approach with a similar task which is also doable), or if there is anyone working on something similar to this. Feel free to contact me about this or any related or unrelated offers for work or collaboration.
&lt;/sup&gt;
&lt;/sub&gt;&lt;/p&gt;</content><author><name></name></author><category term="agi" /><category term="ai" /><category term="task" /><category term="proposal" /><category term="imitate" /><category term="machine-learning" /><summary type="html">Basic idea</summary></entry><entry><title type="html">Adding Layers to the middle of a pre-trained network whithout invalidating the weights</title><link href="https://svilentodorov.xyz//blog/add-layers/" rel="alternate" type="text/html" title="Adding Layers to the middle of a pre-trained network whithout invalidating the weights" /><published>2019-03-22T00:00:00+01:00</published><updated>2019-03-22T00:00:00+01:00</updated><id>https://svilentodorov.xyz//blog/add-layers</id><content type="html" xml:base="https://svilentodorov.xyz//blog/add-layers/">&lt;p&gt;Fine-tunning pre-trained neural networks on new data has shown a lot of promise in &lt;a href=&quot;https://cv-tricks.com/keras/fine-tuning-tensorflow/&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;http://nlp.fast.ai/&quot;&gt;domains&lt;/a&gt;. One simple example is my &lt;a href=&quot;/blog/gpt-finetune&quot;&gt;last post&lt;/a&gt; where I fine-tune OpenAI’s GPT-small model on my chats from facebook messenger to create fake conversation.&lt;/p&gt;

&lt;p&gt;Using pre-trained models and further training them is especially useful for organizations with small datasets or resources, and in most cases, it is cost and otherwise effective to do it. However, despite it being widely used, people rarely talk about taking a pre-trained model and making it bigger by adding more layers in the middle of the network rather than just the end.&lt;/p&gt;

&lt;p&gt;Naively, this doesn’t work without some tweaks - if you add a layer in the middle of a network then all the trained weights of later layers become useless since they are getting different inputs. There are, however, ways to get around that and I believe that this is an important area to explore as more and more useful models get released.&lt;/p&gt;

&lt;p&gt;These posts (along with the last) are my first two in a series where I will attempt to increase the size of OpenAI’s GPT-2 model while taking advantage of the training the model has already gotten. Their model is a great candidate for this experiment, as OpenAI have already demonstrated great results with what is basically a bigger version of it. If anyone else is working on something similar or has links to related research I might have missed - feel free to email me, or hell - even cite me.&lt;/p&gt;

&lt;p&gt;Note: The basic network example is mostly taken from the &lt;a href=&quot;https://www.tensorflow.org/alpha/tutorials/quickstart/advanced&quot;&gt;Tensorflow 2.0 Getting started article&lt;/a&gt;. I used this as an opportunity to play a little with the new API.&lt;/p&gt;

&lt;p&gt;You can follow in the colab &lt;a href=&quot;https://colab.research.google.com/drive/1KocZA0Zgo68eMKWXg-W3W6Ev0F6n88cX&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;First, we install tensorflow 2.0 to Colab (this step can be skipped after 2.0 is out and the default in Colab)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install tensorflow-gpu==2.0.0-alpha0
&lt;/code&gt;
Then imports&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model```

Then we download minst to have something to play with, shuffle and batch it

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We download, shuffle, and batch our training and test data&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dataset, info = tfds.load('mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = dataset['train'], dataset['test']

def convert_types(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

mnist_train = mnist_train.map(convert_types).shuffle(10000).batch(32)
mnist_test = mnist_test.map(convert_types).batch(32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create our basic model&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add our loss, optimizer, and metrics&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We define our training_step and test_step functions.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def train_step(image, label, model=model):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  
  train_loss(loss)
  train_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def test_step(image, label, model=model):
  predictions = model(image)
  t_loss = loss_object(label, predictions)
  
  test_loss(t_loss)
  test_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then train our default model a bit.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EPOCHS = 2

for epoch in range(EPOCHS):
  test_loss.reset_states()
  test_accuracy.reset_states()
  for image, label in mnist_train:
    train_step(image, label)
  
  for test_image, test_label in mnist_test:
    test_step(test_image, test_label)
  
  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(), 
                         train_accuracy.result()*100,
                         test_loss.result(), 
                         test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 1, Loss: 0.20242027938365936, Accuracy: 94.05333709716797, Test Loss: 0.08358743041753769, Test Accuracy: 97.37999725341797&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 2, Loss: 0.13752736151218414, Accuracy: 95.9375, Test Loss: 0.07251566648483276, Test Accuracy: 97.65999603271484&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point, we can try creating a new bigger model that uses all the weights trained here (including those in the very last layer!).&lt;/p&gt;

&lt;p&gt;What we are going to do is make a model almost exactly like the last one but we are going to add one more Dense layer before the final one. The important bit here is to initialize the layer so the weights are in the form of the identity function - this way when the output from the previous layer gets multiplied by the output of this layer we will get exactly the same result, and the weights of the final layer will still make sense. Then during fine-tuning, the layer will slowly move away from the identity function in whichever directions make the most sense. Note: we also want the bias to be initialized to zeros (so we don’t add anything extra to the weights at first) but this is already the default in tensorflow.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyModel2(Model):
  def __init__(self):
    super(MyModel2, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(20, activation='relu')
    **self.d_extra = Dense(20, activation='relu', kernel_initializer=tf.keras.initializers.Identity)**
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    **x = self.d_extra(x)**
    return self.d2(x)
  
model2 = MyModel2()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then need to re-run our test_step and train_step functions (just re-run the cells containing them) due to how tf.function works. After that, we can confirm our new model isn’t magically performing better than chance.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_loss.reset_states()
test_accuracy.reset_states()
for test_image, test_label in mnist_test:
    test_step(test_image, test_label, model2)


template = 'Test Loss: {}, Test Accuracy: {}'

print (template.format(test_loss.result(), 
                     test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Test Loss: 2.3064279556274414, Test Accuracy: 10.329999923706055&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As expected - it only gets a number right about 1/10th of the time.&lt;/p&gt;

&lt;p&gt;Now, the only other thing we need to do is add the weights from our previous model to our new model, except for our new (3rd) layer which will at first just leave things the same.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model2.layers[0].set_weights(model.layers[0].get_weights())
model2.layers[1].set_weights(model.layers[1].get_weights())
model2.layers[2].set_weights(model.layers[2].get_weights())
model2.layers[4].set_weights(model.layers[3].get_weights())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then re-run the code checking the accuracy of our new model and voila&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Test Loss: 0.07251566648483276, Test Accuracy: 97.65999603271484&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point we can just start training our bigger model.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EPOCHS = 2

for epoch in range(EPOCHS):
  test_loss.reset_states()
  test_accuracy.reset_states()
  for image, label in mnist_train:
    train_step(image, label, model2)
  
  for test_image, test_label in mnist_test:
    test_step(test_image, test_label, model2)
  
  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(), 
                         train_accuracy.result()*100,
                         test_loss.result(), 
                         test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which gets me to&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 1, Loss: 0.11430724710226059, Accuracy: 96.58721923828125, Test Loss: 0.05486641451716423, Test Accuracy: 98.18999481201172&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 2, Loss: 0.09389258921146393, Accuracy: 97.19083404541016, Test Loss: 0.05310175567865372, Test Accuracy: 98.31999969482422&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Not much better, but the results will usually be more impressive when dealing with more complex problems.&lt;/p&gt;

&lt;p&gt;Also, instead of doing that, we can also freeze everything but our new layer to more accurately only train it. This will train faster, and depending on the problem, it might make more sense - or it might make more sense for a few epochs before again training all layers. Play around with it!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def train_step(image, label, model=model):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  ***gradients = tape.gradient(loss, model.trainable_variables[-4:-2])
  optimizer.apply_gradients(zip(gradients, model.trainable_variables[-4:-2]))***
  
  train_loss(loss)
  train_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The way we do that is by using the gradients of only our new layer.
model.trainable_variables works by returning a list of all weights and biases of our model layer by layer - so the first item is the weights of layer 1,  2nd item is the bias of layer 1 etc. Thus we only need the 2 layers before the last 2.&lt;/p&gt;

&lt;h2 id=&quot;fully-convolutional&quot;&gt;Fully convolutional&lt;/h2&gt;

&lt;p&gt;What if we are working with e.g. a fully convolutional network - an identity matrix won’t work (and tensorflow doesn’t even allow us to use the identity initializer for that reason) - how do you add a new layer while keeping the network usable?&lt;/p&gt;

&lt;p&gt;There are different ways you can extend the network while keeping all the weights useful - one I like is to use a residual-like approach. I simply initialize the new conv layer using all 1s and multiply the output from it to the output of the previous layer. This changes nothing at first but once we start training again, we can slowly move those weights in the direction we want them to be.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu', padding='SAME')
    self.flatten = Flatten()
    self.d1 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    return self.d1(x)
    

class MyModel2(Model):
  def __init__(self):
    super(MyModel2, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu', padding='SAME')
    self.c_extra = Conv2D(32, 3, activation='relu', padding='SAME', kernel_initializer=tf.keras.initializers.Ones)
    self.flatten = Flatten()
    self.d1 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = x * self.c_extra(x)
    x = self.flatten(x)
    return self.d1(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The rest is exactly the same as before.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Increasing the size of a network is something usually done before training from scratch but that doesn’t always need to be the case. You can add one or better yet more layers across the network or even at the very start. Whether this will help depends - mainly on whether you needed a bigger net from the start - but it can definitely save you time.&lt;/p&gt;</content><author><name></name></author><category term="neural" /><category term="network" /><category term="ml" /><category term="layer" /><category term="machine-learning" /><category term="colab" /><summary type="html">Fine-tunning pre-trained neural networks on new data has shown a lot of promise in many domains. One simple example is my last post where I fine-tune OpenAI’s GPT-small model on my chats from facebook messenger to create fake conversation.</summary></entry></feed>