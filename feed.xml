<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.6">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2020-03-12T13:53:55+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Svilen Todorov</title><subtitle>Personal site for Data Scientist and Machine Learning Engineer - Svilen Todorov</subtitle><entry><title type="html">Corona Advice for People Who Go Against Advice</title><link href="http://localhost:4000/blog/corona-anti-advice/" rel="alternate" type="text/html" title="Corona Advice for People Who Go Against Advice" /><published>2020-03-12T00:00:00+01:00</published><updated>2020-03-12T00:00:00+01:00</updated><id>http://localhost:4000/blog/Corona-anti-advice</id><content type="html" xml:base="http://localhost:4000/blog/corona-anti-advice/">&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Touch your face - it’s fun.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Visit nursing homes.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t wash your hands much and when you do do it for just a few seconds.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Compare COVID to the flu whenever you can.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you feel sick go to the mall, travel or do something else outside to get your mind off it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Cruise ships, in particular, are getting real cheap - take advantage by buying a ticket for the soonest one.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t stock up - that’s for losers.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Further, actively make fun of anyone who does prepare.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you’ve been forced to work remote now is the time to try doing it at a cafe or a coworking space.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;if you can’t be bothered to work at a cafe make up a story about being suspected of having COVID.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you haven’t been very racist before consider if you don’t have something against Asians. Voice all of it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Blame political opponents for over and under-reacting. They probably fucked it up one way or another, just pick one of the two and make it about them.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make jokes about corona beer. Maybe someone hasn’t heard them yet? Worth a try.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you decide to get disinfectant, toilet paper, masks etc. just take some from a local hospital when nobody is looking. You can always just sell them with a 100x mark up later.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you didn’t manage to steal from the hospital (their security can be pesky, huh?) make some fakes and sell those instead. Easy money.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Increase your consumption of toilet paper, non-perishable food and anything else that’s in short supply immediately.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Don’t wear surgical, n95 or heavier masks. You can use cloth masks or the above but only if you put them on loosely.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Make fun of boomers more than usual.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Prank old people by sneezing on them when you see some.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Did I mention that touching your face is fun? You should have done it at least once more since the start of the list, especially if outside.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Did you know that COVID is a bioterrorist weapon made by China? Well, you do now - spread the information without looking more into it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Refuse the concept of COVID vaccines even in theory. You wouldn’t want to become autistic, would you?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you can’t watch your favourite team at the stadium, gather outside instead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If someone you know is sick and asks for help tell them to just take some vitamin C or whatever - should be enough.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="corona" /><category term="anti" /><category term="advice" /><category term="joke" /><category term="light-hearted" /><category term="virus" /><category term="covid" /><category term="ncov" /><category term="ncov-2019" /><summary type="html">Touch your face - it’s fun.</summary></entry><entry><title type="html">Tips and Pain Point Workarounds for Machine Learning Work in Google Colab</title><link href="http://localhost:4000/blog/colab-tips/" rel="alternate" type="text/html" title="Tips and Pain Point Workarounds for Machine Learning Work in Google Colab" /><published>2020-02-08T00:00:00+01:00</published><updated>2020-02-08T00:00:00+01:00</updated><id>http://localhost:4000/blog/Colab-tips</id><content type="html" xml:base="http://localhost:4000/blog/colab-tips/">&lt;p&gt;&lt;a href=&quot;https://colab.research.google.com/&quot;&gt;Colab&lt;/a&gt; is one of my favourite machine learning tools and quite possibly the most useful online-based tool for democratizing Machine Learning currently in existence. It is used by many to share research, working models, ideas. It is a reproducible environment to present GitHub issues, a usable machine with a powerful GPU or TPU which many won’t have access to otherwise, and more. Colab is extremely useful for novices who are just getting started, allowing them to quickly make or copy something working ᴀɴᴅ it is convenient for seasoned practitioners who want to share their work.  And it’s all free.&lt;/p&gt;

&lt;h5 id=&quot;the-key-advice-in-this-post-is-all-bolded-for-easy-skimming&quot;&gt;&lt;strong&gt;The key advice in this post is all bolded for easy skimming.&lt;/strong&gt;&lt;/h5&gt;

&lt;p&gt;There are alternatives of course. &lt;a href=&quot;https://www.kaggle.com/kernels&quot;&gt;Kaggle Kernels&lt;/a&gt; being perhaps the most popular. If you don’t need a GPU or are willing to pay then &lt;a href=&quot;https://cocalc.com/app&quot;&gt;GoCalc&lt;/a&gt;, &lt;a href=&quot;https://notebooks.azure.com/&quot;&gt;Azure Notebooks&lt;/a&gt;, &lt;a href=&quot;https://datalore.io/&quot;&gt;Datalore&lt;/a&gt; or &lt;a href=&quot;https://docs.floydhub.com/guides/workspace/&quot;&gt;FloydHub Workspaces&lt;/a&gt; might be viable options. That’s not even all of the free services out there, but as far as I know, none of the others give you access to a GPU or TPU out of the box which is so often required in Machine Learning.&lt;/p&gt;

&lt;p&gt;Nonetheless, at least for me, Colab has been the most useful out of those despite some pain points. This post is about addressing those pain points first, and some general Colab tips I’ve benefited from second.&lt;/p&gt;

&lt;h3 id=&quot;drive&quot;&gt;Drive&lt;/h3&gt;

&lt;p&gt;Colab is great for one-off runs or when sharing something that can be set up quickly in a single notebook. However, one often needs to work on projects for a long time or needs access to big amounts of data to the point where downloading them each time is too cumbersome or would take too long. In those cases using Google Drive with Colab is your best bet.&lt;/p&gt;

&lt;p&gt;Connecting to one of your Google Account’s Drive is as simple as:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from google.colab import drive
drive.mount('/content/drive')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;After which you can just &lt;code class=&quot;highlighter-rouge&quot;&gt;cd&lt;/code&gt; into it.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%cd /content/drive/My\ Drive/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I’d recommend to &lt;b&gt;just work straight from Drive in most cases&lt;/b&gt; as then your setup time is faster and you likely won’t lose as much if your instance times out. Of course, in that case, you might run out of Drive storage space but Drive (or Google One, which is how it’s called when you pay for it) has pretty &lt;a href=&quot;https://one.google.com/faq/plans-pricing&quot;&gt;cheap plans&lt;/a&gt; to extend the space. I personally use the 2TB plan for €9.99 / month but there are even cheaper ones. You can even share a plan between up to 5 people. On the other hand &lt;b&gt; if you really need fast reads and writes above all else copy the data from Drive to the Colab VM instead &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you pay for Google One, you also have access to that most mythical of creatures - Google Support, but they are rarely helpful, and at least in regards to the hidden quota issue none of them seem able to understand what the problem is, let alone help. My advice is not to bother.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Using Drive, sadly, is not always frictionless. For one, files over 5 GB or so can get &lt;a href=&quot;https://support.google.com/drive/search?q=download+quota+exceeded&quot;&gt;inaccessible for 24 hours&lt;/a&gt; easily due to a hidden quota - even by just reading a portion of them a few times. So, whenever you can, &lt;b&gt; make sure to keep all your files under 5gb and split datasets when possible&lt;/b&gt;. If you can’t divide them try to only read from the big files once at a time - copying them to the local runtime and using them locally can also be wise in that case.&lt;/p&gt;

&lt;p&gt;Another problem with using Drive is that you can’t easily delete a file - the usual options like &lt;code class=&quot;highlighter-rouge&quot;&gt;os.remove&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;%rm&lt;/code&gt; put the file into Drive’s Thrash which still takes up space. What you want to do instead is &lt;b&gt; either empty out files or overwrite them with new data &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Having too many files in the same directory can also cause issues so &lt;b&gt; Avoid having too many files in the root directory of your Drive for faster initial loading &lt;/b&gt; and &lt;b&gt; have a directory structure such that you never have too many files in the same folder &lt;/b&gt;.&lt;/p&gt;

&lt;p&gt;Additionally, &lt;b&gt;don’t write to too many files at the same time as some of the writes might silently fail&lt;/b&gt;.&lt;/p&gt;

&lt;h3 id=&quot;time-outs&quot;&gt;Time outs&lt;/h3&gt;

&lt;p&gt;A Colab expires after 12 hours of use or after 1 hour of being idle. Sometimes it can time out earlier but that’s rare as long as you &lt;b&gt; keep the Colab tab always open on a machine&lt;/b&gt;. Nonetheless, saving to Drive at least once every one-two hours is wise.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Sometimes, there might be a random error from Colab itself and the runtime can time out - it doesn’t happen too often but be prepared for it.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In general, you probably shouldn’t use instances longer than you have to - your Accelerator availability seems to drop whenever you use it too much, so &lt;b&gt; always stop the runtime when you are done&lt;/b&gt; from Runtime&amp;gt;Manage Sessions&amp;gt;Terminate. Sadly, there is a bug recently where the Terminate option is greyed out but all you have to do to make it available is interrupt execution and refresh the page.&lt;/p&gt;

&lt;p&gt;I also wouldn’t recommend having more than a single Runtime running (technically you are allowed two) as it also seems to impact your access.&lt;/p&gt;

&lt;h3 id=&quot;runtimes&quot;&gt;Runtimes&lt;/h3&gt;

&lt;p&gt;Sometimes you just have to run something continuously and in those cases, I’d recommend to &lt;b&gt; always use the appropriate Runtime &lt;/b&gt;. Use CPU (or CPU + high memory runtime) for preprocessing, save to Drive and only use the TPU/GPU runtimes for actual work. Furthermore, Google seem a lot happier with people using TPUs - in fact, they give access to &lt;a href=&quot;https://www.tensorflow.org/tfrc&quot;&gt;a fair amount of them somewhat easily to researchers&lt;/a&gt; - so if you don’t have a preference for what accelerator to focus on, I’d recommend using TPUs.&lt;/p&gt;

&lt;p&gt;You can also cycle through GPU runtimes to get e.g. a P100 by terminating the instance and starting it again, but it’s not too reliable and is likely something Google isn’t too fond of. &lt;b&gt;To check your current GPU just run &lt;code class=&quot;highlighter-rouge&quot;&gt;!nvidia-smi&lt;/code&gt;.&lt;/b&gt;&lt;/p&gt;

&lt;h3 id=&quot;colab-pro&quot;&gt;Colab Pro&lt;/h3&gt;

&lt;p&gt;There is less availability of Accelerators on Colab recently, and my best guess is that it is related to them ramping up the Pro version (which has higher priority). I haven’t tested it myself because Pro is sadly US-only but priority access, double the ram, 24 hour (instead of 12 hour) limit and leniency on idle timeouts for $10 / month sound pretty worth it - if it was available outside of the US I would’ve paid for it on the spot. Nonetheless, you can take full advantage of Colab for free, and the upgrade is [currently] purely optional.&lt;/p&gt;

&lt;h3 id=&quot;miscellaneous&quot;&gt;Miscellaneous&lt;/h3&gt;

&lt;p&gt;One general pain point with notebooks is that you often end up needing to check something once, e.g. &lt;code class=&quot;highlighter-rouge&quot;&gt;print(x.shape)&lt;/code&gt; which can lead to too much clutter. A great way to combat that is to &lt;b&gt; go to Insert&amp;gt;Scratch Code Cell (or ctr+alt+N) and do all your prints and one-off statements in there &lt;/b&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;If you are not working with Drive you can simply set it up so you download your results to your own machine at the end of your cell. Simply do:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from google.colab import files
files.download(result) 
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;hr /&gt;

&lt;p&gt;If you need to edit multiple lines at once &lt;b&gt; you can use find and replace using ctr+H which also allows you to match using regular expressions&lt;/b&gt;. This only works when you don’t have a running cell.&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Colab is pretty great, and I’ve benefited tremendously from it. There are some annoyances but you can work around them. That’s it.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><category term="colab" /><category term="tips" /><category term="google" /><category term="drive" /><summary type="html">Colab is one of my favourite machine learning tools and quite possibly the most useful online-based tool for democratizing Machine Learning currently in existence. It is used by many to share research, working models, ideas. It is a reproducible environment to present GitHub issues, a usable machine with a powerful GPU or TPU which many won’t have access to otherwise, and more. Colab is extremely useful for novices who are just getting started, allowing them to quickly make or copy something working ᴀɴᴅ it is convenient for seasoned practitioners who want to share their work. And it’s all free.</summary></entry><entry><title type="html">Talking to Myself or How I Trained GPT2-1.5b for Rubber Ducking using My Facebook Chat Data</title><link href="http://localhost:4000/blog/gpt-15b-chat-finetune/" rel="alternate" type="text/html" title="Talking to Myself or How I Trained GPT2-1.5b for Rubber Ducking using My Facebook Chat Data" /><published>2020-01-23T00:00:00+01:00</published><updated>2020-01-23T00:00:00+01:00</updated><id>http://localhost:4000/blog/gpt-15b-finetune</id><content type="html" xml:base="http://localhost:4000/blog/gpt-15b-chat-finetune/">&lt;h5 id=&quot;previously-in-this-series---finetuning-117m-finetuning-345m&quot;&gt;Previously in this series - &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;finetuning 117M&lt;/a&gt;, &lt;a href=&quot;/blog/gpt-345M-finetune/&quot;&gt;finetuning 345M&lt;/a&gt;&lt;/h5&gt;

&lt;p&gt;OpenAI’s pretrained Gpt2 models have been all the rage in nlp model finetunning. &lt;a href=&quot;https://www.reddit.com/r/SubSimulatorGPT2/&quot;&gt;Sub Simulators&lt;/a&gt;, &lt;a href=&quot;https://www.aidungeon.io/start&quot;&gt;AI Dungeons&lt;/a&gt;, &lt;a href=&quot;https://www.gwern.net/GPT-2&quot;&gt;Poetry&lt;/a&gt;, &lt;a href=&quot;https://slatestarcodex.com/2020/01/06/a-very-unlikely-chess-game/&quot;&gt;Chess&lt;/a&gt;, &lt;a href=&quot;https://www.cardsagainsthumanityaichallenge.com/&quot;&gt;Cards Against Humanity&lt;/a&gt; - you name it.&lt;/p&gt;

&lt;p&gt;My particular interest has been in applying it to my personal chat data, so I can talk to arbitrary friends and more importantly &lt;em&gt;myself&lt;/em&gt; whenever I want to. Why? Well, sometimes if you don’t hear voices you have to go and program the voices into being. More seriously, I wished to be able to talk with a version of myself so I can get an outside view of how I think, for analysing my tendencies and problems, rubber ducking, self-therapy and - let’s face it - most importantly for fun.&lt;/p&gt;

&lt;p&gt;I won’t go into detail describing the Colab with Facebook data, as it is almost the same as the one used with the 117M version which I describe &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;here&lt;/a&gt; and &lt;a href=&quot;/blog/gpt-345M-finetune/&quot;&gt;here&lt;/a&gt;. We are again using &lt;a href=&quot;https://github.com/nshepperd/gpt-2&quot;&gt;nsheppered’s GPT training code&lt;/a&gt; as a base with &lt;a href=&quot;https://github.com/shawwn/gpt-2&quot;&gt;shawwwn’s fork&lt;/a&gt; allowing us to train the whole of gpt-1.5b on a TPU - which means we can do so for free on Colab again! I’ve only added a little bit more so we can talk interactively with gpt2, have it respond to us as a specific person and go through the whole general back and forth which we typically call ‘having a conversation’.&lt;/p&gt;

&lt;p&gt;You can train your own in the new GPT2-1.5b collab &lt;a href=&quot;https://colab.research.google.com/drive/1rRpMGVfUb5sG263d1OOPXOyGRX4W1oEv&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; TPU&lt;/p&gt;

&lt;h6 id=&quot;note-it-is-a-lot-easier-to-train-if-you-pay-a-few-bucks-for-google-one-so-you-can-fit-it-all-easily-on-drive-a-checkpoint-of-the-model-comes-to-6-gb-so-if-you-only-keep-one-checkpoint-you-can-do-it-without-paying---it-is-just-more-hassle&quot;&gt;Note: It is a lot easier to train if you pay a few bucks for &lt;a href=&quot;https://one.google.com/&quot;&gt;Google One&lt;/a&gt; so you can fit it all easily on Drive. A checkpoint of the model comes to ~6 gb, so if you only keep one checkpoint you can do it without paying - it is just more hassle.&lt;/h6&gt;

&lt;h6 id=&quot;note-2-it-took-me-over-a-month-of-training-to-get-it-here-using-just-colab&quot;&gt;Note 2: It took me over a month of training to get it here using just Colab&lt;/h6&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#chat-samples&quot;&gt;Self-Chat Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;motivation&quot;&gt;Motivation&lt;/h2&gt;

&lt;p&gt;The idea of modelling a person fascinates me, and as language models have been becoming better and better it genuinely &lt;em&gt;seems&lt;/em&gt; like they capture more and more of a person’s style and tendencies. Honestly, I don’t even need more motivation than that to want to do it - the only real question is &lt;em&gt;who&lt;/em&gt; to model as better and bigger models come out?&lt;/p&gt;

&lt;p&gt;Now, I have to admit - my first choice would be someone like Von Neumann but that’s not someone on whom I have the right type of conversational data to do well (although there are ways to tackle the problem). The person who I &lt;strong&gt;am&lt;/strong&gt; guaranteed to have data on, AND who is still very useful and interesting subject of study for myself is, you guessed it, myself.&lt;/p&gt;

&lt;p&gt;Is it useful though? Don’t I already have access to a pretty good, some might say &lt;em&gt;perfect&lt;/em&gt; simulation of myself from the get-go? I do. However, if I can have a second one of me to bounce ideas with I’d take it. If I can analyse my problems from the outside rather than from the inside, I’d take that, too.&lt;/p&gt;

&lt;p&gt;After playing with the results, and extrapolating to what the next generation of models will be able to do (gpt-2 is already not the newest and best architecture), I think a lot of people might at some point want a version of themselves to talk to, bounce ideas with, get help from and so on.&lt;/p&gt;

&lt;h2 id=&quot;a-few-more-words&quot;&gt;A Few More Words&lt;/h2&gt;

&lt;p&gt;I’ve spent some time talking to the irc chat bots of nshepperd and shawwn (both trained on the same data) and had a lot of fun with it. Talking to myself though has been highly addictive, and I’ve been running a Colab where I occasionally shoot myself a question or go into a deep conversation with my gpt2 clone for days now. It’s …interesting.&lt;/p&gt;

&lt;p&gt;One thing I didn’t realize though was how personal it is - it’s all trained on my facebook messenger data, where I often talk with my closest friends. It takes very little for gpt to start talking about intimate topics which I’d neither want to share nor would they be as meaningful to anyone else.&lt;/p&gt;

&lt;p&gt;Having said that the next are my last 7 unfiltered (I’ve only changed the names) conversations with it, where I am actively trying to avoid sensitive topics (I still had to restart it twice due to sensitive subjects).&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;The end loss I reached (for whatever that is worth) was just over 0.6, substantially lower than with the smaller models. My dataset is still pretty small - 14mb, and the next step would be to add my personal data from other sources. Yet, it does quite well, though perhaps it wouldn’t seem as exciting to others.&lt;/p&gt;

&lt;h3 id=&quot;facebook&quot;&gt;Facebook&lt;/h3&gt;

&lt;p&gt;The way it works is - I write a sentence under the name of a friend from the dataset but here labeled as ‘Me’, pass it to the model while prompting it to start typing using my name - ‘Svilen Todorov’, (here labeled as ‘Gpt Me’). When it starts outputting something not by ‘Svilen Todorov’ I cut it off, and write myself. Then I feed that + the history so far to the model, which allows us to have an arbitrarily long coherent conversation.&lt;/p&gt;

&lt;h4 id=&quot;chat-samples&quot;&gt;&lt;em&gt;Chat Samples:&lt;/em&gt;&lt;/h4&gt;

&lt;details&gt;
&lt;summary&gt;
&lt;i&gt;Example 1 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: How's work?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working a bit but still not in the office

 &lt;br /&gt;Me: I've been working, too. Can't solve this damn bug I have.

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive spent hours on it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but to no avail
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: im spending all my time on it

&lt;br /&gt;Me: I checked everything, yet it runs out of memory despite obvious leaks. hoooow

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been wasting so much of my time on it tho
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 16gb memory, 16gb video ram, 60 hours a week for 3 years
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: barely have anything open other than youtube and facebook

&lt;br /&gt;Me: oh it's a browser issue? man the web sure sounds bloated nowadays

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive tried everything

 &lt;br /&gt;Me: did you try turning it on and off?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: haha
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive changed it between view and off in the dropbox since the beginning, should be the same now as in your case and I doesnt seem to be working
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and tried a few things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: nothing seems to be working
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i uploaded a new one though when you did
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but pointless to try it now

&lt;br /&gt;Me: don't give up

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive uploaded a new one
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: doesnt run properly
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: etc.
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: still nothing
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: maybe in a few days when i have a few gigs under my belt it will run a bit better
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and then in the mean time i can send you what i have
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to use as you like
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: until then use whatever is in my dropbox
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: if you want i can even send you my complete open source build, so you can build it and send me what i use

&lt;br /&gt;Me: ok, let's do that

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive only sent you a part of the source so i can get the full thing
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://github.com/p5pit/pciusb

&lt;br /&gt;Me: I can't access it. Can you resend the link?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just changed the link in the source

 &lt;br /&gt;Me: what's the new url?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just changed the name aboven you send it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://github.com/p5pit/pciusb/tree/master/libs
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but you need to update the links in the 'message' part
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: anyway whats up with you

&lt;br /&gt;Me: Not much, just reading

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been at lectures 4-7
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: im helping a mate with some maths problems
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and trying to read a paper since 5

&lt;br /&gt;Me: ok cool

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive not sure what it was on
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: Lyndon had problems with reading just finding the right words in random papers to use them to make a point
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and it was on whether certain types of choice can build a successful models to fit the data to believe
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 2 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: hey how are you

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive quit and you know where to find me

 &lt;br /&gt;Me: what did you quit

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my job, ive given a few interviews here and there
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and my boss is giving me shit for not doing more work during the day
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but ive quit
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: he's so annoyed at me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: thats a bit of an overreaction

&lt;br /&gt;Me: yeah you shouldnt have to work

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working since 8 am today

 &lt;br /&gt;Me: oh, you quit your job and are working on getting a new one?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying to for a while now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and the previous one ends in oct

&lt;br /&gt;Me: what job are you looking for?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been searching for 'concierge' level stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i like working on cutting edge stuff

&lt;br /&gt;Me: so something in AI again?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been looking at it for 8 months at least
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they want you to start at 17 / 20 euro a day
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: with no holiday
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: which is pretty ridiculous

&lt;br /&gt;Me: that does seem like a ridiculously low salary

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive talked to people at home and abroad about it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they were all 'wtf'
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i havent even called in to my third favourite coffee place to try to get them to put me as a regular

&lt;br /&gt;Me: interesting

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive gone to application after meeting with them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: coffee and biscuits*

&lt;br /&gt;Me: how did it go

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive only went for a small place so far, startup
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and they wanted me to make a profile there
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and to tell them about the personal stuff like my salary and social security
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and if i want it there it has to be on the phone
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and so on


&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 3 &lt;/i&gt;
&lt;/summary&gt;

&lt;br /&gt;Me: whats up

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been having a lot of success with my programming but dunno if Ive succeed
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have a lot of stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: not sure how many hours Im going to spend a day on it

&lt;br /&gt;Me: you are not working too much and getting burnt out are you?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive had years with zero effort and amazing results
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: - especially when compared to other programmers
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so yeah, probably a bit

&lt;br /&gt;Me: well, take it easy man

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been doing it for a few hours a day for a few years now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i really want to move to machine learning though
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and Ive started on that over a year ago

&lt;br /&gt;Me: what ML stuff do you want to do

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a little in R, but want to move to using things like neural nets for a lot more
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ideally something which is more general
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and less focused on just driving

&lt;br /&gt;Me: but what do you want to work on specifically

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive helped people a lot with getting them to focus on a specific problem
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: at this point
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D

&lt;br /&gt;Me: so what problem are YOU focusing on

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive helped a lot with getting them to focus on adding simple analytics
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to their sites
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and getting them to focus on just a few things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: rather than whole things
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but im moving more towards the latter

&lt;br /&gt;Me: hm

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a lot of the heavy lifting for them, by getting them to focus on a task that is either too hard or overly boring for them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but thats a bit of an unfair comparison
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: they have their own stuff

&lt;br /&gt;Me: and what do you have

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done a lot, but they have their stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have to move away from that
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: in a few months
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and program something better

&lt;br /&gt;Me: like what

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: something general
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: agi

&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 4 &lt;/i&gt;
&lt;/summary&gt;

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive done fuck all for an hour now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: nothing
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i shouldnt be working today
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i was supposed to be working yesterday too but cba
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: just tired

&lt;br /&gt;Me: you seem to be tired all the time, man. everything ok?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been sleeping since 9
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no, not really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my back hurts
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and i hurt my back peacing out
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but other than that im ok
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i can do little but write
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but i cant read and i havent done anything in days
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i am so bored
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: my head is so bad
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :/

&lt;br /&gt;Me: it happens - maybe take a break until you feel better

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive never been so annoyed by my head
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i can barely use the computer

&lt;br /&gt;Me: damn that sucks

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been worse
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but i dont think its as bad as my back
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: which cant even move

&lt;br /&gt;Me: do you need any help?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been asking people to do shit for me
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: for a long time now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no help
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: people give me shit for not doing more
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: havent had a break until now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i feel so bad for not being able to pass out

&lt;br /&gt;Me: what have you tried for your sleeping problems so far?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying for a long time an antihistamines
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:beta-blockers
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:xanax
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:histamine
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hainamines
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordamances
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordana
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:hordano
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:shrooms
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;:whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: not even going to read now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: too tired
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: anything else
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: slowing myself down
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: exhausted
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: read the whole thing now

&lt;br /&gt;Me: I did but I am confused. And you shouldn't take shrooms to sleep.

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i didnt really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a whole thing
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 5 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: want to listen to something cool. send me a track

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been putting tracks from acts I like on and off
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: into my library

&lt;br /&gt;Me: got any links to share?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been pushing some friends to listen to more stuff
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so you can do that too now :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=hfTPiS7Nmw
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=QsXgEzx_GGk
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and this other one is one of the earlier songs I liked from them
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.youtube.com/watch?v=Iqz02saPcoM

&lt;br /&gt;Me: cool! I like the cramps. Got any techno tho?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been trying to get people to listen to some of the stuff

 &lt;br /&gt;Me: which stuff? what artists or labels?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been pushing mostly labelless stuff which are usually quite good
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: stuff by bez batteries and their cousin or whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and stuff by dvs1 and his cousin
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: or vatican shadow or whatever
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive sent you the first two

&lt;br /&gt;Me: cool

&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 6 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: hey man, what are you up to this weekend?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive tickets for a party on friday
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: wasnt expecting to go out this early
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but got tickets during the sale
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: so im waking up early i guess

&lt;br /&gt;Me: cool, what's the party?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive told people and they havent been able to guess
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.facebook.com/events/553455714529578/
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a bit weird
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i have friends in charge
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: who i trust more than me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: to tell me if i have a ticket

&lt;br /&gt;Me: looks good. who's playing?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive asked them
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: they said they dont know
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i hope i do
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but they were saying it in a way that it sounds bad
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: aufnahme'samenere
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: its a bit weird
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: the only guy that ive met like him is berghainarian 
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: no wait that was mike
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: still ja
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and its this group that does weird stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: that guy does the sound, and looks kind of weird

&lt;br /&gt;Me: sounds pretty interesting really

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive read a little about him on wiki but thats it
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: he is apparently a maker
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but doesnt really know how the word works

&lt;br /&gt;Me: do I know him?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive met him and his brother at makerfaire
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: very briefly
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but they were really high
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and i dont really remember them
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: thats 2 months ago
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: :D

&lt;br /&gt;Me: fair enough
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;details&gt;
	&lt;summary&gt;
	&lt;i&gt;Example 7 &lt;/i&gt;
&lt;/summary&gt;
&lt;br /&gt;Me: how would you describe yourself

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive been working in the same job for 3 and a half years now
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: relatively new to berlin
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 25
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: 6 years as a programmer/machine learning person
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: pretty much all my time for the last 5 years has been spent on just learning machine learning and not programming really
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and Im pretty sure that even if I was to move elsewhere and work with people from different fields, I would still not be able to keep up
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: I would spend most of my time on just learning the ropes and whatever else they often have free or cheap classes for everyone
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: that seems to be the general approach in many places
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: but I havent seen that approach work for me
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: i spend literally every day with people and projects that dont allow me to do more than that

&lt;br /&gt;Me: and what about your personal life and hobbies?

 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: ive just been doing the same stuff ever since I moved to berlin a year ago
 &lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: didnt really do much else than that and the online courses to where I am now
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: I used to spend my time more on random stuff
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: like programming, music, reading things like that

&lt;br /&gt;Me: what about going out

&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: and its rained a lot the last few days which doesnt help either
&lt;br /&gt;&lt;b&gt;GPT Me&lt;/b&gt;: https://www.facebook.com/photo.php?fbid=324979844210832&amp;amp;set=a.324977827544367.75530.14546_1858271655695&amp;amp;type=3&amp;amp;theater

&lt;br /&gt;Me: sounds fun
&lt;/details&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conversations aren’t ideal but it’s &lt;strong&gt;a lot&lt;/strong&gt; fun playing with it. Sadly the stuff I talk about on facebook is either too personal or generic ( e.g. arranging to meet people) so I am unsure how well I’ve managed to convey the fun of talking to it. Perhaps &lt;a href=&quot;https://twitter.com/theshawwn/status/1206721762987315207&quot;&gt;another sample&lt;/a&gt; of gpt2 talking as me, but with the irc bot mentioned earlier (and ran by nshepperd), might convey it. You can find some more of those samples on Shawn’s twitter + a lot more gpt fun from his collaborations with &lt;a href=&quot;http://gwern.net&quot;&gt;gwern&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;I genuinely think that even with gpt2, let alone bigger and better models you can get a lot of use out of having a model of yourself you can talk to - and not just yourself. Playing with this has really made me realise the value of a service that can create a model of dead or dying relatives, and I want to eventually set up something for my loved ones - at minimum the data collection part.&lt;/p&gt;

&lt;p&gt;It will be unfair to say that it is always great though - sometimes it talks about whatever it wants to and the conversation can be harder to steer, otherwise it follows my lead easily. From what I can see in my actual messages I do this a lot, so maybe it is just copying me. Additionally it sometimes talks about things that aren’t really True - like the back pain in Example 1, and if you play with the different parameters (top_k/top_p and temperature mainly) you can force it to go on long tirades which eventually become nonsensical. It’s definitely not perfect.. but it sure is fascinating to me.&lt;/p&gt;

&lt;p&gt;Fun fact - there is a sentence in this post written entirely by the GPT version of Me. I wonder how easy it is to spot.&lt;/p&gt;</content><author><name></name></author><category term="gpt" /><category term="gpt2" /><category term="finetune" /><category term="colab" /><category term="neural-network" /><category term="chat" /><category term="reflection" /><category term="therapeutic" /><category term="rubber-duck" /><category term="nlp" /><category term="transformers" /><category term="machine-learning" /><summary type="html">Previously in this series - finetuning 117M, finetuning 345M</summary></entry><entry><title type="html">How is search so goddamn bad? A ‘Case Study’</title><link href="http://localhost:4000/blog/bad-search/" rel="alternate" type="text/html" title="How is search so goddamn bad? A ‘Case Study’" /><published>2020-01-19T00:00:00+01:00</published><updated>2020-01-19T00:00:00+01:00</updated><id>http://localhost:4000/blog/bad-search</id><content type="html" xml:base="http://localhost:4000/blog/bad-search/">&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Google Search is ridiculously, utterly bad. Before your brain starts recalling successful searches you’ve had, and before this turns into &lt;em&gt;only&lt;/em&gt; a rant let’s see some examples.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/gsearch1.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;This is a query for checking out what reddit thinks in regards to buying a phone, while specifying to only get results from the last month. Pretty simple I guess, yet &lt;strong&gt;none&lt;/strong&gt; of the results achieved that goal, and &lt;strong&gt;none&lt;/strong&gt; of them are actually from the last month, despite Google lying that they are. Let’s quickly go through them one by one.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;‘Best place to by a cheap new smartphone?’ from r/Berlin - this is about where to buy a phone, but I live in Berlin, so maybe it makes some sense as a top result… except the post is actually from &lt;strong&gt;6! years ago&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;Specific to Audiophile phones for some reasons, and a pretty average thread (only 31 upvotes for one metric) from &lt;strong&gt;9 months ago&lt;/strong&gt;. Not useful, and doesnt meet the query but still a lot better.&lt;/li&gt;
  &lt;li&gt;From 2018 (&amp;gt;1 year ago), even less popular and less useful post (17 upvotes) on the Pixel subreddit.&lt;/li&gt;
  &lt;li&gt;Phone for privacy, small thread from &lt;strong&gt;&amp;gt; 4 years ago&lt;/strong&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;6 years ago&lt;/strong&gt;, wtf.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;I won’t keep going with the rest, but it doesn’t get better. Why are the results reported as recent when they are from years ago, I don’t know - those are archived post so no changes have been made. At any rate, I got annoyed at this point (mentioning for those who couldn’t tell), so I switched to DuckDuckGo.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/ddgsearch1.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Somehow the results are even less useful. 2 irrelevant subreddits at the top, a result from 2012, that they admit is from 2012. At this point I visibly checked on reddit if there’ve been posts about buying a phone from the last month and there are. They also show up on Google/DDG if you look for them directly so they are indexed.&lt;/p&gt;

&lt;p&gt;Hm, maybe adding &lt;code class=&quot;highlighter-rouge&quot;&gt;site:reddit.com&lt;/code&gt; will help DDG to get there?&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div style=&quot;text-align: center;&quot;&gt;&lt;img class=&quot;img-fluid&quot; src=&quot;/static/ddgsearch2.png&quot; /&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;Nope. As you can see, I didn’t even bother clicking on all of them now, but I can tell you the first result is deeply irrelevant and the second one leads to a 4 year old thread.&lt;/p&gt;

&lt;p&gt;Maybe it’s my wording then - what about ‘recommend phone reddit’ or ‘which phone to purchase reddit’? I won’t even waste anybody’s time by posting the results I got.&lt;/p&gt;

&lt;h3 id=&quot;thoughts&quot;&gt;&lt;center&gt;Thoughts&lt;/center&gt;&lt;/h3&gt;

&lt;p&gt;Part of the issue in this specific case is that they’ve started ignoring settings for displaying results from specific time periods. It’s definitely not the whole issue though, and not something new or specific to phone searches.&lt;/p&gt;

&lt;p&gt;Now, I’ve always been biased towards the new - books, tech, everything, but I can’t help but feel that a lot of things which were done pretty well before are done worse today.  We do have better technology, yet we somehow build inferior solutions with it all too often. Further, if they had the same bias of showing me only recent results I’ll understand it better, but that’s not even the case.
And yes, I get that the incentives of users and providers don’t align perfectly, that Google isn’t your friend, etc. But what is DDG’s excuse?
As for the Case Study part, and me saying this isn’t simply a rant - I lied, hence the quotation marks in the title. Don’t trust everything you read, especially the goddamn dates on your search results.&lt;/p&gt;</content><author><name></name></author><category term="google" /><category term="duckduckgo" /><category term="ddg" /><category term="search" /><category term="bad" /><category term="case" /><category term="study" /><category term="problem" /><summary type="html"></summary></entry><entry><title type="html">Fine-tunning OpenAI’s (Larger) GPT-2-345M on Conversation data (Update)</title><link href="http://localhost:4000/blog/gpt-345M-finetune/" rel="alternate" type="text/html" title="Fine-tunning OpenAI's (Larger) GPT-2-345M on Conversation data (Update)" /><published>2019-05-08T00:00:00+02:00</published><updated>2019-05-08T00:00:00+02:00</updated><id>http://localhost:4000/blog/gpt-345m-finetune</id><content type="html" xml:base="http://localhost:4000/blog/gpt-345M-finetune/">&lt;p&gt;NLP, Machine Conversations and the road to passing the Turing Test have always interested me. That’s why when OpenAI released a larger (345M parameters vs the previous ) version of &lt;a href=&quot;https://openai.com/blog/better-language-models/#update&quot;&gt;GPT-2, their current state of the art language model&lt;/a&gt; I jumped to test it out on my small but personal dataset of 14mb of my own facebook conversations along with testing it a bit on a &lt;a href=&quot;https://www.kaggle.com/rtatman/ubuntu-dialogue-corpus&quot;&gt;Two person Ubuntu-related dialogue corpus&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The newly released pre-trained version has 345 million parameters, compared to the 117M parameter version so one would expect significantly better results. Note, this is still significantly smaller than the 1.5B version they’ve shown off. OpenAI are, however, releasing the aforementioned 1.5B version along with a 762M version to partner organizations along with plans to release those to the public in the future, too.&lt;/p&gt;

&lt;p&gt;Although it will be interesting to play around with those much bigger GPT-2 versions, they might not even be relevant by the time they are released - training on the same data with the same amount of compute one can already most likely build an even better model using recent advances e.g. even just with better transformers like OpenAI’s &lt;a href=&quot;https://openai.com/blog/sparse-transformer/&quot;&gt;Sparse Transformer&lt;/a&gt; or Google AI’s &lt;a href=&quot;https://ai.googleblog.com/2019/01/transformer-xl-unleashing-potential-of.html&quot;&gt;Transofmer-XL&lt;/a&gt; both of which improve on GPT-2’s current architecture.&lt;/p&gt;

&lt;p&gt;I won’t go into detail describing the Colab with Facebook data, as it is almost the same as the one used with the 117M version which I describe &lt;a href=&quot;/blog/gpt-finetune/&quot;&gt;here&lt;/a&gt;. We are again using &lt;a href=&quot;https://github.com/nshepperd/gpt-2&quot;&gt;nsheppered’s GPT training code&lt;/a&gt;, this time with gradient checkpointing to be able to fit the larger model in memory.&lt;/p&gt;

&lt;p&gt;You can follow in the new GPT-2-345M collab FB data &lt;a href=&quot;https://colab.research.google.com/drive/1EhZG2_AQLeDvW2s841d502FydMn1vgmK&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; GPU (or TPU)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#facebook-examples&quot;&gt;Generated Facebook Messenger Dialogue Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;There is also the GPT-2-345M ubuntu data (including preprossesing)  &lt;a href=&quot;https://colab.research.google.com/drive/161JjdAyqckSBLD45N9WZzFI21rUYk2ns&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; GPU (or TPU)&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;&lt;a href=&quot;#ubuntu-examples&quot;&gt;Generated Ubuntu Dialogue Samples&lt;/a&gt;&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&quot;results&quot;&gt;Results&lt;/h2&gt;

&lt;p&gt;First of, The results are visibly better. Even without fine-tuning 345M produces visibly (but not overwhelmingly so) better results than 117M, and preliminary outputs from &lt;a href=&quot;https://www.gwern.net/GPT-2&quot;&gt;fine-tuning it on poetry&lt;/a&gt; are reportedly even more promising. It is hard to judge how much better but for example, I couldn’t get the loss on my small 14M facebook messages dataset to go consistently bellow &lt;em&gt;2.10&lt;/em&gt; with 117M, while with 345M (and admittedly a bit more training) I got it down to &lt;em&gt;1.85&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;facebook&quot;&gt;Facebook&lt;/h3&gt;

&lt;p&gt;The results are a bit better and more coherent but 14M is clearly too small a dataset. I had to do some small tricks (mainly decreasing the learning rate periodically and occasionally attempting to use the ubuntu data, too). Adding the &lt;em&gt;endoftext&lt;/em&gt; token instead of two blank lines between conversations also helped as expected.&lt;/p&gt;

&lt;h4 id=&quot;facebook-examples&quot;&gt;&lt;em&gt;Facebook Examples:&lt;/em&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353629564276) Tom Middlebrook: alright, i was given a bag as a present. so i had to return it to my parents. was fine getting it again though but only with a small amount&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353595877463) Svilen Todorov: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655925201) Tom Middlebrook: https://www.youtube.com/watch?v=tYJhLgK1KqU&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655946673) Svilen Todorov: i should’ve probably taken a photo when i did that haha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353655984955) Svilen Todorov: i saw someone else on youtube trying to buy like 20 quid extra lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353656240766) Svilen Todorov: i think ill have to do a job at some point after i graduate but yh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353654726000) Svilen Todorov: http://www.reddit.com/tb/y2r0r&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1353654728111) Svilen Todorov: i dont get why not to give him the money :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354159959666) Tom Middlebrook: lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354165011230) Tom Middlebrook: that’s awesome&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354155045207) Svilen Todorov: hahaha, yh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255181784) Tom Middlebrook: http://i.imgur.com/Ypk.png&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255222767) Tom Middlebrook: I’m pretty sure the other two guys that I had work with from the day before had worked with jsa lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354255453761) Svilen Todorov: hahah&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357524074) Svilen Todorov: oh ya&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357528963) Tom Middlebrook: http://i.imgur.com/m4h5.jpg&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357535641) Tom Middlebrook: looks really cool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357540333) Svilen Todorov: hahahaha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357562322) Svilen Todorov: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1354357563001) Tom Middlebrook: looks like a cool outfit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878572976) Ioannis Agathocleous: Ill have to see a place in the east of london&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878585860) Svilen Todorov: maybe&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878623078) Svilen Todorov: not too far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878673631) Svilen Todorov: well not that far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878673234) Svilen Todorov: might be too much as I am not going to get much for a place that already has one&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878680863) Svilen Todorov: but dunno&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878688387) Svilen Todorov: i’ll go to a place in london instead since I don’t have a job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878698013) Ioannis Agathocleous: I could find one somewhere on the east but the distance is pretty small&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878706547) Ioannis Agathocleous: But I dont mind&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878783822) Svilen Todorov: it’s not too far&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878792946) Svilen Todorov: well, Im pretty sure it’d be easier to just find a place in london instead if I get too many people to live with&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496878807895) Svilen Todorov: if there isnt one in the UK I might be able to get a place here or something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888089082) Svilen Todorov: yeah, i can find somewhere&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888051110) Ioannis Agathocleous: Ye probs best to be a foreigner&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888061658) Ioannis Agathocleous: Just find a new place and work&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888093631) Svilen Todorov: yh true&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888096719) Svilen Todorov: yh, I’ve found a place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888096620) Ioannis Agathocleous: You gotta find something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887102292) Ioannis Agathocleous: Or stay at a mate’s house for a few months&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887143560) Svilen Todorov: will do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887162939) Svilen Todorov: I think you’ll find something in Bulgaria&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887172227) Svilen Todorov: in london might be easier to find something&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496887193938) Svilen Todorov: so yh find what you want&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496888245750) Ioannis Agathocleous: Ye will do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895273936) Svilen Todorov: just saw a flat for 200 which is 2 hours from my place&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895284880) Svilen Todorov: I can also get a place cheaper if I move there too&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895358255) Ioannis Agathocleous: Seems like a decent deal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496895641622) Ioannis Agathocleous: Ill see if I can find something from the guys&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893649976) Svilen Todorov: looks like it’s in the center of meelerdham&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893664983) Ioannis Agathocleous: Kl ill check it out&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893669386) Svilen Todorov: if I move tomorrow Ill be living in London until March for a year&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1496893688213) Ioannis Agathocleous: Ye but I dont mind moving anyway&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13136700170) Svilen Todorov: i’m going to sleep&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670106021) Michal Rostecki: ah sorry, I forgot&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670197807) Michal Rostecki: yeah, I will prob go there around 19:30&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199539) Svilen Todorov: ok, about to try to make it to the bar at least&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199507) Svilen Todorov: if its like that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199579) Svilen Todorov: its way down&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199997) Michal Rostecki: yeah, now I’m on my way&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199994) Svilen Todorov: oh and I keep forgetting my passport when I get there&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313670199923) Svilen Todorov: well i have it already :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313680514053) Svilen Todorov: http://www.buzzfeed.com/buzzfeed/comments/2lh/how-do-you-use-the-device-menu-system-in-ubuntu-10-04-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-lucid-with-ubuntu-10-04-lucid-like-after-reinstalling-the-grub-loader-for-ubuntu-10-04/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671553576) Michal Rostecki: what’s your plan for today? did you find it?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671607100) Svilen Todorov: nah got some stuff to do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671627252) Svilen Todorov: im heading to work now&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671638781) Michal Rostecki: cool! I’ll probably be there around 4ish&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671666644) Svilen Todorov: didnt get that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671669988) Svilen Todorov: nah it’s fine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1313671670159) Michal Rostecki: ok&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396720429) Carmen Quasi: are you going to a bar?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396782520) Svilen Todorov: yeah sure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544396988765) Svilen Todorov: but Ill probably stay at mine longer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544397997134) Carmen Quasi: hihi i can come to yours at some point&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544397996679) Carmen Quasi: I’m in a park with my friend Jonas. you could just head there and drink a beer&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402012633) Svilen Todorov: Ja&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402269095) Svilen Todorov: If I will come by the same time Ill pass by to get my backpack&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402529078) Carmen Quasi: it’s not so big a park&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544402839650) Carmen Quasi: i dont get where you are&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407472321) Svilen Todorov: Ja same&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407506681) Svilen Todorov: It’s in the center&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544407505377) Svilen Todorov: A lot bigger than London for sure&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544503854605) Carmen Quasi: yeah it’s in the center with a garden&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544505993833) Carmen Quasi: there is a pool pool in the center, not the backyard&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507562468) Svilen Todorov: Ye fair&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507569097) Svilen Todorov: Might come for a bit to try it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1544507617387) Carmen Quasi: nice&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h3 id=&quot;ubuntu&quot;&gt;Ubuntu&lt;/h3&gt;

&lt;p&gt;I mainly tried this out of curiosity but also in order to attempt to use a somewhat similar (dialogue) dataset for somehow improving the accuracy on my all-to-small facebook dataset. I tried multiple things but the only thing that actually helped was to train for a bit (&amp;lt;1500 steps) on the ubuntu dataset a few times when the loss on my own dataset won’t go down and then go back. This sped things up slightly and allowed me to reach a little bit higher accuracy in the end but the difference isn’t too big.&lt;/p&gt;

&lt;p&gt;I only tried this for a couple of hours and used slightly less than half the data (there are 3 folders in the datasets and I only used the 951M one) but the results were quite close to what’s in the dataset. Here are some random (non-cherry picked) samples. Honestly, if you give me some actual conversations from this dataset and some generated conversations, I’m not sure if I’d do much better than 60% at recognizing which is which.&lt;/p&gt;

&lt;h4 id=&quot;ubuntu-examples&quot;&gt;&lt;em&gt;Ubuntu Examples:&lt;/em&gt;&lt;/h4&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738600) chrism_: hi all&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738760) chrism_: can somebody help me set up a nvidia x driver? my system doesn’t display an X server. I think it’s not a graphical problem.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738820) jocke: I would use xorg1-driver&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738900) jocke: then do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738960) jocke: I usually install both but at least you can do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191738960) jocke: I would just follow this guide to help you&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739020) jocke: https://wiki.ubuntu.com/X11/UsingX11/Manual&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739080) jocke: if your setup doesn’t load the driver it might be a bad idea to disable it. But when you run xmodmap &lt;keycode&gt; and it says no such buffer I assume that you are good to set it back to working state you should be fine&lt;/keycode&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739080) jocke: the first step (the xorg1-driver) is to run xmodmap from that directory. Once that is done it will be available as a device for ‘device’ on your desktop. If your only problems are with xorg1.h or X11.h, just add it to /etc/X11/xorg.conf&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1191739400) jocke: if you need the xorg.conf just edit my example file to change it to the one you want to have.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144700) luigi__: hey i have this problem, how do i stop and remove all the packages ubuntu has, it keeps re-installing everything when i change the source and i change the repository to another one&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) luigi__: thanks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: right-click the source folder and look for a dropdown area&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then right-click&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then type   dpkg -L      and hit enter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144760) cnj_: then click add&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) cnj_: it will start downloading everything, just in case&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) cnj_: it will tell you to restart the system to do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144820) luigi__: what do you mean?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: in here?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) luigi__: i don’t have that on my laptop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: I’ll give you a link.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: http://paste.ubuntu-nl.org/134440/&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: this is my machine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_:  my&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: my problem&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) luigi__: the thing is its working perfectly fine on my laptop&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144880) cnj_: the laptop has no problems with downloading any package that is offered through my terminal&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: I have no connection to you.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: in ubuntu/lubuntu/solaris&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) luigi__: so if i just installed those ubuntu packages, they still need reinstallation?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1299144940) cnj_: yes&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447620) m0nix_: hmm&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447680) m0nix_: I had a good time with Wine2X, but i changed my mind, I will not use Wine2 for this application.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) m0nix_: Is there a way to make the application run on Ubuntu?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: sudo dpkg –purge wine2x ?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: I got wine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447740) MrFool: yes.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447800) MrFool: I guess you don’t really need the whole x server, but that you can just install.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447860) MrFool: Wine2 is just an extension from Wine3&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189447920) MrFool: Yes you want to run X?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1189442140) MrFool: You have a server&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;
&lt;br /&gt;&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) kalzior: anyone know where i can get fiesty to work with this new version?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) nishant: you can use gtk-fiesty&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) kalzior: thanks&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1199918140) nishant: I have never really used freestyle, but the gui interface does a good job&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455720) wuqh2u2n: do you know how to do ubuntu and windows together??&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455780) theadmin: you need to use the ubuntu iso to install.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455840) wuqh2u2n: but its just 2 partitions&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321455900) theadmin: ok so when installing ubun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321446060) theadmin: which one is the right?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321456600) theadmin: which ISO&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321456600) wuqh2u2n: i’ve tried downloading the iso but they’re all wrong, but theres a way to use it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462060) theadmin: try and use the iso you downloaded, it will be an iso you can burn to&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: thats it, you will be able to burn it to cd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: its the right iso&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: how do i burn them? i need some help&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462300) theadmin: i need to burn the iso from the internet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1321462540) theadmin: you need to have a cd&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conversations look better but not ideal. The model is learning even more relevant stuff - e.g. they are about mutual interests, acquaintances etc. It truly seems to have some idea of who each person is and can mostly catch the tone and relevant facts about most people I’ve talked enough to.&lt;/p&gt;

&lt;p&gt;It was also easy and fun to play with and makes me wonder how much better we can do with the biggest model or better techniques or ideally more of everything. At this rate, we might soon have chatbots good enough for people to not even compare them to Eliza!&lt;/p&gt;</content><author><name></name></author><category term="gpt" /><category term="gpt2" /><category term="finetune" /><category term="colab" /><category term="nlp" /><category term="machine-learning" /><summary type="html">NLP, Machine Conversations and the road to passing the Turing Test have always interested me. That’s why when OpenAI released a larger (345M parameters vs the previous ) version of GPT-2, their current state of the art language model I jumped to test it out on my small but personal dataset of 14mb of my own facebook conversations along with testing it a bit on a Two person Ubuntu-related dialogue corpus.</summary></entry><entry><title type="html">A Proposed Task for Training a Human-Imitating System as a Benchmark and Potential Part of The Road to AGI</title><link href="http://localhost:4000/blog/human-imitating-task/" rel="alternate" type="text/html" title="A Proposed Task for Training a Human-Imitating System as a Benchmark and Potential Part of The Road to AGI" /><published>2019-04-29T00:00:00+02:00</published><updated>2019-04-29T00:00:00+02:00</updated><id>http://localhost:4000/blog/human-imitating-task</id><content type="html" xml:base="http://localhost:4000/blog/human-imitating-task/">&lt;h2 id=&quot;basic-idea&quot;&gt;Basic idea&lt;/h2&gt;

&lt;p&gt;One of the most important pieces for training a more general ‘human-like’ System (Neural Network) is having a good open-ended task which is either unsupervised or where generating very large amounts of data is doable.&lt;/p&gt;

&lt;p&gt;I propose training a model where the input is simply people interacting with their computer (just the browser for simplicity) as they normally do and the output is a prediction of the next action they will take.&lt;/p&gt;

&lt;p&gt;&lt;sub&gt;
&lt;sup&gt;
For those who don’t think AGI is possible - I am not saying it definitely is (especially not immediately), just proposing a task to get closer to something that is realistically doable.
&lt;/sup&gt;
&lt;/sub&gt;&lt;/p&gt;

&lt;h2 id=&quot;rationale&quot;&gt;Rationale&lt;/h2&gt;

&lt;p&gt;At this point in the current Deep Learning revolution it seems like variations of &lt;a href=&quot;https://github.com/NVlabs/stylegan&quot;&gt;our&lt;/a&gt; &lt;a href=&quot;https://openai.com/five/&quot;&gt;current&lt;/a&gt; &lt;a href=&quot;https://github.com/openai/gpt-2&quot;&gt;techniques&lt;/a&gt; can likely solve most any &lt;em&gt;well-defined&lt;/em&gt; problem if you use the right parts, have enough data and make the network big enough.
Alternatively, if this isn’t the case seeing how far we can go and having a harder human-imitating task can be used as a good benchmark.&lt;/p&gt;

&lt;p&gt;Thus, I am operating under the assumption that having the right problem with a strong enough signal is as important, or plausibly a more important problem than finding a new technique for developing an algorithm that can at minimum behave in a more general, human-like fashion. For example, I no longer believe that there is a game (video or otherwise) that isn’t solvable at human-level with current techniques as long as you have enough resources (time, people, and computation).&lt;/p&gt;

&lt;h2 id=&quot;task&quot;&gt;Task&lt;/h2&gt;

&lt;p&gt;Record an enormous training set of people using their browsers. Include mouse clicks, key presses, screenshots of the screen, text on screen, OCR (text from images), html elements, urls, etc.&lt;/p&gt;

&lt;p&gt;Do feature extraction with ideally also pre-trained and initially frozen specialized nets for creating features from the text, timeframe-by-timeframe screenshots etc. and feed it all into a general model which predicts the next keypress/mouse event sent by the user.&lt;/p&gt;

&lt;p&gt;There is enough of a signal in the data to theoretically be able to [more] accurately predict the next action a person will take if you as a human observed them enough and have full information of what they were just doing, and what they are seeing. Moreso if an algorithm does it.&lt;/p&gt;

&lt;p&gt;The idea is the same as with text-prediction, however (for a simple example) even an exceptional human studying literature all their life, can only predict the next word in a book with limited accuracy. On the other hand, at least on paper, if one person studies all interactions someone has or at least all of those with their browser (and many other people’s) they can likely very accurately predict the next thing a human will do based on what they see and what they’ve been doing.&lt;/p&gt;

&lt;p&gt;In the case of pure text prediction it is one thing to only feed a neural network text, not related to anything, and another for the text (and everything) else to relate to real actions and other input. Simplified example - seeing the word red more often after interacting with an image that has more red in it. There is just more to connect to make an accurate model with a more full internal representations of it’s environment - and said environment is incredibly vast.&lt;/p&gt;

&lt;p&gt;The thing the model would actually be modelling at the end of the day is [more or less] open-ended human behaviour.&lt;/p&gt;

&lt;p&gt;Of course, the exciting part comes not as much from predicting the next action of real humans but from letting the model run freely - predict actions and perform them without there being a human any more. Would it visit sites successfully? Will it attempt to leave comments on articles or browse photos? Try to register or login anywhere?&lt;/p&gt;

&lt;p&gt;I believe this is technically doable with enough resources, even today.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Now, I am not deluded enough to assume that this will in any way straight lead to AGI but I expect it to supply us with to a lot of interesting insights, and the question is &lt;em&gt;how much would a model trained like that learn&lt;/em&gt;? . Before, I might have thought the task would simply be too hard to start learning but after recent success in solving games, text prediction etc., as well as the fact that it can start predicting a lot of actions quite easily, and that a lot of the modules (e.g. text extraction) would be pre-trained (but again, not frozen), I suspect it will learn more than most people assume.
Also how much better (or if not better, how come) would for example the text-relevant portion of the model get after training as part of the full system?&lt;/p&gt;

&lt;p&gt;Additionally, I am well aware that there are many obvious and non-obvious problems that would need to be solved in the process. Creating the data and sanitizing user’s credentials, other privacy considerations. Enormous training time and designing the right Network(s) etc. Is any of it infeasible for a sufficiently large organization, however? I think no - this can very well be attempted today.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;
&lt;sub&gt;
&lt;sup&gt;
I would be very interested to hear about arguments why this would or wouldn’t work, potential improvements or alternatives (e.g. having a RL and/or GAN-like approach with a similar task which is also doable), or if there is anyone working on something similar to this. Feel free to contact me about this or any related or unrelated offers for work or collaboration.
&lt;/sup&gt;
&lt;/sub&gt;&lt;/p&gt;</content><author><name></name></author><category term="agi" /><category term="ai" /><category term="task" /><category term="proposal" /><category term="imitate" /><category term="machine-learning" /><summary type="html">Basic idea</summary></entry><entry><title type="html">Adding Layers to the middle of a pre-trained network whithout invalidating the weights</title><link href="http://localhost:4000/blog/add-layers/" rel="alternate" type="text/html" title="Adding Layers to the middle of a pre-trained network whithout invalidating the weights" /><published>2019-03-22T00:00:00+01:00</published><updated>2019-03-22T00:00:00+01:00</updated><id>http://localhost:4000/blog/add-layers</id><content type="html" xml:base="http://localhost:4000/blog/add-layers/">&lt;p&gt;Fine-tunning pre-trained neural networks on new data has shown a lot of promise in &lt;a href=&quot;https://cv-tricks.com/keras/fine-tuning-tensorflow/&quot;&gt;many&lt;/a&gt; &lt;a href=&quot;http://nlp.fast.ai/&quot;&gt;domains&lt;/a&gt;. One simple example is my &lt;a href=&quot;/blog/gpt-finetune&quot;&gt;last post&lt;/a&gt; where I fine-tune OpenAI’s GPT-small model on my chats from facebook messenger to create fake conversation.&lt;/p&gt;

&lt;p&gt;Using pre-trained models and further training them is especially useful for organizations with small datasets or resources, and in most cases, it is cost and otherwise effective to do it. However, despite it being widely used, people rarely talk about taking a pre-trained model and making it bigger by adding more layers in the middle of the network rather than just the end.&lt;/p&gt;

&lt;p&gt;Naively, this doesn’t work without some tweaks - if you add a layer in the middle of a network then all the trained weights of later layers become useless since they are getting different inputs. There are, however, ways to get around that and I believe that this is an important area to explore as more and more useful models get released.&lt;/p&gt;

&lt;p&gt;These posts (along with the last) are my first two in a series where I will attempt to increase the size of OpenAI’s GPT-2 model while taking advantage of the training the model has already gotten. Their model is a great candidate for this experiment, as OpenAI have already demonstrated great results with what is basically a bigger version of it. If anyone else is working on something similar or has links to related research I might have missed - feel free to email me, or hell - even cite me.&lt;/p&gt;

&lt;p&gt;Note: The basic network example is mostly taken from the &lt;a href=&quot;https://www.tensorflow.org/alpha/tutorials/quickstart/advanced&quot;&gt;Tensorflow 2.0 Getting started article&lt;/a&gt;. I used this as an opportunity to play a little with the new API.&lt;/p&gt;

&lt;p&gt;You can follow in the colab &lt;a href=&quot;https://colab.research.google.com/drive/1KocZA0Zgo68eMKWXg-W3W6Ev0F6n88cX&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;First, we install tensorflow 2.0 to Colab (this step can be skipped after 2.0 is out and the default in Colab)&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;pip install tensorflow-gpu==2.0.0-alpha0
&lt;/code&gt;
Then imports&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflow_datasets as tfds
import tensorflow as tf
import numpy as np

from tensorflow.keras.layers import Dense, Flatten, Conv2D
from tensorflow.keras import Model```

Then we download minst to have something to play with, shuffle and batch it

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We download, shuffle, and batch our training and test data&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;dataset, info = tfds.load('mnist', with_info=True, as_supervised=True)
mnist_train, mnist_test = dataset['train'], dataset['test']

def convert_types(image, label):
  image = tf.cast(image, tf.float32)
  image /= 255
  return image, label

mnist_train = mnist_train.map(convert_types).shuffle(10000).batch(32)
mnist_test = mnist_test.map(convert_types).batch(32)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Create our basic model&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;super&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;).&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;__init__&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Conv2D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;m&quot;&gt;3&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d2&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;

  &lt;span class=&quot;n&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;call&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;conv1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;flatten&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;x&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;self&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;d2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;MyModel&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Add our loss, optimizer, and metrics&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loss_object = tf.keras.losses.SparseCategoricalCrossentropy()

optimizer = tf.keras.optimizers.Adam()

train_loss = tf.keras.metrics.Mean(name='train_loss')
train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')

test_loss = tf.keras.metrics.Mean(name='test_loss')
test_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='test_accuracy')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We define our training_step and test_step functions.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def train_step(image, label, model=model):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  gradients = tape.gradient(loss, model.trainable_variables)
  optimizer.apply_gradients(zip(gradients, model.trainable_variables))
  
  train_loss(loss)
  train_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def test_step(image, label, model=model):
  predictions = model(image)
  t_loss = loss_object(label, predictions)
  
  test_loss(t_loss)
  test_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then train our default model a bit.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EPOCHS = 2

for epoch in range(EPOCHS):
  test_loss.reset_states()
  test_accuracy.reset_states()
  for image, label in mnist_train:
    train_step(image, label)
  
  for test_image, test_label in mnist_test:
    test_step(test_image, test_label)
  
  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(), 
                         train_accuracy.result()*100,
                         test_loss.result(), 
                         test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 1, Loss: 0.20242027938365936, Accuracy: 94.05333709716797, Test Loss: 0.08358743041753769, Test Accuracy: 97.37999725341797&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 2, Loss: 0.13752736151218414, Accuracy: 95.9375, Test Loss: 0.07251566648483276, Test Accuracy: 97.65999603271484&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point, we can try creating a new bigger model that uses all the weights trained here (including those in the very last layer!).&lt;/p&gt;

&lt;p&gt;What we are going to do is make a model almost exactly like the last one but we are going to add one more Dense layer before the final one. The important bit here is to initialize the layer so the weights are in the form of the identity function - this way when the output from the previous layer gets multiplied by the output of this layer we will get exactly the same result, and the weights of the final layer will still make sense. Then during fine-tuning, the layer will slowly move away from the identity function in whichever directions make the most sense. Note: we also want the bias to be initialized to zeros (so we don’t add anything extra to the weights at first) but this is already the default in tensorflow.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyModel2(Model):
  def __init__(self):
    super(MyModel2, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu')
    self.flatten = Flatten()
    self.d1 = Dense(20, activation='relu')
    **self.d_extra = Dense(20, activation='relu', kernel_initializer=tf.keras.initializers.Identity)**
    self.d2 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    x = self.d1(x)
    **x = self.d_extra(x)**
    return self.d2(x)
  
model2 = MyModel2()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then need to re-run our test_step and train_step functions (just re-run the cells containing them) due to how tf.function works. After that, we can confirm our new model isn’t magically performing better than chance.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;test_loss.reset_states()
test_accuracy.reset_states()
for test_image, test_label in mnist_test:
    test_step(test_image, test_label, model2)


template = 'Test Loss: {}, Test Accuracy: {}'

print (template.format(test_loss.result(), 
                     test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;blockquote&gt;
  &lt;p&gt;Test Loss: 2.3064279556274414, Test Accuracy: 10.329999923706055&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;As expected - it only gets a number right about 1/10th of the time.&lt;/p&gt;

&lt;p&gt;Now, the only other thing we need to do is add the weights from our previous model to our new model, except for our new (3rd) layer which will at first just leave things the same.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;model2.layers[0].set_weights(model.layers[0].get_weights())
model2.layers[1].set_weights(model.layers[1].get_weights())
model2.layers[2].set_weights(model.layers[2].get_weights())
model2.layers[4].set_weights(model.layers[3].get_weights())
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We can then re-run the code checking the accuracy of our new model and voila&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Test Loss: 0.07251566648483276, Test Accuracy: 97.65999603271484&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;At this point we can just start training our bigger model.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;EPOCHS = 2

for epoch in range(EPOCHS):
  test_loss.reset_states()
  test_accuracy.reset_states()
  for image, label in mnist_train:
    train_step(image, label, model2)
  
  for test_image, test_label in mnist_test:
    test_step(test_image, test_label, model2)
  
  template = 'Epoch {}, Loss: {}, Accuracy: {}, Test Loss: {}, Test Accuracy: {}'
  print (template.format(epoch+1,
                         train_loss.result(), 
                         train_accuracy.result()*100,
                         test_loss.result(), 
                         test_accuracy.result()*100))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Which gets me to&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 1, Loss: 0.11430724710226059, Accuracy: 96.58721923828125, Test Loss: 0.05486641451716423, Test Accuracy: 98.18999481201172&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;Epoch 2, Loss: 0.09389258921146393, Accuracy: 97.19083404541016, Test Loss: 0.05310175567865372, Test Accuracy: 98.31999969482422&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Not much better, but the results will usually be more impressive when dealing with more complex problems.&lt;/p&gt;

&lt;p&gt;Also, instead of doing that, we can also freeze everything but our new layer to more accurately only train it. This will train faster, and depending on the problem, it might make more sense - or it might make more sense for a few epochs before again training all layers. Play around with it!&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;@tf.function
def train_step(image, label, model=model):
  with tf.GradientTape() as tape:
    predictions = model(image)
    loss = loss_object(label, predictions)
  ***gradients = tape.gradient(loss, model.trainable_variables[-4:-2])
  optimizer.apply_gradients(zip(gradients, model.trainable_variables[-4:-2]))***
  
  train_loss(loss)
  train_accuracy(label, predictions)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The way we do that is by using the gradients of only our new layer.
model.trainable_variables works by returning a list of all weights and biases of our model layer by layer - so the first item is the weights of layer 1,  2nd item is the bias of layer 1 etc. Thus we only need the 2 layers before the last 2.&lt;/p&gt;

&lt;h2 id=&quot;fully-convolutional&quot;&gt;Fully convolutional&lt;/h2&gt;

&lt;p&gt;What if we are working with e.g. a fully convolutional network - an identity matrix won’t work (and tensorflow doesn’t even allow us to use the identity initializer for that reason) - how do you add a new layer while keeping the network usable?&lt;/p&gt;

&lt;p&gt;There are different ways you can extend the network while keeping all the weights useful - one I like is to use a residual-like approach. I simply initialize the new conv layer using all 1s and multiply the output from it to the output of the previous layer. This changes nothing at first but once we start training again, we can slowly move those weights in the direction we want them to be.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;class MyModel(Model):
  def __init__(self):
    super(MyModel, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu', padding='SAME')
    self.flatten = Flatten()
    self.d1 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = self.flatten(x)
    return self.d1(x)
    

class MyModel2(Model):
  def __init__(self):
    super(MyModel2, self).__init__()
    self.conv1 = Conv2D(32, 3, activation='relu', padding='SAME')
    self.c_extra = Conv2D(32, 3, activation='relu', padding='SAME', kernel_initializer=tf.keras.initializers.Ones)
    self.flatten = Flatten()
    self.d1 = Dense(10, activation='softmax')

  def call(self, x):
    x = self.conv1(x)
    x = x * self.c_extra(x)
    x = self.flatten(x)
    return self.d1(x)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The rest is exactly the same as before.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Increasing the size of a network is something usually done before training from scratch but that doesn’t always need to be the case. You can add one or better yet more layers across the network or even at the very start. Whether this will help depends - mainly on whether you needed a bigger net from the start - but it can definitely save you time.&lt;/p&gt;</content><author><name></name></author><category term="neural" /><category term="network" /><category term="ml" /><category term="layer" /><category term="machine-learning" /><category term="colab" /><summary type="html">Fine-tunning pre-trained neural networks on new data has shown a lot of promise in many domains. One simple example is my last post where I fine-tune OpenAI’s GPT-small model on my chats from facebook messenger to create fake conversation.</summary></entry><entry><title type="html">Generating Fake Conversations by fine-tunning OpenAI’s GPT-2 on data from Facebook Messenger</title><link href="http://localhost:4000/blog/gpt-finetune/" rel="alternate" type="text/html" title="Generating Fake Conversations by fine-tunning OpenAI's GPT-2 on data from Facebook Messenger" /><published>2019-03-12T00:00:00+01:00</published><updated>2019-03-12T00:00:00+01:00</updated><id>http://localhost:4000/blog/gpt-finetune</id><content type="html" xml:base="http://localhost:4000/blog/gpt-finetune/">&lt;p&gt;One of the most interesting problems in NLP has always been human-like conversation and many are still considering passing the Turing Test as the holy grail of the field. In this post, I show how to use a state of the art model on your own data (I use my own messages sent on facebook) to generate (somewhat) realistic conversations.&lt;/p&gt;

&lt;p&gt;In February 2019 OpenAI released &lt;a href=&quot;https://blog.openai.com/better-language-models/&quot;&gt;information on their new state of the art language model&lt;/a&gt; which created a lot of buzz within the community. While few disagree the results they included are better than anything we’ve seen before (even if mainly because they made a bigger model combining recent advances), many were peeved that OpenAI only released a small pre-trained version of their model rather than the full one they generated examples with. Nonetheless, the small model is also very good, and due to nshepperd’s addition to their code, we can easily fine-tune it on our own data to easily generate (near-)state of the art results specific to whatever we want.&lt;/p&gt;

&lt;p&gt;Since then people have been experimenting with the model, including some like the aforementioned nsheppered adding simple scripts allowing us to fine-tune the model on our data. In this post, I am going to describe how to use easily available tools like those scripts, Google Colab and Facebook’s Data Export option in order to create borderline realistic conversation snippets.&lt;/p&gt;

&lt;p&gt;If you don’t have enough data on facebook, you should be able to easily export your conversation data from pretty much any other service and train on that. Better yet, you can combine data from different sources - as usual with machine learning, the more data you have the better.&lt;/p&gt;

&lt;p&gt;You can follow in the colab &lt;a href=&quot;https://colab.research.google.com/drive/1OiieFQZyROURR9kvfrsytsy4lGTSSIwP&quot;&gt;here&lt;/a&gt;. Make sure to click Runtime&amp;gt; Change Runtime type&amp;gt; GPU (or TPU)&lt;/p&gt;

&lt;h2 id=&quot;code&quot;&gt;Code&lt;/h2&gt;

&lt;p&gt;We start with the imports&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import os
import json
import random
import re
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then we clone &lt;a href=&quot;https://github.com/Tenoke/gpt-2&quot;&gt;my fork&lt;/a&gt; of &lt;a href=&quot;https://github.com/nshepperd/gpt-2&quot;&gt;nsheppered’s GPT&lt;/a&gt; finetuning repo - I’ve only made some small changes to it - mainly adding a couple of extra command line options for changing things like the learning rate and adding a stopping point. We also cd into it, and install the requirements and download the model.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!git clone https://github.com/Tenoke/gpt-2.git
cd gpt-2
!pip3 install -r requirements.txt
!sh download_model.sh 117M
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next, we need to download our facebook messages. Facebook explains how to do it &lt;a href=&quot;https://www.facebook.com/help/1701730696756992?helpref=hc_global_nav&quot;&gt;here&lt;/a&gt;. Only select ‘messages’ and for the format select ‘json’. After it is ready you can either download the file by clicking download or by using Dev Tools. To do so open them with F12, go to sources, and click download (and then just cancel it), then find the entry starting with file.php, right click it and ‘copy as curl’ as in this screenshot
&lt;img src=&quot;/static/screenshot-fb.png&quot; alt=&quot;Facebook Screenshot&quot; class=&quot;img-responsive&quot; /&gt;
then just add ! in front of the command (to run it in colab) and &lt;code class=&quot;highlighter-rouge&quot;&gt;--output fb-json.zip&lt;/code&gt; at the end to name the file.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;
!curl &amp;lt;link&amp;gt; --output fb-json.zip
&lt;/code&gt;
After that, we unzip the file and get a list containing all files with message data (as opposed to the other stuff that facebook includes in the zip)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!unzip fb-json.zip

files = []
for p, d, f in os.walk('messages/inbox'):
    for file in f:
        if file.endswith('message.json'):
            files.append(f'{p}/{file}')

len(files)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You should see a non-zero number at this point if everything is going as planned. In my case 560.&lt;/p&gt;

&lt;p&gt;I’ve also included a few functions - one to fix the encoding and escaping in facebook’s data, and two to detect cyrilic so I can exclude any chats I have not in English (this part would be irrelevant for most)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def fix_encoding(s):
  return re.sub('[\xc2-\xf4][\x80-\xbf]+',lambda m: m.group(0).encode('latin1').decode('utf8'),s)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now it is time to actually create a corpus from all those files. If you have a lot of data, you might want to do this in steps. There’s also a &lt;code class=&quot;highlighter-rouge&quot;&gt;banned_names&lt;/code&gt; tuple where you can add any names you don’t want to appear in the corpus.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
text_corpus = ''
banned_names = ('vladislav')
for file in files:
  with open(file, 'r') as f:
      try:
        msgs = json.load(f)['messages']
        msgs.reverse()
      except:
        pass
      else:
        if not any(bn in file for bn in banned_names):
        for msg in msgs:
          try:
            content = fix_encoding(msg['content'])
            to_add  = f&quot;({msg['timestamp_ms']}) {msg['sender_name']}: {content}\n&quot;
            text_corpus += to_add
          except KeyError:
            pass
        print(file)
        
          text_corpus += '\n\n'
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then save the data to a file, check how big the file is (14m in my case) and encode the data (unnecessary but it is faster if you do it before training).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;with open('fb-cleaned.txt', 'w') as f:
  f.write(text_corpus)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;It is time to start trainning!&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!PYTHONPATH=src ./train.py --dataset fb-cleaned.txt.npz --sample_every=250 --learning_rate=0.0001 --stop_after=251
!PYTHONPATH=src ./train.py --dataset fb-cleaned.txt.npz --sample_every=250 --learning_rate=0.001 --stop_after=751
!PYTHONPATH=src ./train.py --dataset fb-cleaned.txt.npz --sample_every=250 --learning_rate=0.0001 --beta=0.95 --stop_after=1251
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;At this point, we can see samples, although feel free to change the learning rate, beta or train for more cycles. The code so far takes under an hour to run for me but it depends on the size of your dataset among other things.&lt;/p&gt;

&lt;p&gt;We need to first copy the new trainned weights.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!cp -r /content/gpt-2/checkpoint/run1/* /content/gpt-2/models/117M/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After which we can either let it generate chats on its own with&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!python3 src/generate_unconditional_samples.py --top_k 40 --temperature 0.9
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Or we can force it to give us chats with a specific person (or on a topic)
by running&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;!python3 src/interactive_conditional_samples.py --top_k 40 --temperature 0.9
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;And then giving it something like &lt;code class=&quot;highlighter-rouge&quot;&gt;(137378602389) Ioannis Agathocleous: hi&lt;/code&gt; in the interactive prompt to get a chat with that person.&lt;/p&gt;

&lt;p&gt;Here are some samples. Some are better than others&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;(134972898517) Ioannis Agathocleous: lol sounds better to me&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349728992965) Svilen Todorov: lol if its fine you wont be able to be a middle aged man without pay&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680771216) Ioannis Agathocleous: but thats kind of what you were meant to think&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680826981) Ioannis Agathocleous: probs not&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13496808663733) Svilen Todorov: hah&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13496808683320) Ioannis Agathocleous: u know anyone else that can pay me in the end&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13496808843212) Ioannis Agathocleous: i heard someone pay them last year and i want to buy a house&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680884843) Svilen Todorov: yeh, well, its good to me as i can get some money with them&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13496809066473) Svilen Todorov: i am&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680951765) Svilen Todorov: and i wont have to bother with taxes and whatnot&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680951723) Svilen Todorov: they shouldnt do things that are worth it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680997574) Ioannis Agathocleous: thats great&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134968099728) Ioannis Agathocleous: but its a long process&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134968099818) Ioannis Agathocleous: it doesnt matter how your kids loan is&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134968099824) Ioannis Agathocleous: just pay someone&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680986601) Svilen Todorov: dunno if it wouldnt matter&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680997829) Ioannis Agathocleous: if its going to be good Id expect them to do that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349680999997) Svilen Todorov: but yeh, thats kind of what i said&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134968099961) Ioannis Agathocleous: well it doesnt matter, its the thing&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The conversations usually make sense for the person. We discuss things like sleeping problems, how we are and what we are doing often so you get stuff like:&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;(15191201253003) Svilen Todorov: haha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519120163882) Svilen Todorov: what are you up to&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519120147982) Svilen Todorov: dunno&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519120174335) Svilen Todorov: whats going on&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519120175033) Anna Gaydukevich: I got a present on the desk!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519120187017) Anna Gaydukevich: how’s it going?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519122123459) Svilen Todorov: alright&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(15191212141350) Svilen Todorov: how are you feeling&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519122127646) Svilen Todorov: havent been out at all&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121366360) Anna Gaydukevich: yeah sounds good&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121387453) Svilen Todorov: hahah&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(15191214178936) Svilen Todorov: im on my way back&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(15191214278912) Anna Gaydukevich: how was your trip?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121428989) Svilen Todorov: im still in the office&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121581623) Svilen Todorov: 😀&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(15191218780009) Anna Gaydukevich: alright sounds good&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121887000) Svilen Todorov: i slept like 5 hours&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121833743) Svilen Todorov: was too hard to sleep tonight&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519121898425) Svilen Todorov: 😀&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519290525900) Lisa Martins: what are your plans?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519290547803) Svilen Todorov: they have to do something in the office for an hour&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519290549909) Svilen Todorov: but i dont really sleep much&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519290556640) Svilen Todorov: so it’s a bit more fun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519290563817) Svilen Todorov: so it wont be too bad&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1519296376813) Anna Gaydukevich: yeah i might try’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;

  &lt;p&gt;(152665502433) Svilen Todorov: cool, nice!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(152665505561) Svilen Todorov: did u check your messages&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655031868) Eliza Beth: I’ve seen him before. This isn’t the main one though. Maybe the one of you’ll find out.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(152665506496) Svilen Todorov: its more like what I saw at my friend’s place and he was kind of curious so it might’ve been like some sort of a catch up, with some of the others’ friends with a nice airheaded man&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655080653) Eliza Beth: Jeez!&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655087642) Eliza Beth: He says it’s the only thing you’ve read yet lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655089792) Svilen Todorov: well it’s like a few people in my area, so i reckon thats where you’re going&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655053361) Eliza Beth: I assume he might buy it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655797371) Svilen Todorov: but you can still look for it now tho&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(15266557573782) Svilen Todorov: I dont know though&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(152665582321) Svilen Todorov: i reckon the first 2 days in a row wouldve gotten more in the first few days&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526658692365) Svilen Todorov: and you can buy it now if you only just know the local people&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655791832) Svilen Todorov: tho he seems to think the local girl is the most famous person here because she’s probably the best friend, so not sure whether she’ll buy money for that&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655871168) Eliza Beth: Omg dude how was work?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655781360) Eliza Beth: I think it’s the same as the first few months. It was fairly fun&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526655892792) Svilen Todorov: but you’ll be a bit more able to pick up for later&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1526656997775) Svilen Todorov: i was telling people what you can do to get a flat or whatever&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;However, it sometimes makes understandable but funny mistakes. E.g. my girlfriend has ‘gay’ in her name so it came up with this&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;(1596993813136) Svilen Todorov: hey how was ur day&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993881281) Svilen Todorov: just came in the morning, and the guy came, and we were still sitting on the couch&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993881783) Svilen Todorov: so was pretty good&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993913019) Svilen Todorov: and then i got to bed&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993945750) Svilen Todorov: wasnt too bad even a bit after i went to bed&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993957122) Anna Gaydukevich: oh god thats the good part of me and me being gay&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993963304) Svilen Todorov: well not your fault&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596993968954) Anna Gaydukevich: just felt kinda shit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994070202) Svilen Todorov: like i just fell asleep so you didnt know what I would’ve been doing with me&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994075337) Svilen Todorov: haha i guess there was a moment in it, when someone came to ask me if i wanted a drink&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994119171) Svilen Todorov: so i told her that i am gay&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994135885) Svilen Todorov: that was weird&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994155756) Anna Gaydukevich: lol&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994176836) Anna Gaydukevich: but i can’t believe it&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994177861) Anna Gaydukevich: 😀&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994177563) Anna Gaydukevich: are you gay?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994179040) Anna Gaydukevich: not my fault&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596998641459) Svilen Todorov: dunno that I have a different level in most ways&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596998646174) Anna Gaydukevich: oh yup&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596994186636) Anna Gaydukevich: why do you think i am more of a gay guy than a gay guy&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596998742412) Anna Gaydukevich: haha&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1596998770044) Svilen Todorov: seems like a lot of gay guys are gay’&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;It can also get creepy, for example in this conversation it generated with a friend of mine who passed away a few months ago&lt;/p&gt;

&lt;blockquote&gt;

  &lt;p&gt;(134914376916) Sam Rendall: yeah definitely the next 5 days when I go home I would be super bad, not so much&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134914382811) Sam Rendall: well yeah, I didnt go and still went home and I can take that out too :D&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915443033) Sam Rendall: yes just keep in the office that  takes you up&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915443939) Svilen Todorov: you will do&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915445710) Svilen Todorov: its like 8-9 hours in a row&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915447500) Sam Rendall: ok cool&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915447700) Sam Rendall: yeah that’s sweet&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915448937) Svilen Todorov: cool, i hope that you’re still alive&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915457569) Svilen Todorov: but i have to go home to smoke in the morning after i went by and take your stuff&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134915459816) Sam Rendall: not bad for you&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134900491569) Svilen Todorov: and i’m gonna stay on the couch where you will be sleeping in the morning before coming&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13490049834) Sam Rendall: ohaha yeah :P&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349004961272) Svilen Todorov: ugh&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(134900496518) Sam Rendall: so much more of your body still hurts than mine&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349008060178) Svilen Todorov: damn&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349008171520) Svilen Todorov: that sounds shit&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(13490092616) Sam Rendall: its a fucked up thing, not as fast of an idea&lt;/p&gt;
&lt;/blockquote&gt;

&lt;blockquote&gt;
  &lt;p&gt;(1349009693548) Sam Rendall: my stomach hasnt been hurting to this point&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;The conversations look okay, though sometimes they are less coherent. It is clearly learning useful stuff - the timestamps generally go up, the conversations are relevant to the people in them, and so is the structure. Training it with more data would also definitely help.&lt;/p&gt;

&lt;p&gt;Another way to make it produce slightly better results is to finetune it in the end on the chat from a specific person before generating conversations with them (I’ve added the relevant code to the colab).&lt;/p&gt;

&lt;p&gt;We haven’t gotten anywhere near passing the Turing Test but I suspect that if you hook this up to facebook’s API to respond for you, it might take a while for some people to figure it out.&lt;/p&gt;</content><author><name></name></author><category term="gpt" /><category term="gpt2" /><category term="finetune" /><category term="colab" /><category term="nlp" /><category term="machine-learning" /><summary type="html">One of the most interesting problems in NLP has always been human-like conversation and many are still considering passing the Turing Test as the holy grail of the field. In this post, I show how to use a state of the art model on your own data (I use my own messages sent on facebook) to generate (somewhat) realistic conversations.</summary></entry><entry><title type="html">Building an in-browser Rock Paper Scissors Neural Network capable of beating humans</title><link href="http://localhost:4000/blog/rps/" rel="alternate" type="text/html" title="Building an in-browser Rock Paper Scissors Neural Network capable of beating humans" /><published>2018-07-31T00:00:00+02:00</published><updated>2018-07-31T00:00:00+02:00</updated><id>http://localhost:4000/blog/RPS</id><content type="html" xml:base="http://localhost:4000/blog/rps/">&lt;p&gt;As the &lt;a href=&quot;https://archive.nytimes.com/www.nytimes.com/interactive/science/rock-paper-scissors.html&quot;&gt;New York Times&lt;/a&gt; have demonstrated, an algorithm can consistently win against humans at Rock Paper Scissors (RPS) using the right statistical techniques. The fact that it is doable, the simplicity, and the popularity of the game make it a good choice for attempting to do it using Deep Learning and making it run with tensorflow.js.&lt;/p&gt;

&lt;p&gt;The goals here are simple - we want to run the Neural Net in the user’s browser, and we want it to have some edge and win against most humans.&lt;/p&gt;

&lt;h2 id=&quot;framing-the-problem&quot;&gt;Framing the problem&lt;/h2&gt;

&lt;p&gt;A common instinct when thinking of games is to have an agent-based approach using reinforcement learning - e.g. Q learning. However, reinforcement learning has a reputation for being somewhat finicky, unstable, and often relatively computationally expensive.&lt;/p&gt;

&lt;p&gt;In this specific case, we can, however, frame the problem as a classification problem - predicting whether the next move by the human is going to be a Rock/Paper/Scissors and simply doing the opposite ourselves - which allows us to use somewhat more robust, standard and hopefully smaller models.&lt;/p&gt;

&lt;h2 id=&quot;preparing-to-build-the-model&quot;&gt;Preparing to build the model&lt;/h2&gt;

&lt;p&gt;We are going to be running the finished model fully in the browser and despite it being possible to define everything in JavaScript, some things are easier to do in python. For one, the tensorflow.js API is not quite as complete and nice as the python one, and even more importantly - I find preprocessing of data much easier in python and working in a Jupyter notebook in general.&lt;/p&gt;

&lt;p&gt;Thus, we are going to take advantage of being able to import a tensorflow/keras model into tensorflow.js, which as far as I can tell is almost always the better option.&lt;/p&gt;

&lt;p&gt;Note: The &lt;a href=&quot;https://github.com/Tenoke/tensorflowjs-rps&quot;&gt;associated repo&lt;/a&gt; has the code, data, and all pip-installable prerequisites.&lt;/p&gt;

&lt;h3 id=&quot;overview-of-approach&quot;&gt;Overview of approach&lt;/h3&gt;

&lt;p&gt;We are going to use an LSTM, so data from the previous moves made by the player can influence a prediction of their next move, while also being able to work with a non-fixed number of moves. After we predict their next move, we are simply going to do the move which wins against them (e.g. if we predict they are going to throw rock, we throw paper).&lt;/p&gt;

&lt;p&gt;We are also going to use a little trick to counter how weak the signal in the data is. Even though humans don’t quite play randomly (if they did, we wouldn’t ever be able to perform better against anyone, except by chance) they hardly follow an exceptionally easy to spot pattern either. In fact, we should expect it to be fairly hard, and data-intensive for our model to spot a pattern and to even start learning anything at all about the data thrown at it.&lt;/p&gt;

&lt;p&gt;What we are going to do, in order to force the model to at least initially learn &lt;em&gt;something&lt;/em&gt; about RPS is to give it a secondary objective - to guess who won the current round (for which it has all the data), and to keep track of how many wins it has against the player overall. A fairly easy objective, compared to our main objective of guessing the player’s next move.&lt;/p&gt;

&lt;p&gt;This has two benefits - it makes the model learn something about RPS (mainly which hand wins against which other hand), and it is also extremely helpful while building the model - if we see that it is not even learning the secondary objective, then there is little hope for it to learn the main one. Indeed, this helped me identify multiple bugs during development.&lt;/p&gt;

&lt;h3 id=&quot;imports&quot;&gt;Imports&lt;/h3&gt;

&lt;p&gt;Let’s start by importing the main things we need - numpy and keras.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;from tensorflow import keras
import numpy as np
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;data&quot;&gt;Data&lt;/h3&gt;

&lt;p&gt;I looked for any RPS data to build the initial model on, and the first thing which popped out and seemed usable was &lt;a href=&quot;https://github.com/PizzaRollExpert/Rock-paper-scissors-data/&quot;&gt;This repo&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%%bash
wget https://github.com/PizzaRollExpert/Rock-paper-scissors-data/raw/master/data.txt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After downloading it we need to create a dict for converting the symbols in the dataset to numbers.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;move_to_n = {
    's':1,
    'p':2,
    'x':3
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then create a function that iterates over all the moves in the files, converts them to numbers using the move_to_n dict, and splits all the moves that are part of the same game into arrays. The code can be simplified and sped up, but as it is for a one-time preprocessing of a very specifically formatted dataset, there’d be little point.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def convert_data(data):
    result = []
    game = []
    for row in data:
        moves = []
        if row == '-':
            if len(game) &amp;gt; 2:
                result.append(np.array(game))
                game = []
            continue
        for move in row:
            moves.append(move_to_n[move])
        if len(moves) &amp;gt; 1:
            game.append(np.array(moves))
        moves = []
    return np.array(result)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then open the data we downloaded, split it by rows, convert it using our function, and then flip it - so we have the data both from the perspective of the first player (e.g. rock vs paper) and the second one (e.g. paper vs rock).&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;data = convert_data(data)
data2 = np.array([np.flip(games, 1) for games in data]) #reverse for player2
data = np.concatenate((data2, data))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we have our X - the prepared data for passing to the model to base its guesses, but we need to create our y (ground truth), too.&lt;/p&gt;

&lt;p&gt;First, we are going to make a small helper function for determining who won a given round. I googled around for an easier way to check who won instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;if player1 == 'rock' and player2 ==..&lt;/code&gt;, and I created the function below, based on what I saw &lt;a href=&quot;https://stackoverflow.com/questions/11377117/rock-paper-scissors-determine-win-loss-tie-using-math&quot;&gt;Here&lt;/a&gt;. I admit, I had to triple-check if it works properly.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def calculate_winner(x):
    result = x[0] - x[1]
    if result == 0: return 0 #tie
    if result in (1,-2): return 1 #win
    return -1 #lose
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And then another function to create our matching X, and y. We go over each game (which has multiple moves by the same players), add a running score using &lt;code class=&quot;highlighter-rouge&quot;&gt;calculate_winner&lt;/code&gt; for our secondary objective, and use next round’s move from player 1 as the portion of y in our primary objective.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;def create_xy_winner(data):
    result_y = []
    result_x = []
    for game in data:
        game_y = []
        score = 0
        for i, moves in enumerate(game):
            if i+1 == len(game):
                result_y.append(np.array(game_y))
                result_x.append(game[:-1]) #remove last moves from x
                #skip last game as we dont know what player1 will choose next move
                continue
            score += calculate_winner(game[i])
            game_y.append([game[i+1][0], score]) #append next player1 move
    return np.array(result_x), np.array(result_y)

X, y = create_xy_winner(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;model&quot;&gt;Model&lt;/h3&gt;

&lt;p&gt;We have our data nicely formatted now, so it is time to create our model.&lt;/p&gt;

&lt;p&gt;We start with an Input layer, which has dimensions (None, 8) - None for the timesteps (as many as we pass) which correspond to the rounds in a given game, and 8 because after playing with it, I realized that one-hot encoding our (e.g. Rock - Paper) input data works better after testing. 
We then add a Dense layer (Note: instead of one-hot encoding and Dense layer, we could’ve also just used an Embedding).
After that is our LSTM layer, which is the key to remembering and using information from previous rounds.
We then add two Dense layers in a row, with the second one outputting a probability representing whether we expect player 1 to throw, rock paper, or scissors.
We then do the same for our secondary objective - how many wins has player 1 had so far, which is a single number.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;main_input&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;dtype&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'float32'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;32&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LSTM&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;96&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;return_sequences&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;True&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;main_output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'relu'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;main_output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'softmax'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;20&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;'tanh'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;lstm&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;Dense&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;m&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;keras&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;k&quot;&gt;Model&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;inputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_input&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,],&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;outputs&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;=[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;main_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;second_output&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also create an optimizer - Adam with the normal defaults, and compile the model, so the primary objective is treated as much more important (1.0) than our secondary objective (0.2), and we choose their respective loss functions - categorical crossentropy for the primary (as we are choosing categories - rock, paper or scissors), and mean squared error for the number of wins.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;opt = keras.optimizers.Adam(lr=0.001)
model.compile(loss=['categorical_crossentropy', 'mse'], optimizer=opt, metrics=['accuracy'], loss_weights=[1., 0.2])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;That’s it for the model. I also attempted some variations - adding Dropout, Batchnorm, and more/less layers, but the current implementation worked best out of those I tried.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;Now, is the time to train our model, however, I get an error when I try to model.fit the data - The model expects our data to be one-hot encoded (or more specifically to have a shape of *,8 rather than *,2), but I didn’t do that initially. Now, because we are working with so little data, and a tiny model (all of it trains in a few seconds on my CPU), we can just do it as we pass the data.&lt;/p&gt;

&lt;p&gt;We iterate over all the games we have, one-hot encode our X and primary y, using the built-in keras function &lt;code class=&quot;highlighter-rouge&quot;&gt;to_categorical&lt;/code&gt;, re-shape the data until we get it how the model expects it (I rarely get this quite right on the first try) and train the model using a batch size of 1.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    for i in range(len(X)):
        X_, y_ = X[i], y[i]
        X_ = keras.utils.to_categorical(X_, 4)
        X_ = X_.reshape(1, X_.shape[0], 8)
        y_1 = keras.utils.to_categorical(y_[:,0].reshape(1, y_.shape[0], 1), 4)
        y_2 = y_[:,1].reshape(1, y_.shape[0], 1)
        verbose = 2 if (i % 15 == 0) else 0 # print only 1/25th of the time
        model.fit(X_, [y_1, y_2], epochs=1, shuffle=False, batch_size=1, verbose=verbose)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we have a trained model and it seems to reach an accuracy of 1 for guessing the number of wins (after fixing a couple of bugs), so we know the model is doing something right. After testing it out though, it will sometimes be very exploitable (e.g. getting stuck into throwing scissors 10 times in a row).&lt;/p&gt;

&lt;p&gt;So, my next step was to increase the data we have - mainly by playing a bunch of games against it and then re-training by adding the new data in. After doing that, I sent it to 12 people, with only one of them managing to win (after at least 50 rounds), so I added their data to main data and re-trained it again.&lt;/p&gt;

&lt;p&gt;Realistically, the model is somewhat overfitted against playing against me, as that is where the bulk of the data comes from - and it really does wipe the floor with me. If we truly want to optimize the model, we’d keep collecting more data from different people and keep training it, but it already seems to be able to win against most people (given a sufficient number of rounds) which is good enough for our purposes.&lt;/p&gt;

&lt;h3 id=&quot;exporting-the-model&quot;&gt;Exporting the model&lt;/h3&gt;

&lt;p&gt;Now that we have built and trained our model we just need to load it with tensorflow.js and make it work in the browser. We first export it.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import tensorflowjs as tfjs
tfjs.converters.save_keras_model(model, 'Full-Model')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;%%bash
rm -rf static/Full-Model
mv Full-Model static/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;importing-the-model-to-javascript&quot;&gt;Importing the model to JavaScript&lt;/h3&gt;

&lt;p&gt;All we need in order to load our model is to import tensorflow.js, which we will do in our html via a CDN.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&amp;lt;script src=&quot;https://cdn.jsdelivr.net/npm/@tensorflow/tfjs@0.11.2&quot;&amp;gt; &amp;lt;/script&amp;gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;After that, we can simply load it in our JavaScript like so&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;const fullModel  = tf.loadModel('/static/Full-Model/model.json')
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;&lt;em&gt;Note: A lot of things are still broken in tensorflow.js - for one, if you name your layers yourself, you can’t import the model. For another, it doesn’t support the default naming of the newest version of tensorflow either. If you have any problems, try using an older version of tensorflow, or tensorflow.js. When I had that issue, it was most easily fixed using &lt;code class=&quot;highlighter-rouge&quot;&gt;import keras&lt;/code&gt; instead of &lt;code class=&quot;highlighter-rouge&quot;&gt;from tensorflow import keras&lt;/code&gt;, with newest versions from pip/conda for both.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;After we have loaded the model we create the dicts for converting from number to move, and back.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;n_to_move = {
  1:'rock',
  2:'paper',
  3:'scissors'
}

move_to_n = {
  'rock': 1,
  'paper': 2,
  'scissors': 3
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We also choose a random first move (as we have no data yet) and create a list to hold the moves so far and some counters for the wins.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function getRandomInt(max, starting=1) {
  return (starting + Math.floor(Math.random() * max))
}
//Choose a random first move
let nextComputerMove = getRandomInt(3)
currentMoves = []
let humanWinCounter = 0
let computerWinCounter = 0
let tieCounter = 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We then create a copy of the &lt;code class=&quot;highlighter-rouge&quot;&gt;to_categorical&lt;/code&gt; function we used for one-hot encoding the data, as it doesn’t exist in tensorflow.js, and a function to determine who has won (and update the win counters)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function to_categorical(n, length=4) {
  let result = Array.from({length}, ()=&amp;gt; 0.)
  result[n] = 1.
  return result
}

function updateCounters(humanMove, computerMove) {
  switch (humanMove - computerMove) {
    case 0:
      tieCounter++
      return 0
    case  1: 
    case -2:
      humanWinCounter++
      return 1
    default:
      computerWinCounter++
      return 2
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;We will also need a function to determine our move based on our prediction of the player’s next move (e.g. if we predict rock, throw paper)&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;
function moveBasedOn(move) {
  switch(move) {
    case 1:
      return 2
    case 2:
      return 3
    case 3:
      return 1
  }
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now we can create a function, which uses the moves done by the player and computer so far, and uses the model to choose our next move. We only use the last 28 moves - I tried a few numbers between the last 20 and last 35 moves, and somewhere between 25 and 30 seemed to perform best in my limited testing, likely due to the nature of our training data in combination with LSTMs sometimes getting more brittle after a larger number of timesteps.&lt;/p&gt;

&lt;p&gt;Tensorflow.js functions are mostly asynchronous (with some of them having a synchronous version like .dataSync), so any function which works with the model needs to be asynchronous, too.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;async&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;function&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;calculateNextComputerMove&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;const&lt;/span&gt; &lt;span class=&quot;no&quot;&gt;model&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;await&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;fullModel&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;lastMoves&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;currentMoves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;Math&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;currentMoves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;28&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;),&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;currentMoves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;tensor3d&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;([&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;lastMoves&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;lastMoves&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;length&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;8&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;let&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;model&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;predict&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;data&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;as1D&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// we are getting predictions for all moves (because of how we trained the network)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// but only care for the prediction for the next move - the last 4 numbers&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;slice&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;-&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;4&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// next we turn that into a single number from a one-hot encoding&lt;/span&gt;
  &lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;argMax&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;dataSync&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;()[&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;]&lt;/span&gt;
  &lt;span class=&quot;c1&quot;&gt;// and turn that into our next move based on what will beat the human&lt;/span&gt;
  &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;nx&quot;&gt;moveBasedOn&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nx&quot;&gt;nextHumanMovePrediction&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;When we have that, all that is needed is a function that takes the player’s move, shows them the model’s move (which has been calculated before they even made theirs), updates the counters and uses the current data to calculate the model’s next move.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;function chooseMove(move) {
  console.log('human: ', n_to_move[move])
  var winner = updateCounters(move, nextComputerMove)
  console.log(winner) # 0 = tie, 1 = human, 2 = computer
  currentMoves.push(to_categorical(move).concat(to_categorical(nextComputerMove)))
  calculateNextComputerMove().then(nextMove=&amp;gt;nextComputerMove=nextMove)
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That’s it. You can now play against the model by opening the console in your browser, and calling chooseMove(..) with ‘rock’, ‘paper’ or ‘scissors’, or building a frontend around the model like the one in the &lt;a href=&quot;https://github.com/Tenoke/tensorflowjs-rps&quot;&gt;github repo&lt;/a&gt; accompanying this post.&lt;/p&gt;</content><author><name></name></author><category term="machine-learning" /><summary type="html">As the New York Times have demonstrated, an algorithm can consistently win against humans at Rock Paper Scissors (RPS) using the right statistical techniques. The fact that it is doable, the simplicity, and the popularity of the game make it a good choice for attempting to do it using Deep Learning and making it run with tensorflow.js.</summary></entry></feed>